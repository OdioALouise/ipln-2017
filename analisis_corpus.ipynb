{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from lxml import etree\n",
    "import re\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12106\n",
      "-La semana pasada mi hijo hizo un triple salto mortal desde 20 metros de altura - ¿Es trapecista? -Era :(\n",
      "-Yo ya voy por mi segundo millón de dólares... -¿!Ah, si!? -Es que el primero nunca lo hice... #fb\n",
      "-Ayer fue mi cumpleaños y no me felicitaste - ¡FéÉLíCÍDáÁDÉéS! - ¿Qué haces? -Felicitarte con retraso.\n",
      "No es flojera, es un estado de ahorro de energía corporal :)\n",
      "- ¿Cómo te fue en matemática? -Vos sabes que soy muy pacífica - ¿Y eso qué tiene que ver? -No me gustan los problemas jajaja -Castigada - :(\n",
      "\"El pesimista se queja del viento; el optimista espera que cambie; el realista ajusta las velas\" Feliz miércoles.\n",
      "-¿Y tú desde cuando llevas pendiente? \n",
      "-Desde que mi mujer se lo encontró en el coche y le dije que era mío...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texto=\"\"\n",
    "print ( len(corpus))\n",
    "for text in corpus['text'][:7]:\n",
    "    texto+=text + '\\n'\n",
    "print (texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml = analyzer.run(texto.encode(), 'flush')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia numero  1\n",
      "form=\"-\" \t\n",
      "form=\"La\" \t\n",
      "form=\"semana\" \t\n",
      "form=\"pasada\" \t\n",
      "form=\"mi\" \t\n",
      "form=\"hijo\" \t\n",
      "form=\"hizo\" \t\n",
      "form=\"un\" \t\n",
      "form=\"triple\" \t\n",
      "form=\"salto\" \t\n",
      "form=\"mortal\" \t\n",
      "form=\"desde\" \t\n",
      "form=\"20_metros\" \t\n",
      "form=\"de\" \t\n",
      "form=\"altura\" \t\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xbf\" \t\n",
      "form=\"Es\" \t\n",
      "form=\"trapecista\" \t\n",
      "form=\"?\" \t\n",
      "form=\"-\" \t\n",
      "form=\"Era\" \t\n",
      "form=\":\" \t\n",
      "form=\"(\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  2\n",
      "form=\"-\" \t\n",
      "form=\"Yo\" \t\n",
      "form=\"ya\" \t\n",
      "form=\"voy\" \t\n",
      "form=\"por\" \t\n",
      "form=\"mi\" \t\n",
      "form=\"segundo\" \t\n",
      "form=\"mill\\xc3\\xb3n\" \t\n",
      "form=\"de\" \t\n",
      "form=\"d\\xc3\\xb3lares\" \t\n",
      "form=\"...\" \t\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xbf\" \t\n",
      "form=\"!\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  3\n",
      "form=\"Ah\" \t\n",
      "form=\",\" \t\n",
      "form=\"si\" \t\n",
      "form=\"!\" \t\n",
      "form=\"?\" \t\n",
      "form=\"-\" \t\n",
      "form=\"Es\" \t\n",
      "form=\"que\" \t\n",
      "form=\"el\" \t\n",
      "form=\"primero\" \t\n",
      "form=\"nunca\" \t\n",
      "form=\"lo\" \t\n",
      "form=\"hice\" \t\n",
      "form=\"...\" \t\n",
      "form=\"#\" \t\n",
      "form=\"fb\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  4\n",
      "form=\"-\" \t\n",
      "form=\"Ayer\" \t\n",
      "form=\"fue\" \t\n",
      "form=\"mi\" \t\n",
      "form=\"cumplea\\xc3\\xb1os\" \t\n",
      "form=\"y\" \t\n",
      "form=\"no\" \t\n",
      "form=\"me\" \t\n",
      "form=\"felicitaste\" \t\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xa1\" \t\n",
      "form=\"F\\xc3\\xa9\\xc3\\x89L\\xc3\\xadC\\xc3\\x8dD\\xc3\\xa1\\xc3\\x81D\\xc3\\x89\\xc3\\xa9S\" \t\n",
      "form=\"!\" \t\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xbf\" \t\n",
      "form=\"Qu\\xc3\\xa9\" \t\n",
      "form=\"haces\" \t\n",
      "form=\"?\" \t\n",
      "form=\"-\" \t\n",
      "form=\"Felicitar\" \t\n",
      "form=\"te\" \t\n",
      "form=\"con\" \t\n",
      "form=\"retraso\" \t\n",
      "form=\".\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  5\n",
      "form=\"No\" \t\n",
      "form=\"es\" \t\n",
      "form=\"flojera\" \t\n",
      "form=\",\" \t\n",
      "form=\"es\" \t\n",
      "form=\"un\" \t\n",
      "form=\"estado\" \t\n",
      "form=\"de\" \t\n",
      "form=\"ahorro\" \t\n",
      "form=\"de\" \t\n",
      "form=\"energ\\xc3\\xada\" \t\n",
      "form=\"corporal\" \t\n",
      "form=\":\" \t\n",
      "form=\")\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  6\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xbf\" \t\n",
      "form=\"C\\xc3\\xb3mo\" \t\n",
      "form=\"te\" \t\n",
      "form=\"fue\" \t\n",
      "form=\"en\" \t\n",
      "form=\"matem\\xc3\\xa1tica\" \t\n",
      "form=\"?\" \t\n",
      "form=\"-\" \t\n",
      "form=\"Vos\" \t\n",
      "form=\"sabes\" \t\n",
      "form=\"que\" \t\n",
      "form=\"soy\" \t\n",
      "form=\"muy\" \t\n",
      "form=\"pac\\xc3\\xadfica\" \t\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xbf\" \t\n",
      "form=\"Y\" \t\n",
      "form=\"eso\" \t\n",
      "form=\"qu\\xc3\\xa9\" \t\n",
      "form=\"tiene\" \t\n",
      "form=\"que\" \t\n",
      "form=\"ver\" \t\n",
      "form=\"?\" \t\n",
      "form=\"-\" \t\n",
      "form=\"No\" \t\n",
      "form=\"me\" \t\n",
      "form=\"gustan\" \t\n",
      "form=\"los\" \t\n",
      "form=\"problemas\" \t\n",
      "form=\"jajaja\" \t\n",
      "form=\"-\" \t\n",
      "form=\"Castigada\" \t\n",
      "form=\"-\" \t\n",
      "form=\":\" \t\n",
      "form=\"(\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  7\n",
      "form=\"&quot;\" \t\n",
      "form=\"El\" \t\n",
      "form=\"pesimista\" \t\n",
      "form=\"se\" \t\n",
      "form=\"queja\" \t\n",
      "form=\"de\" \t\n",
      "form=\"el\" \t\n",
      "form=\"viento\" \t\n",
      "form=\";\" \t\n",
      "form=\"el\" \t\n",
      "form=\"optimista\" \t\n",
      "form=\"espera\" \t\n",
      "form=\"que\" \t\n",
      "form=\"cambie\" \t\n",
      "form=\";\" \t\n",
      "form=\"el\" \t\n",
      "form=\"realista\" \t\n",
      "form=\"ajusta\" \t\n",
      "form=\"las\" \t\n",
      "form=\"velas\" \t\n",
      "form=\"&quot;\" \t\n",
      "form=\"Feliz\" \t\n",
      "form=\"mi\\xc3\\xa9rcoles\" \t\n",
      "form=\".\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  8\n",
      "form=\"-\" \t\n",
      "form=\"\\xc2\\xbf\" \t\n",
      "form=\"Y\" \t\n",
      "form=\"t\\xc3\\xba\" \t\n",
      "form=\"desde\" \t\n",
      "form=\"cuando\" \t\n",
      "form=\"llevas\" \t\n",
      "form=\"pendiente\" \t\n",
      "form=\"?\" \t\n",
      "\n",
      "\n",
      "\n",
      "Sentencia numero  9\n",
      "form=\"-\" \t\n",
      "form=\"Desde\" \t\n",
      "form=\"que\" \t\n",
      "form=\"mi\" \t\n",
      "form=\"mujer\" \t\n",
      "form=\"se\" \t\n",
      "form=\"lo\" \t\n",
      "form=\"encontr\\xc3\\xb3\" \t\n",
      "form=\"en\" \t\n",
      "form=\"el\" \t\n",
      "form=\"coche\" \t\n",
      "form=\"y\" \t\n",
      "form=\"le\" \t\n",
      "form=\"dije\" \t\n",
      "form=\"que\" \t\n",
      "form=\"era\" \t\n",
      "form=\"m\\xc3\\xado\" \t\n",
      "form=\"...\" \t\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern_form = re.compile(r'form=\"(.*?)\"')\n",
    "num_sentences=1\n",
    "for sentence in xml:\n",
    "    print(\"Sentencia numero \", num_sentences)\n",
    "    for token in sentence:\n",
    "        m = re.search(pattern_form, etree.tostring(token).decode())\n",
    "        print(m.group(0), \"\\t\")\n",
    "    print(\"\\n\\n\")\n",
    "    num_sentences+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de votos 26943\n",
      "No humor, votos 14131  porcentaje  52.4477600861\n",
      "Humor 1 estrella, votos  2960  porcentaje  10.9861559589\n",
      "Humor 2 estrellas, votos  2421  porcentaje  8.98563634339\n",
      "Humor 3 estrellas, votos  3274  porcentaje  12.1515792599\n",
      "Humor 4 estrellas, votos  2541  porcentaje  9.43102104443\n",
      "Humor 5 estrellas, votos  1616  porcentaje  5.99784730728\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTACION Cantidad de votos por calificacion\n",
    "#Calificaciones\n",
    "no_humor=0\n",
    "humor_e1=0\n",
    "humor_e2=0\n",
    "humor_e3=0\n",
    "humor_e4=0\n",
    "humor_e5=0\n",
    "total=0\n",
    "\n",
    "#Vectores\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "for calificacion in vec_no_humor[:]:\n",
    "    no_humor+=calificacion\n",
    "\n",
    "for calificacion in vec_e1[:]:\n",
    "    humor_e1+=calificacion\n",
    "    \n",
    "for calificacion in vec_e2[:]:\n",
    "    humor_e2+=calificacion\n",
    "    \n",
    "for calificacion in vec_e3[:]:\n",
    "    humor_e3+=calificacion\n",
    "    \n",
    "for calificacion in vec_e4[:]:\n",
    "    humor_e4+=calificacion\n",
    "\n",
    "for calificacion in vec_e5[:]:\n",
    "    humor_e5+=calificacion\n",
    "\n",
    "total=no_humor + humor_e1 + humor_e2 + humor_e3 + humor_e4 + humor_e5\n",
    "\n",
    "print(\"Total de votos\", total)\n",
    "print(\"No humor, votos\", no_humor,\" porcentaje \", no_humor/total*100)\n",
    "print(\"Humor 1 estrella, votos \", humor_e1, \" porcentaje \", humor_e1/total*100)\n",
    "print(\"Humor 2 estrellas, votos \", humor_e2, \" porcentaje \", humor_e2/total*100)\n",
    "print(\"Humor 3 estrellas, votos \", humor_e3, \" porcentaje \", humor_e3/total*100)\n",
    "print(\"Humor 4 estrellas, votos \", humor_e4, \" porcentaje \", humor_e4/total*100)\n",
    "print(\"Humor 5 estrellas, votos \", humor_e5, \" porcentaje \", humor_e5/total*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets  12106\n",
      "Los tweets mas votados tienen  21 votos\n",
      "Los tweets con dos o menos votos forman un total de  8668\n",
      "Los chistes mas graciosos son\n",
      "1 \n",
      " —¿A dónde vas tan maquillada? —A una fiesta, mamá. —¿Eres el payaso? —¡MAMÁAAA! :( —JAJAJÁ, cállate y hazme reír o no vas. —Ok. :(\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTACION Cantidad de votos por tweet\n",
    "#Vectores\n",
    "vec_ids=corpus['id'][:]\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "#Variables\n",
    "cant_tweets=len(corpus)\n",
    "\n",
    "print(\"Cantidad de tweets \", cant_tweets)\n",
    "\n",
    "tweet_id_vot=np.zeros((cant_tweets, 2), np.int64)\n",
    "\n",
    "contador=0\n",
    "for i in range(0, cant_tweets):\n",
    "    tweet_id_vot[i, 0]=vec_ids[i]\n",
    "    tweet_id_vot[i,1]=vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i]\n",
    "\n",
    "\n",
    "array=tweet_id_vot[:,1]\n",
    "array=np.sort(array, axis=None)\n",
    "last=array[len(array) - 1]\n",
    "\n",
    "lesseq_3=0\n",
    "for i in range(0, cant_tweets):\n",
    "    if( array[i] > 2 ):\n",
    "        break;\n",
    "    lesseq_3+=1\n",
    "\n",
    "print(\"Los tweets mas votados tienen \", last, \"votos\")\n",
    "print(\"Los tweets con dos o menos votos forman un total de \", lesseq_3)\n",
    "\n",
    "contador=1\n",
    "print(\"Los chistes mas graciosos son\")\n",
    "for i in range(0, cant_tweets):\n",
    "    if( vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] == 21 ):\n",
    "        print(contador, \"\\n\", corpus['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Te voy a dar dos medallas, una por idiota y otra por si la perdes\n",
      "—Buenos días, princesa. ¿Te puedo decir princesa, verdad? \n",
      "\n",
      "—No. \n",
      "\n",
      "—Ok. Buenos días profe, ¿me deja pasar?\n",
      "Hoy comienza la dieta de AJO y AGUA... \"A joderse y aguantarse\" by @TannisPaEs #chistes #fb\n",
      "RT @SantiContreras: Israel y Hamás se acusan mutuamente por la muerte de niños en Gaza: http://t.co/pHeNYpCusQ http://t.co/JQHjyatz6o\n",
      "Nunca cambies la amistad por el amor. Recuerda: la amistad está cuando el amor se va.\n",
      "Mediana primera persona  0.3750041301833801420785\n",
      "Mediana segunda persona  0.278543007266\n",
      "Media primera persona  0.375\n",
      "Media segunda persona  0.278543007266\n",
      "Moda primera persona  ModeResult(mode=array([ 0.25]), count=array([1]))\n",
      "Moda segunda persona  ModeResult(mode=array([ 0.18569534]), count=array([1]))\n"
     ]
    }
   ],
   "source": [
    "pattern_tag = re.compile(r'tag=\"(.*?)\"')\n",
    "\n",
    "first_second_person=np.zeros((len(corpus), 2))\n",
    "contador=0\n",
    "\n",
    "tam=len(corpus)\n",
    "\n",
    "for i in np.random.randint(0, tam, 5):\n",
    "    text = corpus['text'][i]\n",
    "    print(text)\n",
    "    total_token=0\n",
    "    token_1=0\n",
    "    token_2=0\n",
    "    xml = analyzer.run(text.encode(), 'flush')\n",
    "    #num_sentences=1\n",
    "    for sentence in xml:\n",
    "        #print(\"Sentencia numero \", num_sentences)\n",
    "        for token in sentence:\n",
    "            m = re.search(pattern_tag, etree.tostring(token).decode())\n",
    "            tag=m.group(1)\n",
    "            if (tag[0] == 'V'):\n",
    "                #print(\"Verbo \", tag[0])                \n",
    "                if(tag[4] == '1'):\n",
    "                    token_1+=1\n",
    "                    #print(\"Primera persona \", tag[4], \"\\t\")\n",
    "                elif(tag[4]=='2'):\n",
    "                    token_2+=1\n",
    "                    #print(\"Segunda persona\", tag[4], \"\\t\")\n",
    "                #else:\n",
    "                    #token_no12+=1\n",
    "                    #print(\"TOken sin primera ni segunda persona\")\n",
    "            elif(tag[0] == 'P'):\n",
    "                #print(\"Pronombre \", tag[0])                \n",
    "                if(tag[2] == '1'):\n",
    "                    token_1+=1\n",
    "                    #print(\"Primera persona \", tag[2], \"\\t\")\n",
    "                elif(tag[2]=='2'):\n",
    "                    token_2+=1\n",
    "                    #print(\"Segunda persona\", tag[2], \"\\t\")\n",
    "                #else:\n",
    "                    #token_no12+=1\n",
    "                    #print(\"TOken sin primera ni segunda persona\")                    \n",
    "            elif(tag[0] == 'D'):\n",
    "                #print(\"Determinante \", tag[0])                \n",
    "                if(tag[2] == '1'):\n",
    "                    token_1+=1\n",
    "                    #print(\"Primera persona \", tag[2], \"\\t\")\n",
    "                elif(tag[2]=='2'):\n",
    "                    token_2+=1\n",
    "                    #print(\"Segunda persona\", tag[2], \"\\t\")\n",
    "                #else:\n",
    "                    #token_no12+=1\n",
    "                    #print(\"TOken sin primera ni segunda persona\")\n",
    "            #else:\n",
    "                #token_no12+=1\n",
    "                #print(\"Token sin primera ni segunda persona \")\n",
    "            total_token+=1\n",
    "        #print(\"\\n\\n\")\n",
    "        #num_sentences+=1\n",
    "    #print(\"Total de tokens \", total_token)\n",
    "    #print(\"Primera persona \", token_1)\n",
    "    #print(\"Segunda persona \", token_2)\n",
    "    #print(\"Contador \", contador)\n",
    "    #print(\"Medida primera persona \", token_1/math.sqrt(total_token))\n",
    "    #print(\"Medida segunda persona \", token_2/math.sqrt(total_token)) \n",
    "    first_second_person[contador,0]=token_1/math.sqrt(total_token)\n",
    "    first_second_person[contador,1]=token_2/math.sqrt(total_token)    \n",
    "    contador+=1\n",
    "    print (\"Dato numero \",contador, \"Porcentaje \", contador/len(corpus),  end=\"\\r\")\n",
    "    time.sleep(1)\n",
    "\n",
    "#first_second_person    \n",
    "print(\"Mediana primera persona \", np.median(first_second_person[:][0]))\n",
    "print(\"Mediana segunda persona \", np.median(first_second_person[:][1]))\n",
    "\n",
    "print(\"Media primera persona \", np.mean(first_second_person[:][0]))\n",
    "print(\"Media segunda persona \", np.mean(first_second_person[:][1]))\n",
    "\n",
    "print(\"Moda primera persona \", stats.mode(first_second_person[:][0]))\n",
    "print(\"Moda segunda persona \", stats.mode(first_second_person[:][1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de links encontrados  3280\n"
     ]
    }
   ],
   "source": [
    "pattern_http = re.compile(r'http://')\n",
    "\n",
    "num_links=0\n",
    "\n",
    "for text in corpus['text'][:]:\n",
    "    result = pattern_http.findall(text) \n",
    "    num_links+=len(result)\n",
    "\n",
    "print(\"Cantidad de links encontrados \", num_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~sebastiandaloia/0 or inside your plot.ly account where it is named 'mpl-basic-histogram'\n"
     ]
    }
   ],
   "source": [
    "# Learn about API authentication here: https://plot.ly/python/getting-started\n",
    "# Find your api_key here: https://plot.ly/settings/api\n",
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='sebastiandaloia', api_key='cMoSSDnwQHyafA8prIEv')\n",
    "\n",
    "votos_nums = tweet_id_vot[:,1]\n",
    "\n",
    "plt.hist(votos_nums)\n",
    "plt.title(\"Histograma de votos por tweet\")\n",
    "plt.xlabel(\"Votos\")\n",
    "plt.ylabel(\"Tweets\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "plot_url = plotly.plotly.plot_mpl(fig, filename='mpl-basic-histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sebastiandaloia/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='sebastiandaloia', api_key='cMoSSDnwQHyafA8prIEv')\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "labels = ['No humor','1 Estrella','2 Estrellas','3 Estrellas', '4 Estrellas', '5 Estrellas']\n",
    "values = [14131, 2960, 2421, 3274, 2541, 1616]\n",
    "\n",
    "trace = go.Pie(labels=labels, values=values)\n",
    "\n",
    "plotly.plotly.iplot([trace], filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tutorial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just plain boring\n",
      "entirely predictable and lacks energy\n",
      "no surprises and very few laughs\n",
      "very powerful\n",
      "the most fun film of the summer\n"
     ]
    }
   ],
   "source": [
    "#Creamos corpus\n",
    "lista = []\n",
    "lista.append(\"just plain boring\")\n",
    "lista.append(\"entirely predictable and lacks energy\")\n",
    "lista.append(\"no surprises and very few laughs\")\n",
    "lista.append(\"very powerful\")\n",
    "lista.append(\"the most fun film of the summer\")\n",
    "\n",
    "for d in lista:\n",
    "    print(d)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia numero  1\n",
      "Sentencia numero  2\n",
      "Sentencia numero  3\n",
      "Sentencia numero  4\n",
      "Sentencia numero  5\n",
      "just plain boring noun noun noun\n",
      "entirely predictable and lacks energy noun adjective noun noun adverb\n",
      "no surprises and very few laughs adverb adjective noun noun noun adjective\n",
      "very powerful noun adjective\n",
      "the most fun film of the summer adverb noun noun noun noun adverb adjective\n"
     ]
    }
   ],
   "source": [
    "#Tokenizamos\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "num_sentences=1\n",
    "pos=0\n",
    "for d in lista:\n",
    "    xml = analyzer.run(d.encode(), 'flush')\n",
    "    print(\"Sentencia numero \", num_sentences)\n",
    "    for sentence in xml:\n",
    "        for token in sentence:\n",
    "            m = re.search(pattern_pos, etree.tostring(token).decode())\n",
    "            lista[pos] = lista[pos] + \" \" + m.group(1)\n",
    "        num_sentences+=1\n",
    "        pos+=1\n",
    "\n",
    "for d in lista:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = vectorizer.fit_transform(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia numero  1\n",
      "predictable with no fun adjective noun adverb noun\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento\n",
    "test_list=[]\n",
    "test_list.append(\"predictable with no fun\")\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "num_sentences=1\n",
    "pos=0\n",
    "for d in test_list:\n",
    "    xml = analyzer.run(d.encode(), 'flush')\n",
    "    print(\"Sentencia numero \", num_sentences)\n",
    "    for sentence in xml:\n",
    "        for token in sentence:\n",
    "            m = re.search(pattern_pos, etree.tostring(token).decode())\n",
    "            test_list[pos] = test_list[pos] + \" \" + m.group(1)\n",
    "        num_sentences+=1\n",
    "        pos+=1\n",
    "\n",
    "for d in test_list:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = vectorizer.transform(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categ_list=np.array([0,0,0,1,1])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_features , categ_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
