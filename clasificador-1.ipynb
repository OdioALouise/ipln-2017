{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importación de librerias útiles\n",
    "import pandas\n",
    "#corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "\n",
    "#Librerias cientificas scipy, numpy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Lectura de archivos xml\n",
    "from lxml import etree\n",
    "\n",
    "#Expresiones regulares\n",
    "import re\n",
    "\n",
    "#Pyfreeling\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Para aplicar Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar comentarios con menos de tres votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Listas para salvar los tweets filtrados\n",
    "id_f = []\n",
    "text_f = []\n",
    "account_id_f=[]\n",
    "nh_f=[]\n",
    "sh1_f=[]\n",
    "sh2_f=[]\n",
    "sh3_f=[]\n",
    "sh4_f=[]\n",
    "sh5_f=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_id=corpus['id'][:]\n",
    "vec_text=corpus['text'][:]\n",
    "vec_account_id=corpus['account_id'][:]\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    if vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= 3:\n",
    "        id_f.append(vec_id[i])\n",
    "        text_f.append(vec_text[i])\n",
    "        account_id_f.append(vec_account_id[i])\n",
    "        nh_f.append(vec_no_humor[i])\n",
    "        sh1_f.append(vec_e1[i])\n",
    "        sh2_f.append(vec_e2[i])\n",
    "        sh3_f.append(vec_e3[i])\n",
    "        sh4_f.append(vec_e4[i])\n",
    "        sh5_f.append(vec_e5[i])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets con al menos tres votos 3438\n",
      "Tweets guardados en corpus_filtro1.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweets con al menos tres votos\", len(text_f))\n",
    "d = {'id' : id_f,\n",
    "    'text' : text_f,\n",
    "    'account_id': account_id_f,\n",
    "    'n':nh_f, \n",
    "    '1':sh1_f,\n",
    "    '2':sh2_f,\n",
    "    '3':sh3_f,\n",
    "    '4':sh4_f,\n",
    "    '5':sh5_f\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro1.csv')\n",
    "print(\"Tweets guardados en corpus_filtro1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La estructura con la que se va a trabajar ahora se levanta del archivo corpus_filtro1.cvs, que contiene solo los tweets con a los sumo tres votos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La siguiente actividad es eliminar Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_f = []\n",
    "\n",
    "corpus_filtro1 = pandas.read_csv(\"corpus_filtro1.csv\",encoding='utf-8')\n",
    "\n",
    "pattern_hashtag = re.compile(r'#.+?\\b')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(corpus_filtro1)):\n",
    "    text_f.append(re.sub(pattern_hashtag, \"\", corpus_filtro1['text'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets sin hashtags\n",
      "Tweets guardados en corpus_filtro2.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweets sin hashtags\")\n",
    "d = {'id' : corpus_filtro1['id'][:],\n",
    "    'text' : text_f,\n",
    "    'account_id': corpus_filtro1['account_id'][:],\n",
    "    'n':corpus_filtro1['n'][:], \n",
    "    '1':corpus_filtro1['1'][:],\n",
    "    '2':corpus_filtro1['2'][:],\n",
    "    '3':corpus_filtro1['3'][:],\n",
    "    '4':corpus_filtro1['4'][:],\n",
    "    '5':corpus_filtro1['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro2.csv')\n",
    "print(\"Tweets guardados en corpus_filtro2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La estructura con la que se va a trabajar ahora se levanta desde corpus_filtro2.csv. Esta estructura contiene los tweets que tienen al menos tres votos y se han eliminado del texto los hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A continuación se agregará al corpus la información de si es humorístico o no y manteniendo solo los campos 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro2 = pandas.read_csv(\"corpus_filtro2.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "text = []\n",
    "category=[]\n",
    "h1=[]\n",
    "h2=[]\n",
    "h3=[]\n",
    "h4=[]\n",
    "h5=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_text=corpus_filtro2['text'][:]\n",
    "vec_no_humor =corpus_filtro2['n'][:]\n",
    "vec_e1 =corpus_filtro2['1'][:]\n",
    "vec_e2 =corpus_filtro2['2'][:]\n",
    "vec_e3 =corpus_filtro2['3'][:]\n",
    "vec_e4 =corpus_filtro2['4'][:]\n",
    "vec_e5 =corpus_filtro2['5'][:]\n",
    "\n",
    "for i in range(len(corpus_filtro2)):\n",
    "    if  vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= vec_no_humor[i]:\n",
    "        category.append(1)\n",
    "    else:\n",
    "        category.append(0)\n",
    "    text.append(vec_text[i])\n",
    "    h1.append(vec_e1[i])\n",
    "    h2.append(vec_e2[i])\n",
    "    h3.append(vec_e3[i])\n",
    "    h4.append(vec_e4[i])\n",
    "    h5.append(vec_e5[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias\n",
      "Tweets guardados en corpus_filtro3.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias\")\n",
    "d = {'text' : text,\n",
    "    'category': category,\n",
    "     '1':h1,\n",
    "     '2':h2,\n",
    "     '3':h3,\n",
    "     '4':h4,\n",
    "     '5':h5\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro3.csv')\n",
    "print(\"Tweets guardados en corpus_filtro3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se agregará información de POS-tag al texto, para considerar las POS-tag como features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiste 1  137 \n",
      " Chiste 2 False \n",
      " Chiste 3 72\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro3 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro3['text'][:]\n",
    "\n",
    "\n",
    "print(\"Chiste 1 \",len(text_list[2903]), \"\\n Chiste 2\", not math.isnan(text_list[2904]), \"\\n Chiste 3\", len(text_list[2905]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3438   1 %%%% 0 %\r"
     ]
    }
   ],
   "source": [
    "corpus_filtro3 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro3['text'][:]\n",
    "\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "num_sentences=len(corpus_filtro3)\n",
    "numsentence=1\n",
    "pos=0\n",
    "for d in text_list:\n",
    "    if(type(d) == str):\n",
    "        xml = analyzer.run(d.encode(), 'flush')\n",
    "        print(numsentence, \" \", math.floor( numsentence/num_sentences*100),\"%\", end=\"\\r\")\n",
    "        for sentence in xml:        \n",
    "            for token in sentence:\n",
    "                token_byte=etree.tostring(token)\n",
    "                m = re.search(pattern_pos, token_byte.decode())\n",
    "                if m is not None:\n",
    "                    text_list[ pos] = text_list[pos] + \" \" + m.group(1)\n",
    "        #print(text_list[2903 + pos])\n",
    "    #else:\n",
    "    #    print(\"Es nan\")\n",
    "    pos+=1\n",
    "    numsentence+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias\n",
      "Tweets guardados en corpus_filtro4.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias\")\n",
    "d = {'text' : text_list,\n",
    "    'category': corpus_filtro3['category'][:],\n",
    "     '1':corpus_filtro3['1'][:],\n",
    "     '2':corpus_filtro3['2'][:],\n",
    "     '3':corpus_filtro3['3'][:],\n",
    "     '4':corpus_filtro3['4'][:],\n",
    "     '5':corpus_filtro3['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', '1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro4.csv')\n",
    "print(\"Tweets guardados en corpus_filtro4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de datos de entrenamiento y datos de desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "[1241 2566 1048 2353 2749 2111 1410 3291  205 3063 1622  274 1409 1370 2190\n",
      "  969  768 3230 1048 1467  171 2437 2480 1423 2485  551 1616 2287 2225  546\n",
      " 1292  164  186 2532 2625  867  947  451 3372 2746 2230 1531 3062 3096  709\n",
      " 1498 2892  376  741 1232 2092  494 3311 1917  459  937 2836 2046 1908 1135\n",
      " 1877 2306 2886 1843 2316 2843 3162 1778 2750 2472 2181 1126 2673 2528 2512\n",
      "  891  954  256 1621 3075   29  578 1024  490 2799 3266 2555  447 2960 1434\n",
      " 2571 1199  749 3212 1947 1719 3363 2031  601 2254 3190  964 1086 3034 1183\n",
      " 2746 1036  443 3090 1949 1627 2176 2249 1696  266 3299  187  591 1241  827\n",
      "  325 3425 2483 2341  869  668 2415  501 1664   57 1274 2939 2719 2836 2220\n",
      "  724 2954 1192 2196   14 3283  375  544 1872 2574 1510 2111  104 1279 1269\n",
      " 2949  911  912 1112 2904  757 3138 2916 2535  199 1127  256 3282 3328  705\n",
      " 1685 1443  397 2518 1636   33 2268  748  978  851 1905  800 2904  531  426\n",
      " 1730 2632 2952  275 1836 2636  124 1258  815  197 1695 1069 1963   54 1216\n",
      "  367 2889 1367 2274  890 1456 2839 1276  719  804 2850 1088   52  746 3259\n",
      " 1769  261 1739 1712 2372 2609 3404 2467 2575 2100 3194 3419  283  944 1458\n",
      " 1262 1526 2653 2822 3071 2181 1296  128 2293 2183  932 2224 1016 1175  217\n",
      " 2139  599  882 2891 2493 2211  710 1295  600 2508   97 2652 2179  230 2986\n",
      "   28 1024 2189 1471  118 1871  768 2741  874 3162  685  506 2196 1990 1286\n",
      "  971  932  398 2513  688 2113  959  956 1896 3133 1580 2321 1187 3264 2668\n",
      "  893 3199  676 2840 1842 3041  601 2177 1589  483 2957 2546 1763 2559 3345\n",
      " 2914 2126  512  935 1372 2716 2506 2952 1438 3020 1034 1914  437 2807  507\n",
      "  419 2449 1971  239 1890 2169 1117 1149  674 2872 1586 3145  108 2409  359\n",
      "  404 1968 2848 1372  948 2556 2730 2156 2945 1201 2149  224 3008 2266 3331\n",
      "  140 1877 1161 1170 1948 3094  666  769 3423 2254 1683 1168 1785  631 2822\n",
      "  360 2157 3057 1397 1497  886 1794  239  770 1796 2624 2968 2697  837 2854\n",
      " 2602 2736 3220 1182  487 1398 2271 2203 2635 2861  342 3181 1177 1996 2343\n",
      "  646 1347 2451 1310  863 3382 3082 1567 1297  181 2887  775  388 1320 3009\n",
      " 3216 2206 1904 1062 2281 1295 1935 1272 2959   93 3235 1877  893  366 3198\n",
      "  431  673 2120  744 2730 1601  872 2678 1581 1619 1247 2093 1521  838  708\n",
      " 2456  924 2275 2756   68 2010 3079 1818 3119 2818 2779  938 1745 2386  944\n",
      " 1974 1435 2832 2324 3204  198 1367 2254 3215 3297  614 2972  415 3125 2315\n",
      "  289  732 2540 1672 1776 1763   70 3420 2485  376 1096  637  805 1299 2473\n",
      " 1720 2214 2347 2439 1395 2015 3221  580  314 1559  820 1448 1269  808 1321\n",
      " 1445  547 2774 2154  933 1262 2332 1202 2773 1513  565 1216  409  463 1805\n",
      "  894  634  326 1804 1864 2296  621 1751 2418  903 3196 2138 3325  770 2549\n",
      " 2647 2410  699 1613  548  738  863  677 3103 2104 2574 2060 3386 1449 1519\n",
      " 1732 2873 3301   22 3002  784 2411 1762 2827 1569 2542  321 1540 3159  524\n",
      " 2170  496 3252  303  168 2552   62 2825 2028 2429  158 2070 2660 1693 3142\n",
      " 1039 1126 3013 1020 1011   86 3152 2479 2571 1197 2181 3030 3350  820 2041\n",
      " 3381 1061 1268 2277 2442 3327 3373  382  592 1642 1779 2407 2454  628 3106\n",
      " 3412 1092 2471 2756 1936 2943 2453  809 1923 1622  386 2104 1406 2913 1884\n",
      " 2628 1519 2676 2528 2347 2373 1396  242 2553  166  906 3323 1290 2846 1927\n",
      " 1001 1602 1337 3175 1015 2640  210   93 2068 3081  136 1272 3325  809 2825\n",
      " 1085 2124 1068 1240  508 1702  178 1856  792 2476  803 1146 2719 2128  468\n",
      " 1982  634 3307 2877 1180 1715 1135 1392  598 1000 1886  869 3211  651 3082\n",
      " 3070 2169 2310 1419 1329 3256 1283  672 3044  986 3055 1246]\n",
      "Cantidad de tweets para entrenamiento 2808\n",
      "Cantidad de tweets para desarrollo 630\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro4.csv\",encoding='utf-8')\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4) - 1, size=num_dev_tweets)\n",
    "\n",
    "print(num_dev_tweets)\n",
    "print(random_samples)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4['text'][:]\n",
    "vec_category=corpus_filtro4['category'][:]\n",
    "\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "        \n",
    "print(\"Cantidad de tweets para entrenamiento\", len(text_train))\n",
    "print(\"Cantidad de tweets para desarrollo\", len(text_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias para los conjuntos de entrenamiento y desarrollo\n",
      "Tweets guardados en corpus_filtro5_trainingset.csv\n",
      "Tweets guardados en corpus_filtro5_devset.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias para los conjuntos de entrenamiento y desarrollo\")\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset.csv')\n",
    "print(\"Tweets guardados en corpus_filtro5_trainingset.csv\")\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset.csv')\n",
    "print(\"Tweets guardados en corpus_filtro5_devset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion de tweets naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro5_trainingset = pandas.read_csv(\"corpus_filtro5_trainingset.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro5_trainingset['text'][:]\n",
    "category_list=corpus_filtro5_trainingset['category'][:]\n",
    "pos=0\n",
    "for t in text_list:\n",
    "    if type(t) != str:\n",
    "        text_list[pos]=\"NaN\"\n",
    "    pos+=1\n",
    "train_features = vectorizer.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filtro5_devset = pandas.read_csv(\"corpus_filtro5_devset.csv\",encoding='utf-8')\n",
    "text_devlist=corpus_filtro5_devset['text'][:]\n",
    "category_devlist=corpus_filtro5_devset['category'][:]\n",
    "pos=0\n",
    "for t in text_devlist:\n",
    "    if type(t) != str:\n",
    "        text_devlist[pos]=\"NaN\"\n",
    "    pos+=1\n",
    "test_features = vectorizer.transform(text_devlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_features , category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "print(predictions)\n",
    "#nb.predict_proba(test_features)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
