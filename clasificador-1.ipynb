{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importación de librerias útiles\n",
    "import pandas\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "\n",
    "#Librerias cientificas scipy, numpy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Lectura de archivos xml\n",
    "from lxml import etree\n",
    "\n",
    "#Expresiones regulares\n",
    "import re\n",
    "\n",
    "#Pyfreeling\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Para aplicar Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar comentarios con menos de tres votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Listas para salvar los tweets filtrados\n",
    "id_f = []\n",
    "text_f = []\n",
    "account_id_f=[]\n",
    "nh_f=[]\n",
    "sh1_f=[]\n",
    "sh2_f=[]\n",
    "sh3_f=[]\n",
    "sh4_f=[]\n",
    "sh5_f=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_id=corpus['id'][:]\n",
    "vec_text=corpus['text'][:]\n",
    "vec_account_id=corpus['account_id'][:]\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    if vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= 3:\n",
    "        id_f.append(vec_id[i])\n",
    "        text_f.append(vec_text[i])\n",
    "        account_id_f.append(vec_account_id[i])\n",
    "        nh_f.append(vec_no_humor[i])\n",
    "        sh1_f.append(vec_e1[i])\n",
    "        sh2_f.append(vec_e2[i])\n",
    "        sh3_f.append(vec_e3[i])\n",
    "        sh4_f.append(vec_e4[i])\n",
    "        sh5_f.append(vec_e5[i])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets con al menos tres votos 3438\n",
      "Tweets guardados en corpus_filtro1.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweets con al menos tres votos\", len(text_f))\n",
    "d = {'id' : id_f,\n",
    "    'text' : text_f,\n",
    "    'account_id': account_id_f,\n",
    "    'n':nh_f, \n",
    "    '1':sh1_f,\n",
    "    '2':sh2_f,\n",
    "    '3':sh3_f,\n",
    "    '4':sh4_f,\n",
    "    '5':sh5_f\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro1.csv')\n",
    "print(\"Tweets guardados en corpus_filtro1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La estructura con la que se va a trabajar ahora se levanta del archivo corpus_filtro1.cvs, que contiene solo los tweets con a los sumo tres votos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La siguiente actividad es eliminar Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_f = []\n",
    "\n",
    "corpus_filtro1 = pandas.read_csv(\"corpus_filtro1.csv\",encoding='utf-8')\n",
    "\n",
    "pattern_hashtag = re.compile(r'#.+?\\b')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(corpus_filtro1)):\n",
    "    text_f.append(re.sub(pattern_hashtag, \"\", corpus_filtro1['text'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets sin hashtags\n",
      "Tweets guardados en corpus_filtro2.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweets sin hashtags\")\n",
    "d = {'id' : corpus_filtro1['id'][:],\n",
    "    'text' : text_f,\n",
    "    'account_id': corpus_filtro1['account_id'][:],\n",
    "    'n':corpus_filtro1['n'][:], \n",
    "    '1':corpus_filtro1['1'][:],\n",
    "    '2':corpus_filtro1['2'][:],\n",
    "    '3':corpus_filtro1['3'][:],\n",
    "    '4':corpus_filtro1['4'][:],\n",
    "    '5':corpus_filtro1['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro2.csv')\n",
    "print(\"Tweets guardados en corpus_filtro2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La estructura con la que se va a trabajar ahora se levanta desde corpus_filtro2.csv. Esta estructura contiene los tweets que tienen al menos tres votos y se han eliminado del texto los hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A continuación se agregará al corpus la información de si es humorístico o no y manteniendo solo los campos 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro2 = pandas.read_csv(\"corpus_filtro2.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "text = []\n",
    "category=[]\n",
    "h1=[]\n",
    "h2=[]\n",
    "h3=[]\n",
    "h4=[]\n",
    "h5=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_text=corpus_filtro2['text'][:]\n",
    "vec_no_humor =corpus_filtro2['n'][:]\n",
    "vec_e1 =corpus_filtro2['1'][:]\n",
    "vec_e2 =corpus_filtro2['2'][:]\n",
    "vec_e3 =corpus_filtro2['3'][:]\n",
    "vec_e4 =corpus_filtro2['4'][:]\n",
    "vec_e5 =corpus_filtro2['5'][:]\n",
    "\n",
    "for i in range(len(corpus_filtro2)):\n",
    "    if  vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= vec_no_humor[i]:\n",
    "        category.append(1)\n",
    "    else:\n",
    "        category.append(0)\n",
    "    text.append(vec_text[i])\n",
    "    h1.append(vec_e1[i])\n",
    "    h2.append(vec_e2[i])\n",
    "    h3.append(vec_e3[i])\n",
    "    h4.append(vec_e4[i])\n",
    "    h5.append(vec_e5[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias\n",
      "Tweets guardados en corpus_filtro3.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias\")\n",
    "d = {'text' : text,\n",
    "    'category': category,\n",
    "     '1':h1,\n",
    "     '2':h2,\n",
    "     '3':h3,\n",
    "     '4':h4,\n",
    "     '5':h5\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro3.csv')\n",
    "print(\"Tweets guardados en corpus_filtro3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se agregará información de POS-tag al texto, para considerar las POS-tag como features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\r"
     ]
    }
   ],
   "source": [
    "corpus_filtro3 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro3['text'][:]\n",
    "\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "\n",
    "numsentence=1\n",
    "pos=0\n",
    "for d in text_list:\n",
    "    xml = analyzer.run(d.encode(), 'flush')\n",
    "    print(numsentence, end=\"\\r\")\n",
    "    for sentence in xml:        \n",
    "        for token in sentence:\n",
    "            m = re.search(pattern_pos, etree.tostring(token).decode())\n",
    "            if m is not None:\n",
    "                text_list[pos] = text_list[pos] + \" \" + m.group(1)\n",
    "    pos+=1\n",
    "    numsentence+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias\n",
      "Tweets guardados en corpus_filtro4.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias\")\n",
    "d = {'text' : text_list,\n",
    "    'category': category[:5]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro4.csv')\n",
    "print(\"Tweets guardados en corpus_filtro4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de datos de entrenamiento y datos de desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0]\n",
      "Cantidad de tweets para entrenamiento 4\n",
      "Cantidad de tweets para desarrollo 1\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro4.csv\",encoding='utf-8')\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4) - 1, size=num_dev_tweets)\n",
    "\n",
    "print(num_dev_tweets)\n",
    "print(random_samples)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4['text'][:5]\n",
    "vec_category=corpus_filtro4['category'][:5]\n",
    "\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "        \n",
    "print(\"Cantidad de tweets para entrenamiento\", len(text_train))\n",
    "print(\"Cantidad de tweets para desarrollo\", len(text_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias para los conjuntos de entrenamiento y desarrollo\n",
      "Tweets guardados en corpus_filtro5_trainingset.csv\n",
      "Tweets guardados en corpus_filtro5_devset.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias para los conjuntos de entrenamiento y desarrollo\")\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset.csv')\n",
    "print(\"Tweets guardados en corpus_filtro5_trainingset.csv\")\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset.csv')\n",
    "print(\"Tweets guardados en corpus_filtro5_devset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion de tweets naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro5_trainingset = pandas.read_csv(\"corpus_filtro5_trainingset.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro5_trainingset['text'][:]\n",
    "category_list=corpus_filtro5_trainingset['category'][:]\n",
    "train_features = vectorizer.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro5_devset = pandas.read_csv(\"corpus_filtro5_devset.csv\",encoding='utf-8')\n",
    "text_devlist=corpus_filtro5_devset['text'][:]\n",
    "category_devlist=corpus_filtro5_devset['category'][:]\n",
    "test_features = vectorizer.transform(text_devlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_features , category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02213721,  0.97786279]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "print(predictions)\n",
    "nb.predict_proba(test_features)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
