{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importación de librerias útiles\n",
    "import pandas\n",
    "corpus_filtro4median = pandas.read_csv(\"corpus_filtro4median.csv\",encoding='utf-8')\n",
    "\n",
    "#Librerias cientificas scipy, numpy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Lectura de archivos xml\n",
    "from lxml import etree\n",
    "\n",
    "#Expresiones regulares\n",
    "import re\n",
    "\n",
    "#Pyfreeling\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Para aplicar Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=corpus_filtro4median['text'][:]\n",
    "category_list=[]\n",
    "median_list=corpus_filtro4median['median'][:]\n",
    "h1_list=corpus_filtro4median['1'][:]\n",
    "h2_list=corpus_filtro4median['2'][:]\n",
    "h3_list=corpus_filtro4median['3'][:]\n",
    "h4_list=corpus_filtro4median['4'][:]\n",
    "h5_list=corpus_filtro4median['5'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=0\n",
    "for m in median_list:\n",
    "    if(m<1):\n",
    "        category_list.append(0)\n",
    "    else:\n",
    "        category_list.append(1)\n",
    "    pos+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3438   3438   3438\n"
     ]
    }
   ],
   "source": [
    "print(len(category_list), \" \", len(text_list), \" \", len(median_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias\n",
      "Tweets guardados en corpus_filtro4median_category.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias\")\n",
    "d = {'text' : text_list,\n",
    "    'category': category_list,\n",
    "     'median': median_list\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','median'])\n",
    "df.to_csv('corpus_filtro4median_category.csv')\n",
    "print(\"Tweets guardados en corpus_filtro4median_category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4median_category = pandas.read_csv(\"corpus_filtro4median_category.csv\",encoding='utf-8')\n",
    "text=corpus_filtro4median_category['text'][:]\n",
    "category=corpus_filtro4median_category['category'][:]\n",
    "median=corpus_filtro4median_category['median'][:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separacion de datos de entrenamiento y desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "[ 833 2177 3214  140 2393 2157  648 1281 3240 1231 1712 3388 3061 3032  920\n",
      " 2800 3294  231  654 1809 2595 1164 1297   34 1260 2386  923 3398  411 3350\n",
      " 2129  186 1651 1236 1564 2035 3149 2904 1828 2589 1667 1865 1937 1044 1459\n",
      " 1607 2671 3344 2849  917  631  997 2037  743 2987  863 1708 2800 2675 1108\n",
      "   27 1148 1682  273 2407 3215  180 3186 1079  761 1582  840 2877 2418  778\n",
      " 3162  979 1243  270 1107  245 2028 1508 1852  917  274 2204 1089 3121 1381\n",
      " 2721 2043 1764  452 1184 2773 1734 3269 3080 2251  955  640  803 1507  641\n",
      " 1931  382  453 2689 2575   39 2606  982 2935  531  555 1131 1565 2284 2815\n",
      " 1696 1043  238  263 2416 1595 1875  962  899 3061 2087  308 1195 2073   11\n",
      " 3430 2431 1303 2735 2213   50 2591  136 2127 2798 1613 1028 2809  148 1240\n",
      " 1280 1480 1571 1808 3054  879 2896  508  304  288  424 3217 1101  726  203\n",
      "  542  407  568  653 3061 3026  735 3313 1760  391 1737  127  159 1339 2688\n",
      " 3147  484 3414 2884 2260 1390 3240 2859  951 2413 1051 1915 1441  364 1610\n",
      " 1153  567 2819 3078 2425 2715 2968 1070 2051 2172 3266 3311 1742 2019 1798\n",
      "  219 2640  670 2717 1322  247 1838 1809 1934  174  235 3058 3371 1662 1895\n",
      " 1659  451 1229 3000  647 2890 1977 1486 3142 1356 3182 3319 2066 2198  382\n",
      " 2911  586 2065 1158 2799 1684 1596 1629 3219  387 3076  160 2463 1326 1178\n",
      "  414 2028 3392 2454 3307 2249  887 1225 1374 1089  750 1905  775 3368 2013\n",
      " 3131 2643  505 3033 2772 2243 3104 2636 3022  407 1599 3108 1221 3199  589\n",
      "  272 2359 1312  461 3433 2556 1233  585 2325  439 1009 2078   83 2831 2307\n",
      "  538 2840 2062 2076 1243  616 1561 1531 2055 1750  296 1648 1343 2202 2706\n",
      " 3362 1249  773  849 2415  517 1374 2550 2713 3188  732   17  751 2361 1520\n",
      " 2903  313 3271 1330 2427  674 1886  793  610 1171 2710  735 1845 1914 2964\n",
      " 2023 1531 2489 1492  949  671  516 2958  141  738 1291  910  183 1529 2527\n",
      "  638 2017 3306 1823 1207 2914 2954 3187 2505 2352 1392   66 2483 1484 2610\n",
      "  395 1677 2504 1340 2398 2120 1072 1995 2857 1878 2050 2912  934 3299 3397\n",
      " 1723 1197 2259 1059 3059  262  587 1260  193 3180 2327 1349  129  477 3167\n",
      " 1337 3245  419  442 1819  513  231 1519 1826 1988 1861 2038 3156 1600 2072\n",
      "  781  755 2996 1189 3076 1641 1015 1709 2722 2467 1128 2596 3051 1101 1916\n",
      "  431 3316 2588 3332 3293 2496 2386  576 2310 2113 1229 3373 1219 2096 2119\n",
      "  295 2041 2836 1634   71  254 2939 1120 1211 2893  982 1367  588 1755 2092\n",
      " 1859 3015 3387 1013  187  757 2497 1275  771 1643 2775 3342  501 2788 2530\n",
      "   63 1197  287 3360 3303 1963  571 3408 2847 1342  707 1946 3213 2432 2670\n",
      " 3045  181  314 2646  485  626  484 1521 2754  194  131 3052  566  384 1830\n",
      " 1678 3125  914 2036 1220  522 3282 2466  751 1938 1882  269 1783  408  136\n",
      " 1940 2118 1289  475 2825 1048 2715 2097 1406 2028 1309 3353 1492 2736  466\n",
      " 1178 3427  873  962 3386  216 2778 2503 3337  522  605 1483  306 2747 1988\n",
      " 2442  920 1587 1211 1767 3085 3167 1549  861 3346 2683 1968  508  900 1262\n",
      "  603 3142 2750 2634 3295 2808 1258 1334 2456 2056 1244 1983 2736  878 1114\n",
      "  861 2680 1875 3072 1197 1792 1957 1300 2597 1125 1600  375  568  914 2038\n",
      " 2064 2667 1878 1355 2115  604 2210  911 2927 2334   31 1031 3385 1708 2591\n",
      " 1519 2545 2522  577 2578   33 1488  410 1480 2778 1239 1100 2254 1215 1859\n",
      "  925 3317 2513 2369 2946 3031 1797 2879 3126 1421 2492  323 1515 1615 1048\n",
      "  330 1618 1388 1881 1270 2811  581 3178 2657 2284  633  947  339 1337 2025\n",
      "   74 2814 1358  250 3037 2803 2727 1997 2937 2966  890  426  840  339 2352\n",
      "  491 3163 3313 2081  296 1372 3126 3409 1269  307  104 2242]\n",
      "Cantidad de tweets para entrenamiento 2812\n",
      "Cantidad de tweets para desarrollo 626\n"
     ]
    }
   ],
   "source": [
    "num_dev_tweets=math.floor(len(corpus_filtro4median_category)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4median_category) - 1, size=num_dev_tweets)\n",
    "\n",
    "print(num_dev_tweets)\n",
    "print(random_samples)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4median_category['text'][:]\n",
    "vec_category=corpus_filtro4median_category['category'][:]\n",
    "\n",
    "for i in range( len(corpus_filtro4median_category)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "        \n",
    "print(\"Cantidad de tweets para entrenamiento\", len(text_train))\n",
    "print(\"Cantidad de tweets para desarrollo\", len(text_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus y categorias para los conjuntos de entrenamiento y desarrollo\n",
      "Tweets guardados en corpus_filtro5_trainingset_c3.csv\n",
      "Tweets guardados en corpus_filtro5_devset_c3.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando corpus y categorias para los conjuntos de entrenamiento y desarrollo\")\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset_c3.csv')\n",
    "print(\"Tweets guardados en corpus_filtro5_trainingset_c3.csv\")\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset_c3.csv')\n",
    "print(\"Tweets guardados en corpus_filtro5_devset_c3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro5_trainingset_c3 = pandas.read_csv(\"corpus_filtro5_trainingset_c3.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro5_trainingset_c3['text'][:]\n",
    "category_list=corpus_filtro5_trainingset_c3['category'][:]\n",
    "pos=0\n",
    "for t in text_list:\n",
    "    if type(t) != str:\n",
    "        text_list[pos]=\"NaN\"\n",
    "    pos+=1\n",
    "train_features = vectorizer.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro5_devset_c3 = pandas.read_csv(\"corpus_filtro5_devset_c3.csv\",encoding='utf-8')\n",
    "text_devlist=corpus_filtro5_devset_c3['text'][:]\n",
    "category_devlist=corpus_filtro5_devset_c3['category'][:]\n",
    "pos=0\n",
    "for t in text_devlist:\n",
    "    if type(t) != str:\n",
    "        text_devlist[pos]=\"NaN\"\n",
    "    pos+=1\n",
    "test_features = vectorizer.transform(text_devlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_features , category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
