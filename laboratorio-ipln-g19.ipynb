{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importacion de librerias utiles\n",
    "\n",
    "#Libreria para analisis de datos \n",
    "#en este proyecto para leer y grabar\n",
    "#archivos en csv\n",
    "import pandas\n",
    "\n",
    "#Librerias cientificas scipy, numpy, matplotlib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Librerias de incluidas en python time, math\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Lectura de archivos xml\n",
    "from lxml import etree\n",
    "\n",
    "#Expresiones regulares\n",
    "import re\n",
    "\n",
    "#Pyfreeling\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Para implementar modelos de aprendizajes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar comentarios con menos de tres votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga el corpus de humor\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "id_f = []\n",
    "text_f = []\n",
    "account_id_f=[]\n",
    "nh_f=[]\n",
    "sh1_f=[]\n",
    "sh2_f=[]\n",
    "sh3_f=[]\n",
    "sh4_f=[]\n",
    "sh5_f=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_id=corpus['id'][:]\n",
    "vec_text=corpus['text'][:]\n",
    "vec_account_id=corpus['account_id'][:]\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "#Para cada tweet se calcula si tiene al menos tres votos, \n",
    "#en caso afirmativo se guarda su informacion, en caso\n",
    "#negativo se descarta el tweet\n",
    "for i in range(len(corpus)):\n",
    "    if vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= 3:\n",
    "        id_f.append(vec_id[i])\n",
    "        text_f.append(vec_text[i])\n",
    "        account_id_f.append(vec_account_id[i])\n",
    "        nh_f.append(vec_no_humor[i])\n",
    "        sh1_f.append(vec_e1[i])\n",
    "        sh2_f.append(vec_e2[i])\n",
    "        sh3_f.append(vec_e3[i])\n",
    "        sh4_f.append(vec_e4[i])\n",
    "        sh5_f.append(vec_e5[i])\n",
    "        \n",
    "#El primer filtro aplicado es guardado en el archivo corpus_filtro1.csv\n",
    "d = {'id' : id_f,\n",
    "    'text' : text_f,\n",
    "    'account_id': account_id_f,\n",
    "    'n':nh_f, \n",
    "    '1':sh1_f,\n",
    "    '2':sh2_f,\n",
    "    '3':sh3_f,\n",
    "    '4':sh4_f,\n",
    "    '5':sh5_f\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_f = []\n",
    "\n",
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro1 = pandas.read_csv(\"corpus_filtro1.csv\",encoding='utf-8')\n",
    "\n",
    "#Generacion de patron de Hashtag\n",
    "pattern_hashtag = re.compile(r'#.+?\\b')\n",
    "\n",
    "\n",
    "#Se sustituye el Hashtag por el string vacio, los tweets que\n",
    "#van pasando por este procesamiento se van guardando en el arreglo\n",
    "#text_f\n",
    "for i in range(len(corpus_filtro1)):\n",
    "    text_f.append(re.sub(pattern_hashtag, \"\", corpus_filtro1['text'][i]))\n",
    "#El segundo filtro aplicado es guardado en el archivo corpus_filtro2.csv\n",
    "d = {'id' : corpus_filtro1['id'][:],\n",
    "    'text' : text_f,\n",
    "    'account_id': corpus_filtro1['account_id'][:],\n",
    "    'n':corpus_filtro1['n'][:], \n",
    "    '1':corpus_filtro1['1'][:],\n",
    "    '2':corpus_filtro1['2'][:],\n",
    "    '3':corpus_filtro1['3'][:],\n",
    "    '4':corpus_filtro1['4'][:],\n",
    "    '5':corpus_filtro1['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregar la categoria humor o no humor a los documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El criterio de humor o no humor esta basado en el criterio presentado en la letra del laboratorio, en donde se considerará humorístico si la mitad o más de los anotadores lo calificaron con una o más estrellas y no humorístico en caso contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro2 = pandas.read_csv(\"corpus_filtro2.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "text = []\n",
    "category=[]\n",
    "h1=[]\n",
    "h2=[]\n",
    "h3=[]\n",
    "h4=[]\n",
    "h5=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_text=corpus_filtro2['text'][:]\n",
    "vec_no_humor =corpus_filtro2['n'][:]\n",
    "vec_e1 =corpus_filtro2['1'][:]\n",
    "vec_e2 =corpus_filtro2['2'][:]\n",
    "vec_e3 =corpus_filtro2['3'][:]\n",
    "vec_e4 =corpus_filtro2['4'][:]\n",
    "vec_e5 =corpus_filtro2['5'][:]\n",
    "\n",
    "#Si hay igual o mas votos de humor que de no humor la categoria tiene el valor 1\n",
    "#en caso contrario tiene el valor 0\n",
    "for i in range(len(corpus_filtro2)):\n",
    "    if  vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= vec_no_humor[i]:\n",
    "        category.append(1)\n",
    "    else:\n",
    "        category.append(0)\n",
    "    text.append(vec_text[i])\n",
    "    h1.append(vec_e1[i])\n",
    "    h2.append(vec_e2[i])\n",
    "    h3.append(vec_e3[i])\n",
    "    h4.append(vec_e4[i])\n",
    "    h5.append(vec_e5[i])\n",
    "#El tercer procesamiento aplicado sobre los tweets es guardado en el archivo corpus_filtro.3\n",
    "d = {'text' : text,\n",
    "    'category': category,\n",
    "     '1':h1,\n",
    "     '2':h2,\n",
    "     '3':h3,\n",
    "     '4':h4,\n",
    "     '5':h5\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar información de POS-tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro3 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro3['text'][:]\n",
    "\n",
    "#Se genera el patron para identificar las POS de las palabras\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "\n",
    "#Variables para datos estadisticos\n",
    "num_sentences=len(corpus_filtro3)\n",
    "numsentence=1\n",
    "\n",
    "#Variable auxiliar para adherir las POS a los tweets\n",
    "pos=0\n",
    "\n",
    "#Para cada documento en el corpus de tweets que va siendo procesado\n",
    "#utilizando la herramienta freeling se analizan las categorias gramaticales\n",
    "#de todas las palabras que aparecen en cada tweet y esta informacion es\n",
    "#adherida al texto del tweet\n",
    "for d in text_list:\n",
    "    if(type(d) == str):\n",
    "        xml = analyzer.run(d.encode(), 'flush')\n",
    "        print(numsentence, \" \", math.floor( numsentence/num_sentences*100),\"%\", end=\"\\r\")\n",
    "        for sentence in xml:        \n",
    "            for token in sentence:\n",
    "                token_byte=etree.tostring(token)\n",
    "                m = re.search(pattern_pos, token_byte.decode())\n",
    "                if m is not None:\n",
    "                    text_list[ pos] = text_list[pos] + \" \" + m.group(1)\n",
    "    pos+=1\n",
    "    numsentence+=1\n",
    "\n",
    "#El procesamiento actual del corpus es guardado en el documento corpus_filtro4.csv\n",
    "d = {'text' : text_list,\n",
    "    'category': corpus_filtro3['category'][:],\n",
    "     '1':corpus_filtro3['1'][:],\n",
    "     '2':corpus_filtro3['2'][:],\n",
    "     '3':corpus_filtro3['3'][:],\n",
    "     '4':corpus_filtro3['4'][:],\n",
    "     '5':corpus_filtro3['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', '1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de datos de entrenamiento y de desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro4.csv\",encoding='utf-8')\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4)*20/100)\n",
    "#Se generan posiciones en el arreglo tweets al azar que representen el 20% del corpus actual\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4) - 1, size=num_dev_tweets)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4['text'][:]\n",
    "vec_category=corpus_filtro4['category'][:]\n",
    "\n",
    "#Se separa el 20% de corpus para desarrollo del 80% para entrenamiento\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "\n",
    "\n",
    "#Se guardan las instancias corpus de desarrollo en corpus_filtro5_devset.csv\n",
    "#y corpus de entrenamiento en corpus_filtro5_trainingset.csv\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset.csv')\n",
    "\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de mediana, para segundo clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia salvada del corpus que se encuentra en el archivo\n",
    "#corpus_filtro4.csv\n",
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro4.csv\",encoding='utf-8')\n",
    "\n",
    "text_list=corpus_filtro4['text'][:]\n",
    "category_list=corpus_filtro4['category'][:]\n",
    "\n",
    "h1_list=corpus_filtro4['1'][:]\n",
    "h2_list=corpus_filtro4['2'][:]\n",
    "h3_list=corpus_filtro4['3'][:]\n",
    "h4_list=corpus_filtro4['4'][:]\n",
    "h5_list=corpus_filtro4['5'][:]\n",
    "\n",
    "medians=[]\n",
    "pos=0\n",
    "\n",
    "#Caclulo de la mediana, en values se expanden los votos de cada estrella\n",
    "#de la siguiente manera si #cant_3Estrellas_en_tweet = 4, entonces se guardan\n",
    "#en values 4 treses 3,3,3,3 para permitir el calculo de la mediana\n",
    "#Por ultimo en la estructura medians se guarda el valor discreto (0,1,2,3,4 o 5) de la mediana\n",
    "#si la cantidad de votos es par se toma el voto del medio con valor mas grande\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    values = []\n",
    "    for j in range( h1_list[i]):\n",
    "        values.append(1)\n",
    "    for j in range( h2_list[i] ):\n",
    "        values.append(2)\n",
    "    for j in range( h3_list[i] ):\n",
    "        values.append(3)\n",
    "    for j in range( h4_list[i] ):\n",
    "        values.append(4)\n",
    "    for j in range( h5_list[i] ):\n",
    "        values.append(5)\n",
    "    if len(values) == 0:\n",
    "        values.append(0)  \n",
    "    mediana=np.median(values)\n",
    "    if( len(values) % 2 == 1 ):\n",
    "        medians.append(math.floor(mediana))\n",
    "    else:\n",
    "        for i in values:\n",
    "            if( i >= mediana):\n",
    "                medians.append(i)\n",
    "                break\n",
    "    pos+=1\n",
    "\n",
    "#El nuevo procesamiento que suma la informacion de la mediana se guarda\n",
    "#en el archivo corpus_filtro4median.csv\n",
    "d = {'text' : corpus_filtro4['text'][:],\n",
    "    'category' : corpus_filtro4['category'][:],\n",
    "    'median': medians,\n",
    "    '1':corpus_filtro4['1'][:],\n",
    "    '2':corpus_filtro4['2'][:],\n",
    "    '3':corpus_filtro4['3'][:],\n",
    "    '4':corpus_filtro4['4'][:],\n",
    "    '5':corpus_filtro4['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', 'median', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro4median.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion de datos de entrenamiento y desarrollo, para segundo clasificador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Se carga la instancia salvada en el bloque de codigo anterior\n",
    "corpus_filtro4median = pandas.read_csv(\"corpus_filtro4median.csv\",encoding='utf-8')\n",
    "\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4median)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4median) - 1, size=num_dev_tweets)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "median_train=[]\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "median_dev=[]\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4median['text'][:]\n",
    "vec_median=corpus_filtro4median['median'][:]\n",
    "vec_category=corpus_filtro4median['category'][:]\n",
    "\n",
    "\n",
    "for i in range( len(corpus_filtro4median)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        median_dev.append(vec_median[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "        \n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        median_train.append(vec_median[i])\n",
    "        category_train.append(vec_category[i])\n",
    "\n",
    "d = {'text' : text_train,\n",
    "     'median' : median_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'median','category'])\n",
    "df.to_csv('corpus_filtro5_trainingset_c2.csv')\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "     'median': median_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'median', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset_c2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento, clasificador 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4median = pandas.read_csv(\"corpus_filtro4median.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro4median['text'][:]\n",
    "category_list=[]\n",
    "median_list=corpus_filtro4median['median'][:]\n",
    "h1_list=corpus_filtro4median['1'][:]\n",
    "h2_list=corpus_filtro4median['2'][:]\n",
    "h3_list=corpus_filtro4median['3'][:]\n",
    "h4_list=corpus_filtro4median['4'][:]\n",
    "h5_list=corpus_filtro4median['5'][:]\n",
    "pos=0\n",
    "for m in median_list:\n",
    "    if(m<1):\n",
    "        category_list.append(0)\n",
    "    else:\n",
    "        category_list.append(1)\n",
    "    pos+=1\n",
    "d = {'text' : text_list,\n",
    "    'category': category_list,\n",
    "     'median': median_list\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','median'])\n",
    "df.to_csv('corpus_filtro4median_category.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación datos de entrenamiento y desarrollo, clasificador 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4median_category = pandas.read_csv(\"corpus_filtro4median_category.csv\",encoding='utf-8')\n",
    "text=corpus_filtro4median_category['text'][:]\n",
    "category=corpus_filtro4median_category['category'][:]\n",
    "median=corpus_filtro4median_category['median'][:]\n",
    "\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4median_category)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4median_category) - 1, size=num_dev_tweets)\n",
    "\n",
    "print(num_dev_tweets)\n",
    "print(random_samples)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4median_category['text'][:]\n",
    "vec_category=corpus_filtro4median_category['category'][:]\n",
    "\n",
    "for i in range( len(corpus_filtro4median_category)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset_c3.csv')\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset_c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import pandas\n",
    "\n",
    "class Clasificador1:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        self.__clasiffier_1_train()\n",
    "    \n",
    "    def __clasiffier_1_train(self):\n",
    "        corpus_filtro5_trainingset = pandas.read_csv(\"corpus_filtro5_trainingset.csv\",encoding='utf-8')\n",
    "        text_list=corpus_filtro5_trainingset['text'][:]\n",
    "        category_list=corpus_filtro5_trainingset['category'][:]\n",
    "        pos=0\n",
    "        for t in text_list:\n",
    "            if type(t) != str:\n",
    "                text_list[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.train_features = self.vectorizer.fit_transform(text_list)\n",
    "\n",
    "        self.nb_1 = MultinomialNB()\n",
    "        self.nb_1.fit(self.train_features , category_list)\n",
    "    def predict_clasiffier_1(self, path):       \n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text'][:]\n",
    "        category_devlist=corpus_filtro5_devset['category'][:]\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features = self.vectorizer.transform(text_devlist)\n",
    "        return self.nb_1.predict(self.test_features)\n",
    "\n",
    "class Clasificador2:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        self.__clasiffier_2_train()\n",
    "        self.__median_predictor_train()\n",
    "\n",
    "    \n",
    "    def __median_predictor_train(self):\n",
    "        corpus_filtro5 = pandas.read_csv(\"corpus_filtro5_trainingset_c2.csv\",encoding='utf-8')\n",
    "        text_list=corpus_filtro5['text'][:]\n",
    "        median_list=corpus_filtro5['median'][:]\n",
    "        self.category_list_c1=[]\n",
    "\n",
    "        pos=0\n",
    "        for t in text_list:\n",
    "            if type(t) != str:\n",
    "                text_list[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "\n",
    "        self.train_features_c1 = vectorizer.fit_transform(text_list)\n",
    "\n",
    "        for i in median_list:\n",
    "            if(i == 1):\n",
    "                self.category_list_c1.append(1)\n",
    "            else:\n",
    "                self.category_list_c1.append(0)\n",
    "        \n",
    "        self.nb_median1 = MultinomialNB()\n",
    "        self.nb_median1.fit(self.train_features_c1 , category_list_c1[:])\n",
    "\n",
    "        self.category_list_c2=[]\n",
    "        for i in median_list:\n",
    "            if(i == 2):\n",
    "                self.category_list_c2.append(1)\n",
    "            else:\n",
    "                self.category_list_c2.append(0)\n",
    "        self.nb_median2 = MultinomialNB()\n",
    "        self.nb_median2.fit(self.train_features_c1 , self.category_list_c2)\n",
    "\n",
    "        self.category_list_c3=[]\n",
    "        for i in median_list:\n",
    "            if(i == 3):\n",
    "                self.category_list_c3.append(1)\n",
    "            else:\n",
    "                self.category_list_c3.append(0)\n",
    "\n",
    "        self.nb_median3 = MultinomialNB()\n",
    "        self.nb_median3.fit(train_features_c1 , self.category_list_c3)       \n",
    "\n",
    "        self.category_list_c4=[]\n",
    "        for i in median_list:\n",
    "            if(i == 4):\n",
    "                self.category_list_c4.append(1)\n",
    "            else:\n",
    "                self.category_list_c4.append(0)\n",
    "\n",
    "        self.nb_median4 = MultinomialNB()\n",
    "        self.nb_median4.fit(train_features_c1 , self.category_list_c4[:])\n",
    "        \n",
    "        self.category_list_c5=[]\n",
    "        for i in median_list:\n",
    "            if(i == 5):\n",
    "                self.category_list_c5.append(1)\n",
    "            else:\n",
    "                self.category_list_c5.append(0)\n",
    "        self.nb_median5 = MultinomialNB()\n",
    "        self.nb_median5.fit(train_features_c1 , self.category_list_c5)\n",
    "\n",
    "        self.category_list_c0=[]\n",
    "        for i in median_list:\n",
    "            if(i == 0):\n",
    "                self.category_list_c0.append(1)\n",
    "            else:\n",
    "                self.category_list_c0.append(0)\n",
    "                \n",
    "        self.nb_median0 = MultinomialNB()\n",
    "        self.nb_median0.fit(train_features_c1 , self.category_list_c0)\n",
    "        \n",
    "    def __predict_median1_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_c1 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median1.predict_proba(self.test_features_c1)\n",
    "    def __predict_median2_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_c2 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median2.predict_proba(self.test_features_c2)\n",
    "    def __predict_median3_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_c3 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median3.predict_proba(self.test_features_c3)\n",
    "    def __predict_median4_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_c4 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median4.predict_proba(self.test_features_c4)\n",
    "    def __predict_median5_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_c5 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median5.predict_proba(self.test_features_c5)\n",
    "    def __predict_median0_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_c0 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median0.predict_proba(self.test_features_c0)\n",
    "    def __clasiffier_2_train(self):\n",
    "        corpus_filtro5_trainingset_c2 = pandas.read_csv(\"corpus_filtro5_trainingset_c2.csv\",encoding='utf-8')\n",
    "        text_list_c2=corpus_filtro5_trainingset_c2['text'][:]\n",
    "        category_list_c2=corpus_filtro5_trainingset_c2['category'][:]\n",
    "        pos_c2=0\n",
    "        for t in text_list_c2:\n",
    "            if type(t) != str:\n",
    "                text_list_c2[pos_c2]=\"NaN\"\n",
    "            pos_c2+=1\n",
    "        self.train_features_class2 = self.vectorizer.fit_transform(text_list_c2)\n",
    "\n",
    "        self.nb_2 = MultinomialNB()\n",
    "        self.nb_2.fit(self.train_features_class2, category_list_c2)\n",
    "\n",
    "    \n",
    "    def predict_median(self, path):\n",
    "        c1_pred=self.__predict_median1_c2(path)\n",
    "        c2_pred=self.__predict_median2_c2(path)\n",
    "        c3_pred=self.__predict_median3_c2(path)\n",
    "        c4_pred=self.__predict_median4_c2(path)\n",
    "        c5_pred=self.__predict_median5_c2(path)\n",
    "        c0_pred=self.__predict_median0_c2(path)        \n",
    "        predicts=[]\n",
    "        proba=[]\n",
    "        for i in range( len(c1_pred) ):\n",
    "            maximum=np.max([c1_pred[i][1],c2_pred[i][1],c3_pred[i][1],c4_pred[i][1],c5_pred[i][1],c0_pred[i][1]])\n",
    "            if(maximum == c1_pred[i][1]):\n",
    "                predicts.append(1)\n",
    "            elif(maximum == c2_pred[i][1]):\n",
    "                predicts.append(2)\n",
    "            elif(maximum == c3_pred[i][1]):\n",
    "                predicts.append(3)\n",
    "            elif(maximum == c4_pred[i][1]):\n",
    "                predicts.append(4)\n",
    "            elif(maximum == c5_pred[i][1]):\n",
    "                predicts.append(5)\n",
    "            elif(maximum == c0_pred[i][1]):\n",
    "                predicts.append(0)\n",
    "            proba.append(maximum)\n",
    "        corpus_filtro5_devset_c2 = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist_c2=corpus_filtro5_devset_c2['text'][:]\n",
    "        category_devlist_c2=corpus_filtro5_devset_c2['category'][:]\n",
    "        pos=0\n",
    "        for t in text_devlist_c2:\n",
    "            if type(t) != str:\n",
    "                text_devlist_c2[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_class2 = self.vectorizer.transform(text_devlist_c2)\n",
    "                    \n",
    "        return predicts, proba, self.nb_2.predict(self.test_features_class2)\n",
    "\n",
    "class Clasificador3:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        self.__clasiffier_3_train()\n",
    "    def __clasiffier_3_train(self):\n",
    "        corpus_filtro5_trainingset_c3 = pandas.read_csv(\"corpus_filtro5_trainingset_c3.csv\",encoding='utf-8')\n",
    "        text_list=corpus_filtro5_trainingset_c3['text'][:]\n",
    "        category_list=corpus_filtro5_trainingset_c3['category'][:]\n",
    "        pos=0\n",
    "        for t in text_list:\n",
    "            if type(t) != str:\n",
    "                text_list[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.train_features = self.vectorizer.fit_transform(text_list)\n",
    "        self.nb = MultinomialNB()\n",
    "        self.nb.fit(self.train_features , category_list)\n",
    "    def predict_clasiffier_1(self, path):\n",
    "        corpus_filtro5_devset_c3 = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset_c3['text'][:]\n",
    "        category_devlist=corpus_filtro5_devset_c3['category'][:]\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features = self.vectorizer.transform(text_devlist)\n",
    "        return self.nb.predict(self.test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "clasificador = Clasificador1()\n",
    "print(clasificador.predict_clasiffier_1(\"corpus_filtro5_devset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sebastian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sebastian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:142: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sebastian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sebastian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
      " 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
      " 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1\n",
      " 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
      " 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1\n",
      " 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
      " 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
      " 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
      " 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
      " 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1\n",
      " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
      " 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "clasificador=Clasificador2()\n",
    "predict, proba, humor_prob=clasificador.predict_median(\"corpus_filtro5_devset_c2.csv\")\n",
    "print(humor_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "clasificador = Clasificador3()\n",
    "print(clasificador.predict_clasiffier_1(\"corpus_filtro5_devset_c3.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
