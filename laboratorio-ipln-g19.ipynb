{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagenes/pdf/pln1.png)\n",
    "![title](imagenes/pdf/pln2.png)\n",
    "![title](imagenes/pdf/pln3.png)\n",
    "![title](imagenes/pdf/pln4.png)\n",
    "![title](imagenes/pdf/pln5.png)\n",
    "![title](imagenes/pdf/pln6.png)\n",
    "![title](imagenes/pdf/pln7.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from lxml import etree\n",
    "import re\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de votos 26943\n",
      "No humor, votos 14131  porcentaje  52.4477600861\n",
      "Humor 1 estrella, votos  2960  porcentaje  10.9861559589\n",
      "Humor 2 estrellas, votos  2421  porcentaje  8.98563634339\n",
      "Humor 3 estrellas, votos  3274  porcentaje  12.1515792599\n",
      "Humor 4 estrellas, votos  2541  porcentaje  9.43102104443\n",
      "Humor 5 estrellas, votos  1616  porcentaje  5.99784730728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~sebastiandaloia/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPLEMENTACION Cantidad de votos por calificacion, promedio de estrellas, fraccion de votos de humor\n",
    "\n",
    "import pandas\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from lxml import etree\n",
    "import re\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Calificaciones\n",
    "no_humor=0\n",
    "humor_e1=0\n",
    "humor_e2=0\n",
    "humor_e3=0\n",
    "humor_e4=0\n",
    "humor_e5=0\n",
    "total=0\n",
    "\n",
    "#Vectores tomados del corpus\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "\n",
    "#Se suman los votos por cada calificacion\n",
    "for calificacion in vec_no_humor[:]:\n",
    "    no_humor+=calificacion\n",
    "\n",
    "for calificacion in vec_e1[:]:\n",
    "    humor_e1+=calificacion\n",
    "    \n",
    "for calificacion in vec_e2[:]:\n",
    "    humor_e2+=calificacion\n",
    "    \n",
    "for calificacion in vec_e3[:]:\n",
    "    humor_e3+=calificacion\n",
    "    \n",
    "for calificacion in vec_e4[:]:\n",
    "    humor_e4+=calificacion\n",
    "\n",
    "for calificacion in vec_e5[:]:\n",
    "    humor_e5+=calificacion\n",
    "\n",
    "total=no_humor + humor_e1 + humor_e2 + humor_e3 + humor_e4 + humor_e5\n",
    "\n",
    "promedio_estrellas_pv = (humor_e1 + 2*humor_e2 + 3*humor_e3 + 4*humor_e4 + 5*humor_e5)/total\n",
    "promedio_estrellas_pvp = (humor_e1 + 2*humor_e2 + 3*humor_e3 + 4*humor_e4 + 5*humor_e5)/(total - no_humor)\n",
    "\n",
    "fraccion_votos_humor = (total - no_humor)/total\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total de votos\", total)\n",
    "print(\"No humor, votos\", no_humor,\" porcentaje \", no_humor/total*100)\n",
    "print(\"Humor 1 estrella, votos \", humor_e1, \" porcentaje \", humor_e1/total*100)\n",
    "print(\"Humor 2 estrellas, votos \", humor_e2, \" porcentaje \", humor_e2/total*100)\n",
    "print(\"Humor 3 estrellas, votos \", humor_e3, \" porcentaje \", humor_e3/total*100)\n",
    "print(\"Humor 4 estrellas, votos \", humor_e4, \" porcentaje \", humor_e4/total*100)\n",
    "print(\"Humor 5 estrellas, votos \", humor_e5, \" porcentaje \", humor_e5/total*100)\n",
    "print(\"Promedio de estrellas PV \", promedio_estrellas_pv) \n",
    "print(\"Promedio de estrellas PVP \", promedio_estrellas_pvp)\n",
    "print(\"Fracción votos de humor\", fraccion_votos_humor)\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='sebastiandaloia', api_key='cMoSSDnwQHyafA8prIEv')\n",
    "\n",
    "\n",
    "\n",
    "#Impresion de valores usando ploly\n",
    "labels = ['No humor','1 Estrella','2 Estrellas','3 Estrellas', '4 Estrellas', '5 Estrellas']\n",
    "values = [no_humor, humor_e1, humor_e2, humor_e3, humor_e4, humor_e5]\n",
    "trace = go.Pie(labels=labels, values=values)\n",
    "plotly.plotly.iplot([trace], filename='basic_pie_chart')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagenes/pdf/pln8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets  12106\n",
      "Los tweets mas votados tienen  21 votos\n",
      "Los tweets con dos o menos votos forman un total de  8668\n",
      "Los chistes mas graciosos son\n",
      "1 \n",
      " —¿A dónde vas tan maquillada? —A una fiesta, mamá. —¿Eres el payaso? —¡MAMÁAAA! :( —JAJAJÁ, cállate y hazme reír o no vas. —Ok. :(\n",
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~sebastiandaloia/0 or inside your plot.ly account where it is named 'mpl-basic-histogram'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://plot.ly/~sebastiandaloia/0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPLEMENTACION Cantidad de votos por tweet\n",
    "#Vectores\n",
    "vec_ids=corpus['id'][:]\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "#Variables\n",
    "cant_tweets=len(corpus)\n",
    "\n",
    "print(\"Cantidad de tweets \", cant_tweets)\n",
    "\n",
    "tweet_id_vot=np.zeros((cant_tweets, 2), np.int64)\n",
    "\n",
    "contador=0\n",
    "for i in range(0, cant_tweets):\n",
    "    tweet_id_vot[i, 0]=vec_ids[i]\n",
    "    tweet_id_vot[i,1]=vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i]\n",
    "\n",
    "\n",
    "array=tweet_id_vot[:,1]\n",
    "array=np.sort(array, axis=None)\n",
    "last=array[len(array) - 1]\n",
    "\n",
    "lesseq_3=0\n",
    "for i in range(0, cant_tweets):\n",
    "    if( array[i] > 2 ):\n",
    "        break;\n",
    "    lesseq_3+=1\n",
    "\n",
    "print(\"Los tweets mas votados tienen \", last, \"votos\")\n",
    "print(\"Los tweets con dos o menos votos forman un total de \", lesseq_3)\n",
    "\n",
    "contador=1\n",
    "print(\"Los chistes mas graciosos son\")\n",
    "for i in range(0, cant_tweets):\n",
    "    if( vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] == 21 ):\n",
    "        print(contador, \"\\n\", corpus['text'][i])\n",
    "\n",
    "plotly.tools.set_credentials_file(username='sebastiandaloia', api_key='cMoSSDnwQHyafA8prIEv')\n",
    "\n",
    "votos_nums = tweet_id_vot[:,1]\n",
    "\n",
    "plt.hist(votos_nums)\n",
    "plt.title(\"Histograma de votos por tweet\")\n",
    "plt.xlabel(\"Votos\")\n",
    "plt.ylabel(\"Tweets\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "plotly.plotly.plot_mpl(fig, filename='mpl-basic-histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fracción votos de humor 0.475522399139\n"
     ]
    }
   ],
   "source": [
    "#iMPLEMENTACION Fraccion de votos de humor\n",
    "import pandas\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from lxml import etree\n",
    "import re\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Calificaciones\n",
    "no_humor=0\n",
    "humor_e1=0\n",
    "humor_e2=0\n",
    "humor_e3=0\n",
    "humor_e4=0\n",
    "humor_e5=0\n",
    "total=0\n",
    "\n",
    "#Vectores tomados del corpus\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "\n",
    "#Se suman los votos por cada calificacion\n",
    "for calificacion in vec_no_humor[:]:\n",
    "    no_humor+=calificacion\n",
    "\n",
    "for calificacion in vec_e1[:]:\n",
    "    humor_e1+=calificacion\n",
    "    \n",
    "for calificacion in vec_e2[:]:\n",
    "    humor_e2+=calificacion\n",
    "    \n",
    "for calificacion in vec_e3[:]:\n",
    "    humor_e3+=calificacion\n",
    "    \n",
    "for calificacion in vec_e4[:]:\n",
    "    humor_e4+=calificacion\n",
    "\n",
    "for calificacion in vec_e5[:]:\n",
    "    humor_e5+=calificacion\n",
    "\n",
    "total=no_humor + humor_e1 + humor_e2 + humor_e3 + humor_e4 + humor_e5\n",
    "\n",
    "promedio_estrellas_pv = (humor_e1 + 2*humor_e2 + 3*humor_e3 + 4*humor_e4 + 5*humor_e5)/total\n",
    "promedio_estrellas_pvp = (humor_e1 + 2*humor_e2 + 3*humor_e3 + 4*humor_e4 + 5*humor_e5)/(total - no_humor)\n",
    "\n",
    "fraccion_votos_humor = (total - no_humor)/total\n",
    "\n",
    "print(\"Fracción votos de humor\", fraccion_votos_humor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagenes/pdf/pln9.png)\n",
    "![title](imagenes/pdf/pln10.png)\n",
    "![title](imagenes/pdf/pln11.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de estrellas PV  1.3312548714\n",
      "Promedio de estrellas PVP  2.79956290977\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTACION Promedio de estrellas\n",
    "import pandas\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from lxml import etree\n",
    "import re\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Calificaciones\n",
    "no_humor=0\n",
    "humor_e1=0\n",
    "humor_e2=0\n",
    "humor_e3=0\n",
    "humor_e4=0\n",
    "humor_e5=0\n",
    "total=0\n",
    "\n",
    "#Vectores tomados del corpus\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "\n",
    "#Se suman los votos por cada calificacion\n",
    "for calificacion in vec_no_humor[:]:\n",
    "    no_humor+=calificacion\n",
    "\n",
    "for calificacion in vec_e1[:]:\n",
    "    humor_e1+=calificacion\n",
    "    \n",
    "for calificacion in vec_e2[:]:\n",
    "    humor_e2+=calificacion\n",
    "    \n",
    "for calificacion in vec_e3[:]:\n",
    "    humor_e3+=calificacion\n",
    "    \n",
    "for calificacion in vec_e4[:]:\n",
    "    humor_e4+=calificacion\n",
    "\n",
    "for calificacion in vec_e5[:]:\n",
    "    humor_e5+=calificacion\n",
    "\n",
    "total=no_humor + humor_e1 + humor_e2 + humor_e3 + humor_e4 + humor_e5\n",
    "\n",
    "promedio_estrellas_pv = (humor_e1 + 2*humor_e2 + 3*humor_e3 + 4*humor_e4 + 5*humor_e5)/total\n",
    "promedio_estrellas_pvp = (humor_e1 + 2*humor_e2 + 3*humor_e3 + 4*humor_e4 + 5*humor_e5)/(total - no_humor)\n",
    "\n",
    "print(\"Promedio de estrellas PV \", promedio_estrellas_pv) \n",
    "print(\"Promedio de estrellas PVP \", promedio_estrellas_pvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Los chistes mas graciosos son: \n",
      "1  - PV : 4.75 votos:  4\n",
      "   -Mamá tuvimos un examen sorpresa en la escuela - ¿Y que pasó hijo? -Estuve muy sorprendido mamá.\n",
      "2  - PV : 4.5 votos:  4\n",
      "   ¿Cómo se llama la secretaria de Batman? Bati la fea... #chistes #fb\n",
      "3  - PV : 4.5 votos:  4\n",
      "   Quedarse atrapado en la esquina de la ducha porque el agua sale muy fría.\n",
      "4  - PV : 4.4 votos:  5\n",
      "   —Amor, ¿Ya está dentro? —Sí ¿Te lastima? —Sí amor, mételo despacito, duele cuando entra. —Ok, vamos a intentar otro número de zapato :)\n",
      "5  - PV : 4.25 votos:  4\n",
      "   —Violarte es mi sueño —¿QUÉ? —Que estudio diseño —No, ¡ahora me violas!\n",
      "6  - PV : 4.25 votos:  4\n",
      "   — ¡Profe! ¿Puedo ir al baño?. *Regresa con comida*.\n",
      "7  - PV : 4.25 votos:  4\n",
      "   Las mujeres de hoy en día con tal de no hacer limpieza en la casa, se inventan que les gusta el fútbol.\n",
      "8  - PV : 4.25 votos:  4\n",
      "   Siempre será mejor hablar en clases que en el recreo.\n",
      "9  - PV : 4.25 votos:  4\n",
      "   Lunes Martes Miércoles Jueves Casi Sábado Sábado Casi Lunes.\n",
      "10  - PV : 4.25 votos:  4\n",
      "   Todo es comestible hasta que la diarrea demuestre lo contrario...\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTACION Mejores chistes:\n",
    "# 1: Para ser considerados deben tener al menos 3 votos en total 1 de ellos debe ser de humor.\n",
    "# 2: Entre los candidatos los mas graciosos seran los que tengan mejor promedio de estrellas y entre estos los de mas votos.\n",
    "# 3: Se listan los mejores 10 chistes, esto puede cambiarse cambiando el valor de cantidad_mejores_chistes.\n",
    "mejores_chistes = np.array([])\n",
    "cant_votos = []\n",
    "prom_estrellas_pv = []\n",
    "votos_humor = []\n",
    "minimo_votos = 3\n",
    "cantidad_mejores_chistes = 10\n",
    "#Variables\n",
    "cant_tweets=len(corpus)\n",
    "for i in range(0, cant_tweets):\n",
    "    votos_humor.append(vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i])\n",
    "    cant_votos.append(vec_no_humor[i] + votos_humor[i])\n",
    "    prom_estrellas_pv.append((vec_e1[i] + 2*vec_e2[i] + 3*vec_e3[i] + 4*vec_e4[i] + 5*vec_e5[i])/cant_votos[i])\n",
    "    \n",
    "    #debe tener la cantidad de votos necesaria y almenos un voto de humor para ser considerado\n",
    "    if( cant_votos[i] > minimo_votos and votos_humor[i] > 0 ): \n",
    "        #solo se mostraran los 5 mejores chistes.\n",
    "        chiste = np.array([corpus['text'][i], prom_estrellas_pv[i], cant_votos[i]])\n",
    "        if (len(mejores_chistes) > 0):\n",
    "            mejores_chistes = np.vstack((mejores_chistes, chiste))\n",
    "        else :\n",
    "            mejores_chistes = np.append(mejores_chistes, chiste)\n",
    "   \n",
    "print(\"\")\n",
    "print(\"Los chistes mas graciosos son: \")\n",
    "\n",
    "ind = np.lexsort((mejores_chistes[:,2],mejores_chistes[:,1]))\n",
    "for i in range(0, cantidad_mejores_chistes):\n",
    "    j = len(mejores_chistes) - i - 1\n",
    "    print(i+1, \" - PV :\",mejores_chistes[ind][j][1], \"votos: \", mejores_chistes[ind][j][2])\n",
    "    print(\"  \", mejores_chistes[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negación general:  2.440800988551316\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTACION Negatividad\n",
    "pattern_tag = re.compile(r'lemma=\"(no?)\"')\n",
    "\n",
    "tam=len(corpus)\n",
    "total_no=0;\n",
    "total_token=0\n",
    "pos=0\n",
    "for i in np.random.randint(0, tam, 1000):\n",
    "    text = corpus['text'][i]\n",
    "    tweet_token=0\n",
    "    token_no=0\n",
    "    xml = analyzer.run(text.encode(), 'flush')\n",
    "    for sentence in xml:\n",
    "        for token in sentence: \n",
    "            tweet_token+=1\n",
    "            m = re.search(pattern_tag, etree.tostring(token).decode())\n",
    "            if m :\n",
    "                token_no+=1\n",
    "    pos+=1\n",
    "    print(\"Tweet numero \", pos,  end=\"\\r\")\n",
    "    time.sleep(1)\n",
    "             \n",
    "        \n",
    "    total_no+=token_no    \n",
    "    total_token+=tweet_token\n",
    "        \n",
    "print(\"Negación general: \", total_no/math.sqrt(total_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de links encontrados  3280\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTACION cantidad de links\n",
    "\n",
    "pattern_http = re.compile(r'http://')\n",
    "\n",
    "num_links=0\n",
    "\n",
    "for text in corpus['text'][:]:\n",
    "    result = pattern_http.findall(text) \n",
    "    num_links+=len(result)\n",
    "\n",
    "print(\"Cantidad de links encontrados \", num_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediana primera persona  0.0\n",
      "Mediana segunda persona  0.0\n",
      "Media primera persona  0.183470578311\n",
      "Media segunda persona  0.16683360628\n",
      "[0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17407765595569785, 0.48507125007266594, 0.45883146774112349, 0.23570226039551587, 0.0, 0.0, 1.0606601717798212, 0.0, 0.15075567228888181, 0.75, 0.45883146774112349, 0.24253562503633297, 0.0, 0.0, 0.0, 0.0, 0.27735009811261457, 0.17677669529663687, 0.0, 0.0, 0.24253562503633297, 0.2581988897471611, 1.1470786693528088, 0.0, 0.0, 0.0, 0.3481553119113957, 0.0, 0.0, 0.0, 0.0, 0.87287156094396956, 0.18257418583505536, 0.3481553119113957, 0.0, 0.17677669529663687, 0.18257418583505536, 0.0, 0.48507125007266594, 1.2060453783110545, 0.40000000000000002, 0.41702882811414954, 0.0, 0.0, 0.0, 0.22941573387056174, 0.0, 0.0, 0.0, 1.0, 0.55470019622522915, 0.0, 1.0206207261596576, 0.40824829046386307, 0.60302268915552726, 0.16666666666666666, 0.54772255750516607, 0.0, 0.39223227027636809, 0.30151134457776363, 0.59999999999999998, 0.2581988897471611, 0.0, 0.20412414523193154, 0.0, 0.0, 0.0, 0.47140452079103173, 0.0, 0.25, 0.0, 0.0, 0.2581988897471611, 0.34299717028501764, 0.5, 0.0, 0.43643578047198478, 0.31622776601683794, 0.20851441405707477, 0.0, 0.5388159060803247, 0.0, 0.0, 0.0, 0.0, 0.6546536707079772, 0.0, 1.0776318121606494, 0.0, 0.0, 0.0, 0.58834840541455213, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.36514837167011072, 0.63245553203367588, 1.0206207261596576, 0.0, 0.0, 0.1889822365046136, 0.32879797461071458, 0.0, 0.0, 0.17407765595569785, 0.0, 0.0, 0.0, 0.0, 0.81649658092772615, 0.0, 0.0, 0.0, 0.37139067635410372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38490017945975052, 0.17960530202677491, 0.70710678118654746, 0.0, 0.17149858514250882, 0.58834840541455213, 0.17407765595569785, 0.1690308509457033, 0.0, 0.0, 0.15430334996209191, 0.24253562503633297, 0.48507125007266594, 0.25, 0.0, 0.1690308509457033, 0.57735026918962584, 0.0, 0.20851441405707477, 0.0, 0.17407765595569785, 0.28867513459481292, 0.0, 0.0, 0.0, 0.60302268915552726, 0.5303300858899106, 0.0, 0.19611613513818404, 0.19611613513818404, 0.47140452079103173, 0.0, 0.0, 0.0, 0.42640143271122083, 0.0, 0.21821789023599239, 0.37139067635410372, 0.0, 0.0, 0.46852128566581819, 0.0, 0.0, 0.0, 0.35355339059327373, 0.0, 0.18569533817705186, 0.45883146774112349, 0.76980035891950105, 0.63960214906683133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40000000000000002, 0.0, 0.0, 0.0, 0.51449575542752646, 0.0, 0.0, 0.0, 0.31234752377721214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.19245008972987526, 0.0, 0.0, 0.68599434057003528, 1.212678125181665, 0.47140452079103173, 0.0, 0.0, 0.37139067635410372, 0.0, 0.42640143271122083, 0.24253562503633297, 0.70710678118654757, 0.0, 0.0, 0.0, 0.16439898730535729, 0.0, 0.0, 0.0, 0.0, 0.67082039324993692, 0.0, 0.0, 0.0, 0.0, 0.7559289460184544, 0.0, 0.34299717028501764, 0.0, 0.0, 0.0, 0.55708601453115558, 0.0, 0.0, 0.1889822365046136, 0.0, 0.48507125007266594, 0.31622776601683794, 0.49319696191607187, 0.0, 0.0, 0.68599434057003528, 0.0, 0.0, 0.17407765595569785, 0.38490017945975052, 0.17677669529663687, 0.0, 0.0, 0.21821789023599239, 0.0, 0.73029674334022143, 0.17149858514250882, 0.0, 0.5, 0.21821789023599239, 0.0, 0.20851441405707477, 0.0, 0.25, 0.67082039324993692, 0.0, 0.36514837167011072, 0.54772255750516607, 0.0, 0.66666666666666663, 0.0, 0.0, 0.53452248382484879, 0.0, 0.44721359549995793, 0.0, 0.21821789023599239, 0.17407765595569785, 0.0, 0.0, 0.0, 0.0, 0.17407765595569785, 1.0660035817780522, 0.0, 0.19611613513818404, 0.0, 0.0, 0.0, 0.94491118252306805, 0.0, 0.23570226039551587, 0.0, 0.0, 0.0, 0.21320071635561041, 0.23570226039551587, 0.0, 0.6546536707079772, 0.0, 0.73029674334022143, 0.17960530202677491, 0.0, 0.20412414523193154, 1.0, 0.0, 0.0, 1.1094003924504583, 0.0, 0.83205029433784372, 0.0, 0.23570226039551587, 0.0, 0.72760687510899891, 0.76980035891950105, 0.0, 0.0, 0.0, 0.75, 0.0, 0.34299717028501764, 0.20000000000000001, 0.30151134457776363, 0.0, 0.0, 0.0, 0.22941573387056174, 0.0, 0.0, 0.39223227027636809, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22360679774997896, 0.0, 0.0, 0.0, 0.73029674334022143, 0.0, 0.0, 0.17960530202677491, 0.28867513459481292, 0.0, 0.0, 0.5, 0.0, 0.47140452079103173, 0.0, 0.15617376188860607, 0.35355339059327373, 0.0, 0.21320071635561041, 0.0, 0.2672612419124244, 0.0, 0.57735026918962584, 0.0, 0.21821789023599239, 0.41702882811414954, 0.0, 0.35921060405354982, 0.2581988897471611, 0.0, 0.42640143271122083, 0.0, 0.18257418583505536, 0.48507125007266594, 0.0, 0.39223227027636809, 0.0, 0.35921060405354982, 0.0, 0.44721359549995793, 0.0, 0.20000000000000001, 0.0, 0.22941573387056174, 0.44721359549995793, 0.0, 0.20000000000000001, 0.0, 0.22941573387056174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.24253562503633297, 0.40824829046386307, 0.0, 0.28867513459481292, 0.0, 0.0, 0.0, 0.5222329678670935, 0.0, 0.17960530202677491, 0.0, 0.0, 0.5222329678670935, 0.0, 0.0, 0.17960530202677491, 0.0, 0.0, 0.21821789023599239, 0.72760687510899891, 0.0, 0.59999999999999998, 0.0, 0.24253562503633297, 0.40000000000000002, 0.0, 0.0, 0.0, 0.19611613513818404, 0.0, 0.21821789023599239, 0.56694670951384085, 0.0, 0.0, 1.1094003924504583, 0.0, 0.5222329678670935, 0.37139067635410372, 0.0, 0.0, 0.40000000000000002, 0.0, 0.0, 0.21821789023599239, 0.0, 0.14744195615489714, 0.0, 0.82199493652678646, 0.0, 0.0, 0.0, 0.98058067569092022, 0.0, 0.16439898730535729, 0.20412414523193154, 0.0, 0.1889822365046136, 0.0, 0.19611613513818404, 0.35921060405354982, 0.50709255283710997, 0.20851441405707477, 0.17149858514250882, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0, 0.0, 0.19611613513818404, 0.0, 0.0, 0.0, 0.19245008972987526, 0.0, 0.59999999999999998, 0.32444284226152509, 0.0, 0.0, 0.0, 0.40000000000000002, 0.0, 0.70710678118654746, 0.0, 0.0, 0.0, 0.88388347648318433, 0.0, 0.0, 0.35921060405354982, 0.0, 0.0, 0.0, 0.51449575542752646, 0.0, 0.27735009811261457, 0.22941573387056174, 0.0, 0.0, 0.0, 0.0, 0.33806170189140661, 1.150792911137501, 0.43643578047198478, 1.1094003924504583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33806170189140661, 0.0, 0.76980035891950105, 0.0, 0.0, 0.2672612419124244, 0.0, 0.0, 0.47140452079103173, 0.0, 0.30151134457776363, 0.0, 0.0, 0.0, 0.17677669529663687, 0.0, 0.31622776601683794, 0.5388159060803247, 0.0, 0.0, 0.0, 0.5222329678670935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19245008972987526, 0.1889822365046136, 0.0, 0.17960530202677491, 1.0776318121606494, 0.0, 0.21320071635561041, 0.33333333333333331, 0.0, 0.67082039324993692, 0.20412414523193154, 0.0, 0.0, 0.0, 0.7745966692414834, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22360679774997896, 0.0, 0.0, 0.17149858514250882, 0.0, 0.48038446141526137, 0.0, 0.0, 0.2581988897471611, 0.17407765595569785, 0.20000000000000001, 0.42640143271122083, 0.72760687510899891, 0.0, 0.0, 0.0, 0.75, 0.17960530202677491, 0.0, 0.0, 0.0, 0.40000000000000002, 0.0, 0.0, 0.63960214906683133, 0.0, 0.0, 0.36514837167011072, 0.0, 0.72760687510899891, 0.0, 0.0, 0.0, 0.40000000000000002, 0.5303300858899106, 0.0, 0.47140452079103173, 0.5303300858899106, 0.49319696191607187, 0.27735009811261457, 0.0, 0.0, 0.0, 0.43643578047198478, 0.18257418583505536, 0.0, 0.44721359549995793, 0.0, 0.17677669529663687, 0.35921060405354982, 0.28867513459481292, 0.2672612419124244, 0.0, 0.6546536707079772, 0.0, 0.16666666666666666, 0.0, 0.0, 0.28867513459481292, 0.48507125007266594, 0.68599434057003528, 0.17407765595569785, 0.0, 0.2581988897471611, 0.41702882811414954, 0.0, 0.0, 0.0, 0.98058067569092022, 0.73029674334022143, 0.30151134457776363, 0.0, 0.0, 0.21821789023599239, 0.1889822365046136, 0.2672612419124244, 0.0, 0.0, 1.0606601717798212, 0.53452248382484879, 0.0, 0.18569533817705186, 0.68824720161168518, 0.2672612419124244, 0.0, 0.16666666666666666, 0.22360679774997896, 1.044465935734187, 0.20851441405707477, 0.0, 0.0, 0.37139067635410372, 0.0, 0.44721359549995793, 0.0, 0.37139067635410372, 0.0, 0.0, 0.35921060405354982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58834840541455213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20412414523193154, 0.0, 1.386750490563073, 0.0, 0.0, 0.0, 0.64888568452305018, 0.35355339059327373, 0.0, 0.0, 0.13867504905630729, 0.0, 0.0, 0.0, 0.40824829046386307, 0.18569533817705186, 0.0, 0.0, 0.0, 0.16222142113076254, 0.0, 0.68824720161168518, 0.0, 0.0, 1.0327955589886444, 0.0, 0.0, 0.0, 0.20851441405707477, 0.0, 0.21320071635561041, 0.0, 0.0, 0.8703882797784892, 0.0, 0.27735009811261457, 0.0, 0.18569533817705186, 0.0, 0.0, 0.19611613513818404, 0.5388159060803247, 0.88388347648318433, 0.0, 0.50709255283710997, 0.16666666666666666, 0.80000000000000004, 0.0, 0.46291004988627571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22941573387056174, 0.35355339059327373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30151134457776363, 0.0, 0.0, 0.0, 0.76980035891950105, 0.0, 0.58834840541455213, 0.3481553119113957, 0.2672612419124244, 0.34299717028501764, 0.35921060405354982, 0.0, 0.42640143271122083, 0.0, 0.0, 0.0, 0.55708601453115558, 0.41702882811414954, 0.0, 0.0, 0.2672612419124244, 0.0, 0.0, 0.0, 0.20000000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55708601453115558, 0.19245008972987526, 0.17407765595569785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94491118252306805, 0.0, 0.0, 0.17407765595569785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5222329678670935, 0.0, 1.044465935734187, 0.0, 0.0, 0.0, 0.5303300858899106, 0.0, 0.0, 0.0, 0.2581988897471611, 0.0, 0.55708601453115558, 0.36514837167011072, 0.0, 0.0, 0.38490017945975052, 0.0, 0.0, 0.22941573387056174, 0.58834840541455213, 0.23570226039551587, 0.63245553203367588, 0.0, 0.16439898730535729, 0.0, 0.0, 0.3779644730092272, 0.76980035891950105, 0.0, 0.44721359549995793, 0.0, 0.0, 0.50709255283710997, 0.0, 0.68824720161168518, 0.2581988897471611, 0.32444284226152509, 0.0, 0.28867513459481292, 0.0, 0.17407765595569785, 0.17960530202677491, 0.0, 0.51449575542752646, 0.21821789023599239, 0.78446454055273618, 0.0, 0.0, 0.0, 0.21320071635561041, 0.0, 0.0, 0.0, 0.18257418583505536, 0.17407765595569785, 0.0, 0.0, 0.0, 0.18569533817705186, 0.0, 0.40824829046386307, 0.0, 0.0, 0.0, 0.34299717028501764, 0.0, 0.60302268915552726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48507125007266594, 0.19245008972987526, 0.20000000000000001, 0.18257418583505536, 0.0, 0.0, 0.0, 0.0, 0.21320071635561041, 0.0, 0.0, 0.30499714066520933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20000000000000001, 0.0, 0.21821789023599239, 0.39223227027636809, 0.40824829046386307, 0.0, 0.0, 0.22941573387056174, 0.0, 0.0, 0.31622776601683794, 0.0, 0.0, 0.24253562503633297, 0.5, 0.0, 0.0, 0.0, 0.17677669529663687, 0.0, 0.0, 0.0, 0.0, 0.7745966692414834, 0.31622776601683794, 0.37139067635410372, 0.0, 0.57735026918962584, 0.0, 0.0, 0.0, 0.6546536707079772, 0.0, 0.97014250014533188, 0.29814239699997197, 0.81649658092772615, 0.44721359549995793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68824720161168518, 0.18257418583505536, 0.40824829046386307, 0.0, 0.0, 0.0, 0.16439898730535729, 0.0, 0.40824829046386307, 0.19611613513818404, 0.0, 0.0, 0.0, 0.35921060405354982, 0.0, 0.0, 0.28867513459481292, 0.0, 0.20412414523193154, 0.0, 0.0, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0, 0.40000000000000002, 0.2672612419124244, 0.91766293548224698, 0.94491118252306805, 0.0, 0.0, 0.0, 0.0, 0.18569533817705186, 0.0, 0.54772255750516607, 0.0, 0.0, 0.0, 0.48666426339228763, 0.20000000000000001, 0.0, 0.0, 0.24253562503633297, 0.0, 0.0, 0.22360679774997896, 0.2581988897471611, 0.38490017945975052, 0.48507125007266594, 0.5, 0.22941573387056174, 0.0, 0.49319696191607187, 0.22941573387056174, 0.37139067635410372, 0.0, 0.0, 0.21320071635561041, 0.2672612419124244, 0.0, 0.0, 0.0, 0.0, 0.28867513459481292, 0.40000000000000002, 0.0, 0.0, 0.37139067635410372, 0.0, 0.0, 0.0, 0.55470019622522915, 0.40824829046386307, 0.75, 0.0, 0.24253562503633297, 0.45883146774112349, 0.0, 0.19245008972987526, 0.33806170189140661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.53452248382484879, 0.94280904158206347, 0.0, 0.0, 0.24253562503633297, 0.3481553119113957, 0.72760687510899891, 0.45883146774112349, 0.23570226039551587, 0.0, 0.0, 0.0, 0.0, 0.90453403373329089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61237243569579458, 0.0, 0.24253562503633297, 0.0, 0.0, 0.0, 0.25, 0.0, 0.3481553119113957, 0.20412414523193154, 0.31622776601683794, 0.80178372573727319, 0.0, 0.0, 0.36514837167011072, 0.0, 0.0, 0.17677669529663687, 0.18257418583505536, 0.50709255283710997, 0.0, 0.0, 0.40000000000000002, 0.0, 0.23570226039551587, 0.57735026918962573, 0.0, 0.0, 0.22941573387056174, 0.0, 0.0, 1.1666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73029674334022143, 0.0, 0.39223227027636809, 0.0, 0.80000000000000004, 0.2581988897471611, 1.2374368670764582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22360679774997896, 0.0, 0.0, 0.0, 0.2581988897471611, 0.17149858514250882, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17407765595569785, 0.0, 0.45883146774112349, 0.35921060405354982, 0.2581988897471611, 0.0, 0.20851441405707477, 0.39223227027636809, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59999999999999998, 0.23570226039551587, 0.0, 0.54772255750516607, 0.0, 0.20412414523193154, 0.0, 0.0, 0.0, 0.0, 0.75, 0.48507125007266594, 0.0, 0.91766293548224698, 0.0, 0.0, 0.24253562503633297, 0.0, 0.83205029433784372, 0.0, 0.30151134457776363, 0.0, 0.0, 0.24253562503633297, 0.0, 0.47140452079103173, 0.0, 0.0, 0.0, 0.19245008972987526, 0.35921060405354982, 0.17677669529663687, 0.60302268915552726, 0.0, 0.78446454055273618, 0.17407765595569785, 0.1690308509457033, 0.0, 0.45883146774112349, 0.61721339984836765, 0.24253562503633297, 0.0, 0.0, 0.0, 0.50709255283710997, 0.0, 0.0, 0.0, 0.0, 0.3481553119113957, 0.0, 0.54772255750516607, 0.72760687510899891, 0.0, 0.30151134457776363, 0.0, 0.0, 0.0, 0.19611613513818404, 0.23570226039551587, 0.0, 0.0, 0.47140452079103173, 0.0, 0.47140452079103173, 0.21821789023599239, 0.37139067635410372, 0.0, 0.0, 0.62469504755442429, 0.0, 0.0, 0.0, 0.0, 0.33333333333333331, 0.18569533817705186, 0.22941573387056174, 0.0, 0.21320071635561041, 0.20851441405707477, 0.0, 0.19245008972987526, 0.0, 0.0, 0.53452248382484879, 0.20000000000000001, 0.0, 0.0, 0.0, 0.68599434057003528, 0.0, 0.0, 0.0, 0.46852128566581819, 0.0, 0.0, 0.17149858514250882, 0.36514837167011072, 0.0, 0.0, 0.38490017945975052, 0.0, 0.42640143271122083, 0.34299717028501764, 0.0, 0.47140452079103173, 0.0, 0.18257418583505536, 0.18569533817705186, 0.23570226039551587, 0.0, 0.0, 0.47140452079103173, 0.38490017945975052, 0.0, 0.0, 0.16439898730535729, 0.0, 0.20412414523193154, 0.0, 0.0, 0.0, 0.89442719099991586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51449575542752646, 0.43643578047198478, 0.57735026918962584, 1.1180339887498949, 0.74278135270820744, 0.0, 0.0, 0.56694670951384085, 0.0, 0.0, 0.0, 0.32879797461071458, 0.0, 0.0, 0.0, 0.68824720161168518, 0.0, 0.0, 0.57735026918962573, 0.35355339059327373, 0.27735009811261457, 0.0, 0.0, 0.0, 0.0, 0.17149858514250882, 0.0, 0.25, 0.0, 0.43643578047198478, 0.0, 0.72760687510899891, 0.0, 0.0, 0.76980035891950105, 0.36514837167011072, 0.36514837167011072, 0.0, 0.0, 1.2510864843424487, 0.0, 0.80178372573727319, 1.0425720702853738, 0.0, 0.0, 0.43643578047198478, 0.17407765595569785, 0.5, 0.60302268915552726, 0.0, 0.0, 0.17407765595569785, 0.0, 0.1889822365046136, 0.78446454055273618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21320071635561041, 0.0, 0.0, 0.0, 0.20000000000000001, 0.0, 0.5388159060803247, 0.0, 0.20412414523193154, 0.0, 0.5163977794943222, 0.0, 0.0, 0.83405765622829908, 0.0, 0.0, 0.0, 0.22360679774997896, 0.48507125007266594, 0.38490017945975052, 0.63960214906683133, 0.24253562503633297, 0.0, 0.0, 0.0, 0.0, 0.20000000000000001, 0.60302268915552726, 0.18569533817705186, 0.0, 0.24253562503633297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1889822365046136, 0.1889822365046136, 0.0, 0.0, 0.80178372573727319, 0.44721359549995793, 0.81649658092772615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17960530202677491, 0.0, 0.0, 0.0, 0.0, 0.16439898730535729, 0.0, 0.0, 0.31234752377721214, 0.0, 0.0, 0.0, 0.0, 0.80178372573727319, 0.0, 0.57735026918962584, 0.0, 0.43643578047198478, 0.0, 0.0, 0.35921060405354982, 0.5163977794943222, 0.28867513459481292, 0.63960214906683133, 0.0, 0.0, 0.24253562503633297, 0.44721359549995793, 0.0, 0.31622776601683794, 0.0, 0.48507125007266594, 0.0, 0.0, 0.40000000000000002, 0.0, 0.22941573387056174, 0.0, 0.97014250014533188, 0.0, 0.19245008972987526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.20412414523193154, 0.20412414523193154, 0.57735026918962584, 0.45883146774112349, 0.18569533817705186, 0.28867513459481292, 0.5222329678670935, 0.0, 0.5388159060803247, 0.0, 0.7745966692414834, 0.0, 0.68824720161168518, 0.24253562503633297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59999999999999998, 0.0, 0.24253562503633297, 0.40000000000000002, 0.0, 0.0, 0.0, 0.19611613513818404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74278135270820744, 0.0, 0.0, 0.0, 0.57735026918962584, 0.0, 0.0, 0.45883146774112349, 0.88465173692938281, 0.0, 1.150792911137501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16439898730535729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33806170189140661, 0.20851441405707477, 0.34299717028501764, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0, 0.0, 0.39223227027636809, 0.48507125007266594, 0.5303300858899106, 0.0, 0.38490017945975052, 0.63960214906683133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20000000000000001, 0.44721359549995793, 0.0, 0.0, 0.0, 0.0, 0.17677669529663687, 0.0, 0.53452248382484879, 0.35921060405354982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27735009811261457, 0.0, 0.21821789023599239, 0.0, 0.0, 0.0, 0.33806170189140661, 0.98639392383214375, 0.21821789023599239, 0.0, 0.0, 0.0, 0.2581988897471611, 0.0, 0.86602540378443871, 0.33806170189140661, 0.0, 0.38490017945975052, 0.72760687510899891, 0.21821789023599239, 0.53452248382484879, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0, 0.0, 0.0, 0.59999999999999998, 0.0, 0.18257418583505536, 0.31622776601683794, 0.0, 0.0, 0.38490017945975052, 0.0, 0.3481553119113957, 0.0, 0.2581988897471611, 0.1690308509457033, 0.0, 0.0, 0.19245008972987526, 0.0, 0.36514837167011072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61237243569579458, 0.0, 0.0, 0.42640143271122083, 0.0, 0.20000000000000001, 0.22360679774997896, 0.40000000000000002, 0.60302268915552726, 0.0, 0.0, 0.0, 0.1889822365046136, 0.0, 0.0, 0.16012815380508713, 0.48507125007266594, 0.0, 0.0, 0.3481553119113957, 0.0, 0.42640143271122083, 0.0, 0.0, 0.20000000000000001, 0.78446454055273618, 0.0, 0.71842120810709964, 0.0, 0.0, 0.0, 0.20000000000000001, 0.0, 0.2672612419124244, 0.63960214906683133, 0.17677669529663687, 0.0, 0.0, 0.0, 0.24253562503633297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.386750490563073, 0.0, 0.75, 0.34299717028501764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5388159060803247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.27735009811261457, 0.19611613513818404, 0.86602540378443871, 0.0, 0.34299717028501764, 0.17407765595569785, 0.0, 0.0, 0.20851441405707477, 0.0, 0.0, 0.48507125007266594, 0.0, 0.54772255750516607, 0.0, 0.0, 0.0, 0.43643578047198478, 0.0, 0.2672612419124244, 0.0, 0.0, 0.70710678118654746, 0.0, 0.97014250014533188, 0.18569533817705186, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.20851441405707477, 0.48507125007266594, 0.0, 0.37139067635410372, 0.0, 0.44721359549995793, 0.0, 0.37139067635410372, 0.0, 0.20851441405707477, 0.0, 0.40824829046386307, 0.23570226039551587, 0.0, 0.0, 0.27735009811261457, 0.39223227027636809, 0.53452248382484879, 0.0, 0.0, 0.0, 0.80178372573727319, 0.42640143271122083, 0.0, 0.40824829046386307, 0.0, 0.0, 0.0, 0.0, 0.40824829046386307, 0.16222142113076254, 0.0, 0.0, 0.0, 0.13867504905630729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20851441405707477, 0.0, 0.21320071635561041, 0.72760687510899891, 0.0, 0.0, 0.5222329678670935, 0.0, 0.0, 0.0, 0.21320071635561041, 0.0, 0.0, 0.0, 0.0, 0.27735009811261457, 0.0, 0.18569533817705186, 0.0, 0.0, 0.0, 0.71842120810709964, 0.0, 0.0, 0.0, 0.66666666666666663, 0.20000000000000001, 0.0, 0.61721339984836765, 0.0, 0.0, 0.0, 0.0, 0.2672612419124244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19245008972987526, 0.28867513459481292, 0.0, 0.30151134457776363, 0.79056941504209477, 0.0, 0.0, 0.0, 0.0, 0.39223227027636809, 0.17407765595569785, 0.0, 0.0, 0.5388159060803247, 0.0, 0.42640143271122083, 0.2672612419124244, 0.0, 0.0, 0.18569533817705186, 0.41702882811414954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.37139067635410372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1889822365046136, 0.0, 0.0, 0.5222329678670935, 0.24253562503633297, 0.27735009811261457, 0.20000000000000001, 0.20412414523193154, 0.0, 0.69631062382279141, 0.0, 0.3481553119113957, 0.0, 0.60302268915552726, 0.0, 0.35355339059327373, 0.0, 0.0, 0.0, 0.2581988897471611, 0.21320071635561041, 0.0, 0.54772255750516607, 0.0, 0.0, 0.38490017945975052, 0.0, 0.0, 0.0, 0.0, 0.47140452079103173, 0.0, 0.39223227027636809, 0.16439898730535729, 0.53452248382484879, 0.5, 0.0, 0.19245008972987526, 0.0, 0.44721359549995793, 0.0, 0.0, 0.50709255283710997, 0.0, 0.0, 0.5163977794943222, 0.16222142113076254, 0.0, 0.0, 0.0, 0.3481553119113957, 0.17960530202677491, 0.2581988897471611, 0.68599434057003528, 0.21821789023599239, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70710678118654757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21821789023599239, 0.0, 0.0, 1.0, 0.20412414523193154, 0.89442719099991586, 0.0, 1.4596008983995234, 0.17149858514250882, 0.0, 0.15075567228888181, 0.0, 0.0, 0.22360679774997896, 0.45883146774112349, 0.1690308509457033, 0.24253562503633297, 0.19245008972987526, 0.20000000000000001, 0.18257418583505536, 0.0, 0.0, 0.36514837167011072, 0.0, 0.21320071635561041, 0.2581988897471611, 0.0, 0.0, 0.55470019622522915, 0.0, 0.0, 0.0, 0.48507125007266594, 0.19245008972987526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22941573387056174, 0.0, 0.89442719099991586, 0.31622776601683794, 0.27735009811261457, 0.0, 0.0, 0.0, 0.22941573387056174, 0.40824829046386307, 0.0, 0.0, 0.0, 0.33333333333333331, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45883146774112349, 0.0, 0.25, 0.0, 0.14907119849998599, 0.0, 0.0, 0.58834840541455213, 0.0, 0.0, 0.1889822365046136, 0.48038446141526137, 0.22941573387056174, 0.36514837167011072, 0.20412414523193154, 0.0, 0.19245008972987526, 0.35355339059327373, 0.16439898730535729, 0.41702882811414954, 0.0, 0.0, 0.0, 0.0, 0.2672612419124244, 0.71842120810709964, 0.3779644730092272, 0.39223227027636809, 0.0, 0.0, 0.0, 0.21320071635561041, 0.0, 0.67082039324993692, 0.0, 0.0, 0.5, 0.0, 0.40000000000000002, 0.53452248382484879, 0.0, 0.0, 0.28867513459481292, 0.0, 0.0, 0.0, 0.18569533817705186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27735009811261457, 0.0, 0.24253562503633297, 0.85280286542244166, 0.0, 0.0, 0.0, 0.19245008972987526, 0.0, 0.0, 0.22941573387056174, 0.0, 0.0, 0.91766293548224698, 0.0, 0.41702882811414954, 0.33333333333333331, 0.0, 0.0, 0.0, 0.20000000000000001, 0.55470019622522915, 0.0, 0.0, 0.40000000000000002, 0.57735026918962573, 0.0, 0.37139067635410372, 0.0, 0.0, 0.0, 0.0, 0.20412414523193154, 1.0, 0.18569533817705186, 0.0, 0.0, 0.63245553203367588, 0.38490017945975052, 0.33806170189140661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3779644730092272]\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTACION primera y segunda persona\n",
    "\n",
    "pattern_tag = re.compile(r'tag=\"(.*?)\"')\n",
    "\n",
    "first_second_person=np.zeros((len(corpus), 2))\n",
    "contador=0\n",
    "\n",
    "tam=len(corpus)\n",
    "\n",
    "for i in np.random.randint(0, tam, 1000):\n",
    "    text = corpus['text'][i]\n",
    "    total_token=0\n",
    "    token_1=0\n",
    "    token_2=0\n",
    "    xml = analyzer.run(text.encode(), 'flush')\n",
    "    for sentence in xml:\n",
    "        for token in sentence:\n",
    "            m = re.search(pattern_tag, etree.tostring(token).decode())\n",
    "            tag=m.group(1)\n",
    "            if (tag[0] == 'V'):\n",
    "                if(tag[4] == '1'):\n",
    "                    token_1+=1\n",
    "                elif(tag[4]=='2'):\n",
    "                    token_2+=1\n",
    "            elif(tag[0] == 'P'):\n",
    "                if(tag[2] == '1'):\n",
    "                    token_1+=1\n",
    "                elif(tag[2]=='2'):\n",
    "                    token_2+=1\n",
    "            elif(tag[0] == 'D'):\n",
    "                if(tag[2] == '1'):\n",
    "                    token_1+=1\n",
    "                elif(tag[2]=='2'):\n",
    "                    token_2+=1\n",
    "            total_token+=1\n",
    "    first_second_person[contador,0]=token_1/math.sqrt(total_token)\n",
    "    first_second_person[contador,1]=token_2/math.sqrt(total_token)    \n",
    "    contador+=1\n",
    "    print (\"Dato numero \",contador, \"Porcentaje \", contador/len(corpus),  end=\"\\r\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "pp=[]   \n",
    "sp=[]\n",
    "cont=0\n",
    "for t in first_second_person:\n",
    "    if(cont<1000):\n",
    "        pp.append(t[0])\n",
    "        sp.append(t[1])    \n",
    "    cont+=1    \n",
    "#first_second_person    \n",
    "print(\"Mediana primera persona \", np.median(pp))\n",
    "print(\"Mediana segunda persona \", np.median(sp))\n",
    "\n",
    "print(\"Media primera persona \", np.mean(pp))\n",
    "print(\"Media segunda persona \", np.mean(sp))\n",
    "\n",
    "print(pp)\n",
    "\n",
    "print(sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer antes de continuar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes secciones del código se encuentran la construcción de los clasificadores, la elaboración de métricas, el análisis de resultados, en una mancomunión entre textos, archivos csv y código. A continuación se brinda un resumen de las principales áreas de código. También se provee una descripción de los archivos csv que se van generando a partir de la ejecución de los diferentes bloques de código, estos archivos csv fue necesario generarlos para persistir resultados intermedios en el preprocesamiento del corpus. Para acceder a los archivo csv más adelante mencionados ingrese a la siguiente url: https://github.com/OdioALouise/ipln-2017.git e ingrese al branch seba (https://github.com/OdioALouise/ipln-2017/tree/seba).\n",
    "También se detallan las clases y los métodos de los clasificadores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTES\n",
    "    Importación de librerías útiles\n",
    "    Preprocesamiento\n",
    "        Eliminar comentarios con menos de tres votos → corpus_humor_training.csv pasa a ser corpus_filtro1.csv\n",
    "        Eliminar Hashtags → Se eliminan HashTags de los tweets corpus_filtro1.csv pasa a ser corpus_filtro2.csv\n",
    "        Agregar la categoría humor o no humor a los documentos → corpus_filtro2.csv pasa a ser corpus_filtro3.csv\n",
    "        Agregrar información de POS-tag → Se agrega al texto información de POS-tag corpus_filtro3.csv pasa a ser corpus_filtro4.csv\n",
    "        Separación de datos de entrenamiento y desarrollo → Se separan datos de entrenamiento y desarrollo para el clasificador 1\n",
    "        Cálculo de mediana para el segundo clasificador → Se calcula la mediana para el segundo clasificador corpus_filtro4.csv pasa a ser corpus_filtro4median.csv\n",
    "        Separación de datos de entrenamiento y desarrollo para segundo clasificador → Se separan datos de entrenamiento y desarrollo para el clasificador 2\n",
    "        Preprocesamiento clasificador 3 → Se modifica la columna category para que cumpla con la definición de humor o no humor dado para este clasificador, humor si la mediana es mayor o igual a 1 no humor en otro caso.\n",
    "        Separación de datos de entrenamiento y desarrollo para el clasificador 3\n",
    "        Clases para los clasificadores → Se crean las clases para Clasificador1, Clasificador2, Clasificador3\n",
    "    Construcción de clases para los clasificadores\n",
    "    Cálculo de métricas\n",
    "        Clasificador 1\n",
    "        Clasificador 2 + Matrix confusion\n",
    "        Clasificador 3\n",
    "        Legacy Matrix confusion\n",
    "        Clasificador 1 Sin Pos-tag\n",
    "    Resultados de la etapa de desarrollo\n",
    "    Test-set\n",
    "        Eliminar Hashtags → corpus_humor_testing.csv pasa a ser corpus_humor_testing_no_hashtag.csv\n",
    "        Agregar category → corpus_humor_testing_no_hashtag.csv pasa a ser corpus_humor_testing_no_hashtag_with_category.csv\n",
    "        Agregar medians → corpus_humor_testing_no_hashtag_with_category.csv pasa a ser corpus_humor_testing_no_hashtag_with_category_and_median.csv\n",
    "        Agregar category_2 → corpus_humor_testing_no_hashtag_with_category_and_median.csv pasa a ser corpus_humor_testing_no_hashtag_with_category_and_median_and_category_2.csv\n",
    "        Agregar información de POS-tag → corpus_humor_testing_no_hashtag_with_category_and_median_and_category_2.csv pasa a ser corpus_humor_testing_no_hashtag_with_category_and_median_and_category_2_and_postag.csv\n",
    "        Evaluación de clasificadores con el test set y elaboración de métricas\n",
    "    Resultados de la etapa de prueba\n",
    "\n",
    "\n",
    "\n",
    "ARCHIVOS\n",
    "corpus_humor_training(id, text, account_id, n, 1, 2, 3, 4, 5 )\n",
    "Tiene los datos del corpus originales\n",
    "\n",
    "corpus_filtro1(id, text, account_id, n, 1, 2, 3, 4, 5 )\n",
    "Contiene los datos luego de aplicar el filtro de eliminar tweets con menos de tres votos.\n",
    "\n",
    "corpus_filtro2(id, text, account_id, n, 1, 2, 3, 4, 5 )\n",
    "Contiene los datos luego de remover los HaashTags del texto de los tweets\n",
    "\n",
    "corpus_filtro3(text, category, n, 1, 2, 3, 4, 5 )\n",
    "Se agrega categoría category con el criterio si la mitad de los tweets son de humor\n",
    "entonces la categoría del tweet es humor.\n",
    "En category 1 indica humor y 0 no humor\n",
    "\n",
    "corpus_filtro4(text, category. 1, 2, 3, 4, 5)\n",
    "Se agrega información de POS-tag al texto\n",
    "\n",
    "corpus_filtro4median(text, category, median, n, 1, 2, 3, 4, 5)\n",
    "Se agrega el cálculo de la mediana mediante la incorporación de la columna de datos median\n",
    "\n",
    "corpus_filtro4median_category(text, category, median)\n",
    "Se modifica la columna category para que representa humor si la mediana es mayor o igual a 1\n",
    "o no humor en caso contrario. 1 representa humor 0 no humor\n",
    "\n",
    "corpus_filtro5_trainingset(text, category)\n",
    "corpus de entrenamiento para e clasificador 1 \n",
    "\n",
    "corpus_filtro5_devset(text, category)\n",
    "corpus de desarrollo para el clasificador 1 \n",
    "\n",
    "corpus_filtro5_trainingset_c2(text, median, category)\n",
    "corpus de entrenamiento para e clasificador 2\n",
    "\n",
    "corpus_filtro5_devset_c2(text, median, category)\n",
    "corpus de desarrollo para el clasificador 2\n",
    "\n",
    "corpus_filtro5_trainingset_c3(text, median, category)\n",
    "corpus de entrenamiento para e clasificador 3\n",
    "\n",
    "corpus_filtro5_devset_c3(text, median, category)\n",
    "corpus de desarrollo para el clasificador 3\n",
    "\n",
    "corpus_humor_testing(id, text, account_id, n, 1, 2, 3, 4, 5 )\n",
    "Tiene los datos del corpus originales\n",
    "\n",
    "corpus_humor_testing_no_hashtags(id, text, account_id, n, 1, 2, 3, 4, 5 )\n",
    "Contiene los datos luego de remover los HaashTags del texto de los tweets\n",
    "\n",
    "corpus_humor_testing_no_hashtags_with_category(text, category, n, 1, 2, 3, 4, 5 )\n",
    "Se agrega categoría category con el criterio si la mitad de los tweets son de humor\n",
    "entonces la categoría del tweet es humor.\n",
    "En category 1 indica humor y 0 no humor\n",
    "\n",
    "corpus_humor_testing_no_hashtags_with_category_and_median(text, category, median, n, 1, 2, 3, 4, 5 )\n",
    "Se agrega el cálculo de la mediana mediante la incorporación de la columna de datos median\n",
    "\n",
    "corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2(text, category, median, category_2, n, 1, 2, 3, 4, 5 )\n",
    "Se agrega la columna category_2 para que representa humor si la mediana es mayor o igual a 1\n",
    "o no humor en caso contrario. 1 representa humor 0 no humor\n",
    "\n",
    "corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2_and_postag(text, category, median, category_2, n, 1, 2, 3, 4, 5 )\n",
    "Se agrega información de POS-tag al texto\n",
    "\n",
    "CLASES\n",
    "\n",
    "Clasificador1\n",
    "    Clasificador1(path)\n",
    "        path: indica el camino al archivo que contiene el corpus de entrenamiento\n",
    "    __clasifier1_train(path)\n",
    "        path: indica el camino al archivo que contiene el corpus de entrenamiento\n",
    "        En esta clase se realiza el entrenamiento.\n",
    "        En self.nb_1 se guarda el clasificador multinomianl.\n",
    "    predict_clasiffier_1(path)\n",
    "        Se retornan las predicciones sobre los tweets que se encuentran en el archivo dado por path\n",
    "\n",
    "\n",
    "Clasificador2\n",
    "    __median_predictor_train()\n",
    "        Se realiza la predicción de la mediana a partir del corpus guardado en corpus_filtro5_trainingset_c2.csv\n",
    "        self.nb_median1 aprende a clasificar tweets en tweets que tienen mediana 1 y tweets que no tienen mediana 1.\n",
    "        self.category_list_c1 es usado para la clasificación de self.nb_median1, el valor 1 indica que tienen mediana 1, el valor 0 indica que no tienen mediana 1\n",
    "        Idem para\n",
    "        self.nb_median2\n",
    "        self.category_list_c2\n",
    "\n",
    "        self.nb_median3\n",
    "        self.category_list_c3\n",
    "\n",
    "        self.nb_median4\n",
    "        self.category_list_c4\n",
    "\n",
    "        self.nb_median5\n",
    "        self.category_list_c5\n",
    "\n",
    "        self.nb_median0\n",
    "        self.category_list_c0\n",
    "\n",
    "    __predict_median1_c2(path)\n",
    "        path: indica la ruta en donde se encuentran los datos que se van a clasificar\n",
    "        self.median_list_c1, es usada para la prediccón de los datos a clasificar por parte de self.nb_median1\n",
    "        Se retorna una estructura con las probabilidades para cada tweet de que sea o no sea humor con mediana 1\n",
    "\n",
    "    __predict_median2_c2(path)\n",
    "        path: idem\n",
    "        self.median_list_c2: idem \n",
    "        Se retorna una estructura con las probabilidades para cada tweet de que sea o no sea humor con mediana 2\n",
    "\n",
    "    __predict_median3_c2(path)\n",
    "        path: idem\n",
    "        self.median_list_c3: idem \n",
    "        Se retorna una estructura con las probabilidades para cada tweet de que sea o no sea humor con mediana 3\n",
    "\n",
    "    __predict_median4_c2(path)\n",
    "        path: idem\n",
    "        self.median_list_c4: idem \n",
    "        Se retorna una estructura con las probabilidades para cada tweet de que sea o no sea humor con mediana 4\n",
    "\n",
    "    __predict_median5_c2(path)\n",
    "        path: idem\n",
    "        self.median_list_c5: idem \n",
    "        Se retorna una estructura con las probabilidades para cada tweet de que sea o no sea humor con mediana 5\n",
    "\n",
    "    __predict_median0_c2(path)\n",
    "        path: idem\n",
    "        self.median_list_c0: idem \n",
    "        Se retorna una estructura con las probabilidades para cada tweet de que sea o no sea humor con mediana 0\n",
    "\n",
    "    __classifier_2_train\n",
    "        Se entrena el clasificador con el corpus guardado en corpus_filtro5_trainingset_c2.csv.\n",
    "        En self.nb_2 se guarda el clasificador multinomial\n",
    "    predict_median(path)\n",
    "        path: camino a los tweets para los cuales se va a predecir si son humor o no\n",
    "        self.m1 Contiene 1 en su entrada i-esima si el tweet es predicho con mediana 1, se utilizan para la matriz de confusión\n",
    "        self.m2 Análogo\n",
    "        self.m3 Análogo\n",
    "        self.m4 Análogo\n",
    "        self.m5 Análogo\n",
    "        self.m0 Análogo\n",
    "        Se retorna una lista con la clasificación, una lista con las probabilidades para cada mediana predicha. y una lista con la mediana predicha para el i-ésimo tweet clasificado\n",
    "\n",
    "Clasificador3\n",
    "    __clasifier1_train()\n",
    "        Se entrena el clasificador a través del training set que se encuentra en corpus_filtro5_trainingset_c3.csv\n",
    "    predict_clasiffier_1(path, test)\n",
    "        Se retornan las predicciones sobre los tweets que se encuentran en el archivo dado por path\n",
    "        path: indica el camino al archivo que contiene el corpus de entrenamiento\n",
    "        test: indica la columna en donde se encuentra la clasificación para los textos del corpus, hasta ahora useless parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importacion de librerias utiles\n",
    "\n",
    "#Libreria para analisis de datos \n",
    "#en este proyecto para leer y grabar\n",
    "#archivos en csv\n",
    "import pandas\n",
    "\n",
    "#Librerias cientificas scipy, numpy, matplotlib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Librerias de incluidas en python time, math\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Lectura de archivos xml\n",
    "from lxml import etree\n",
    "\n",
    "#Expresiones regulares\n",
    "import re\n",
    "\n",
    "#Pyfreeling\n",
    "from pyfreeling import Analyzer\n",
    "analyzer = Analyzer(config='/usr/share/freeling/config/es.cfg', lang='es')\n",
    "\n",
    "#Para implementar modelos de aprendizajes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar comentarios con menos de tres votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga el corpus de humor\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "id_f = []\n",
    "text_f = []\n",
    "account_id_f=[]\n",
    "nh_f=[]\n",
    "sh1_f=[]\n",
    "sh2_f=[]\n",
    "sh3_f=[]\n",
    "sh4_f=[]\n",
    "sh5_f=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_id=corpus['id'][:]\n",
    "vec_text=corpus['text'][:]\n",
    "vec_account_id=corpus['account_id'][:]\n",
    "vec_no_humor =corpus['n'][:]\n",
    "vec_e1 =corpus['1'][:]\n",
    "vec_e2 =corpus['2'][:]\n",
    "vec_e3 =corpus['3'][:]\n",
    "vec_e4 =corpus['4'][:]\n",
    "vec_e5 =corpus['5'][:]\n",
    "\n",
    "#Para cada tweet se calcula si tiene al menos tres votos, \n",
    "#en caso afirmativo se guarda su informacion, en caso\n",
    "#negativo se descarta el tweet\n",
    "for i in range(len(corpus)):\n",
    "    if vec_no_humor[i] + vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= 3:\n",
    "        id_f.append(vec_id[i])\n",
    "        text_f.append(vec_text[i])\n",
    "        account_id_f.append(vec_account_id[i])\n",
    "        nh_f.append(vec_no_humor[i])\n",
    "        sh1_f.append(vec_e1[i])\n",
    "        sh2_f.append(vec_e2[i])\n",
    "        sh3_f.append(vec_e3[i])\n",
    "        sh4_f.append(vec_e4[i])\n",
    "        sh5_f.append(vec_e5[i])\n",
    "        \n",
    "#El primer filtro aplicado es guardado en el archivo corpus_filtro1.csv\n",
    "d = {'id' : id_f,\n",
    "    'text' : text_f,\n",
    "    'account_id': account_id_f,\n",
    "    'n':nh_f, \n",
    "    '1':sh1_f,\n",
    "    '2':sh2_f,\n",
    "    '3':sh3_f,\n",
    "    '4':sh4_f,\n",
    "    '5':sh5_f\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_f = []\n",
    "\n",
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro1 = pandas.read_csv(\"corpus_filtro1.csv\",encoding='utf-8')\n",
    "\n",
    "#Generacion de patron de Hashtag\n",
    "pattern_hashtag = re.compile(r'#.+?\\b')\n",
    "\n",
    "\n",
    "#Se sustituye el Hashtag por el string vacio, los tweets que\n",
    "#van pasando por este procesamiento se van guardando en el arreglo\n",
    "#text_f\n",
    "for i in range(len(corpus_filtro1)):\n",
    "    text_f.append(re.sub(pattern_hashtag, \"\", corpus_filtro1['text'][i]))\n",
    "#El segundo filtro aplicado es guardado en el archivo corpus_filtro2.csv\n",
    "d = {'id' : corpus_filtro1['id'][:],\n",
    "    'text' : text_f,\n",
    "    'account_id': corpus_filtro1['account_id'][:],\n",
    "    'n':corpus_filtro1['n'][:], \n",
    "    '1':corpus_filtro1['1'][:],\n",
    "    '2':corpus_filtro1['2'][:],\n",
    "    '3':corpus_filtro1['3'][:],\n",
    "    '4':corpus_filtro1['4'][:],\n",
    "    '5':corpus_filtro1['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregar la categoria humor o no humor a los documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El criterio de humor o no humor esta basado en el criterio presentado en la letra del laboratorio, en donde se considerará humorístico si la mitad o más de los anotadores lo calificaron con una o más estrellas y no humorístico en caso contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro2 = pandas.read_csv(\"corpus_filtro2.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "text = []\n",
    "category=[]\n",
    "h1=[]\n",
    "h2=[]\n",
    "h3=[]\n",
    "h4=[]\n",
    "h5=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_text=corpus_filtro2['text'][:]\n",
    "vec_no_humor =corpus_filtro2['n'][:]\n",
    "vec_e1 =corpus_filtro2['1'][:]\n",
    "vec_e2 =corpus_filtro2['2'][:]\n",
    "vec_e3 =corpus_filtro2['3'][:]\n",
    "vec_e4 =corpus_filtro2['4'][:]\n",
    "vec_e5 =corpus_filtro2['5'][:]\n",
    "\n",
    "#Si hay igual o mas votos de humor que de no humor la categoria tiene el valor 1\n",
    "#en caso contrario tiene el valor 0\n",
    "for i in range(len(corpus_filtro2)):\n",
    "    if  vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= vec_no_humor[i]:\n",
    "        category.append(1)\n",
    "    else:\n",
    "        category.append(0)\n",
    "    text.append(vec_text[i])\n",
    "    h1.append(vec_e1[i])\n",
    "    h2.append(vec_e2[i])\n",
    "    h3.append(vec_e3[i])\n",
    "    h4.append(vec_e4[i])\n",
    "    h5.append(vec_e5[i])\n",
    "#El tercer procesamiento aplicado sobre los tweets es guardado en el archivo corpus_filtro.3\n",
    "d = {'text' : text,\n",
    "    'category': category,\n",
    "     'n': vec_no_humor,\n",
    "     '1':h1,\n",
    "     '2':h2,\n",
    "     '3':h3,\n",
    "     '4':h4,\n",
    "     '5':h5\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category', 'n', '1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar información de POS-tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro3 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro3['text'][:]\n",
    "\n",
    "#Se genera el patron para identificar las POS de las palabras\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "\n",
    "#Variables para datos estadisticos\n",
    "num_sentences=len(corpus_filtro3)\n",
    "numsentence=1\n",
    "\n",
    "#Variable auxiliar para adherir las POS a los tweets\n",
    "pos=0\n",
    "\n",
    "#Para cada documento en el corpus de tweets que va siendo procesado\n",
    "#utilizando la herramienta freeling se analizan las categorias gramaticales\n",
    "#de todas las palabras que aparecen en cada tweet y esta informacion es\n",
    "#adherida al texto del tweet\n",
    "for d in text_list:\n",
    "    if(type(d) == str):\n",
    "        xml = analyzer.run(d.encode(), 'flush')\n",
    "        print(numsentence, \" \", math.floor( numsentence/num_sentences*100),\"%\", end=\"\\r\")\n",
    "        for sentence in xml:        \n",
    "            for token in sentence:\n",
    "                token_byte=etree.tostring(token)\n",
    "                m = re.search(pattern_pos, token_byte.decode())\n",
    "                if m is not None:\n",
    "                    text_list[ pos] = text_list[pos] + \" \" + m.group(1)\n",
    "    pos+=1\n",
    "    numsentence+=1\n",
    "\n",
    "#El procesamiento actual del corpus es guardado en el documento corpus_filtro4.csv\n",
    "d = {'text' : text_list,\n",
    "    'category': corpus_filtro3['category'][:],\n",
    "     '1':corpus_filtro3['1'][:],\n",
    "     '2':corpus_filtro3['2'][:],\n",
    "     '3':corpus_filtro3['3'][:],\n",
    "     '4':corpus_filtro3['4'][:],\n",
    "     '5':corpus_filtro3['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', '1','2','3','4','5'])\n",
    "df.to_csv('corpus_filtro4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de datos de entrenamiento y de desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro4.csv\",encoding='utf-8')\n",
    "\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4)*20/100)\n",
    "#Se generan posiciones en el arreglo tweets al azar que representen el 20% del corpus actual\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4) - 1, size=num_dev_tweets)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4['text'][:]\n",
    "vec_category=corpus_filtro4['category'][:]\n",
    "\n",
    "#Se separa el 20% de corpus para desarrollo del 80% para entrenamiento\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "\n",
    "\n",
    "#Se guardan las instancias corpus de desarrollo en corpus_filtro5_devset.csv\n",
    "#y corpus de entrenamiento en corpus_filtro5_trainingset.csv\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset.csv')\n",
    "\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de mediana, para segundo clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia salvada del corpus que se encuentra en el archivo\n",
    "#corpus_filtro4.csv\n",
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro4.csv\",encoding='utf-8')\n",
    "corpus_filtro3 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "\n",
    "\n",
    "text_list=corpus_filtro4['text'][:]\n",
    "category_list=corpus_filtro4['category'][:]\n",
    "\n",
    "n_list=corpus_filtro3['n'][:]\n",
    "h1_list=corpus_filtro4['1'][:]\n",
    "h2_list=corpus_filtro4['2'][:]\n",
    "h3_list=corpus_filtro4['3'][:]\n",
    "h4_list=corpus_filtro4['4'][:]\n",
    "h5_list=corpus_filtro4['5'][:]\n",
    "\n",
    "medians=[]\n",
    "pos=0\n",
    "\n",
    "#Caclulo de la mediana, en values se expanden los votos de cada estrella\n",
    "#de la siguiente manera si #cant_3Estrellas_en_tweet = 4, entonces se guardan\n",
    "#en values 4 treses 3,3,3,3 para permitir el calculo de la mediana\n",
    "#Por ultimo en la estructura medians se guarda el valor discreto (0,1,2,3,4 o 5) de la mediana\n",
    "#si la cantidad de votos es par se toma el voto del medio con valor mas grande\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    values = []\n",
    "    for j in range( h1_list[i]):\n",
    "        values.append(1)\n",
    "    for j in range( h2_list[i] ):\n",
    "        values.append(2)\n",
    "    for j in range( h3_list[i] ):\n",
    "        values.append(3)\n",
    "    for j in range( h4_list[i] ):\n",
    "        values.append(4)\n",
    "    for j in range( h5_list[i] ):\n",
    "        values.append(5)\n",
    "    for j in range( n_list[i] ):\n",
    "        values.append(0)\n",
    "    mediana=np.median(values)\n",
    "    if( len(values) % 2 == 1 ):\n",
    "        medians.append(math.floor(mediana))\n",
    "    else:\n",
    "        for i in values:\n",
    "            if( i >= mediana):\n",
    "                medians.append(i)\n",
    "                break\n",
    "    pos+=1\n",
    "\n",
    "    \n",
    "#El nuevo procesamiento que suma la informacion de la mediana se guarda\n",
    "#en el archivo corpus_filtro4median.csv\n",
    "d = {'text' : corpus_filtro4['text'][:],\n",
    "    'category' : corpus_filtro4['category'][:],\n",
    "    'median': medians,\n",
    "    'n': corpus_filtro3['n'][:],\n",
    "    '1':corpus_filtro4['1'][:],\n",
    "    '2':corpus_filtro4['2'][:],\n",
    "    '3':corpus_filtro4['3'][:],\n",
    "    '4':corpus_filtro4['4'][:],\n",
    "    '5':corpus_filtro4['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', 'median', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_filtro4median.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion de datos de entrenamiento y desarrollo, para segundo clasificador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Se carga la instancia salvada en el bloque de codigo anterior\n",
    "corpus_filtro4median = pandas.read_csv(\"corpus_filtro4median.csv\",encoding='utf-8')\n",
    "\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4median)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4median) - 1, size=num_dev_tweets)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "median_train=[]\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "median_dev=[]\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4median['text'][:]\n",
    "vec_median=corpus_filtro4median['median'][:]\n",
    "vec_category=corpus_filtro4median['category'][:]\n",
    "\n",
    "\n",
    "for i in range( len(corpus_filtro4median)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        median_dev.append(vec_median[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "        \n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        median_train.append(vec_median[i])\n",
    "        category_train.append(vec_category[i])\n",
    "\n",
    "d = {'text' : text_train,\n",
    "     'median' : median_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'median','category'])\n",
    "df.to_csv('corpus_filtro5_trainingset_c2.csv')\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "     'median': median_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'median', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset_c2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento, clasificador 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4median = pandas.read_csv(\"corpus_filtro4median.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro4median['text'][:]\n",
    "category_list=[]\n",
    "median_list=corpus_filtro4median['median'][:]\n",
    "h1_list=corpus_filtro4median['1'][:]\n",
    "h2_list=corpus_filtro4median['2'][:]\n",
    "h3_list=corpus_filtro4median['3'][:]\n",
    "h4_list=corpus_filtro4median['4'][:]\n",
    "h5_list=corpus_filtro4median['5'][:]\n",
    "pos=0\n",
    "for m in median_list:\n",
    "    if(m<1):\n",
    "        category_list.append(0)\n",
    "    else:\n",
    "        category_list.append(1)\n",
    "    pos+=1\n",
    "d = {'text' : text_list,\n",
    "    'category': category_list,\n",
    "     'median': median_list\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','median'])\n",
    "df.to_csv('corpus_filtro4median_category.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación datos de entrenamiento y desarrollo, clasificador 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n",
      "[1961  698  322 1402 2228  278 1015 2403 2499 2859 1952 2101  572 2387 2126\n",
      " 1790  346 1560 2443 1102 2520 1675 1090 1355  766 3318 2320  822  670 2915\n",
      " 1751 2272 1376 2323 1624  534 1925 1235  454 2656 2934 2543 3391  174  425\n",
      " 1179  620 2121 2991 1616 1518 2558    9 1848 2323 1307 1667  609 2411 2319\n",
      " 2211 2976 2921 3340  167 1084 2921 2058  244  921 2022 2719 1115  192 1267\n",
      "  927 1669 1353 1907  728 2410 3051 1448 2373 1189 2928  288  997  723 2832\n",
      " 1795  752  548   49  241 3382 2344 2428 2446   20 2812 1924  375 2407 2888\n",
      " 3220  669  350  375  967  269 1731 1149  621  530 3186 2135 2466  106 2885\n",
      " 2269 3310  665 3196  671 2873 2781 3433 1720  273 2303 1175 2465 1016 2408\n",
      " 1884  784 1508 2033 2645 1676 2596 2368  127 3070  398  717 2319 2322 1908\n",
      "  600 1835 3368   77  996 1934 2413  918  352 3079 2596 1505  655  457 2538\n",
      "  109 1488 2360 2459 2610 2177  326 2709  272 2336 2210 1267  956 1090 1947\n",
      "  411 2480 1371  515 2434  187 3395  332 2957 2764 2887 1381 1912 1247  300\n",
      " 1748 2225 2149 3383 2477 1788 1209  384 3151 1183   70  547 1484 2528 2539\n",
      " 2633 2979 1876 1915 2256 3399 3300 1948  165  382 2960 2780 2679 1558  323\n",
      " 1308 2865 1387 1791 2138 2116 1176 1296 3267 1874 1289 1988 3216 2924  233\n",
      " 2460 1995 2264 2998 3146 3346 2260 3128  910 2469  626  673 2408 2498 2760\n",
      "  580 3320  919  364 1736 2747  477  125 2007  251 1510 1925  880 2326 2902\n",
      " 1201 2948 2945 1996 1274 2883 1835 1982 2190 3090 2630  225  988 1827 3377\n",
      " 1452 2116  217  816 1265  992 3168 2403  721  800 3426 1865  404 2779 3122\n",
      " 3086  907 1867 2001 2174 1759 2784 1331 2892 1456 1373 3059 1035 3024 3339\n",
      " 3241  401  277  884 2960  729 2916 3047  289 2075 3018 1365 2482 2018 2802\n",
      "  954 1030  612 2837 1209  939 3305  229  617  254 2133 1391 2585  604 3150\n",
      " 1766 2044 3112 1364 3140 2142 2000  224 1785 2680 2409  622 3222 1724 1371\n",
      " 2106 2974   86 1919 1285 2499 2592 2182 2111  779 1206 2858  823 1798   34\n",
      "  831 1600  901 2941 2232 2792 2686 2077 1379 2306 1740 1771 1121 2103 3250\n",
      " 1084 3084 1150 2082 2969 3194 1768 1105 2743 1241 2528 1289 3121 3433 3335\n",
      " 3392 2225 2630 3163  718 2530 3165 1161 2214 3156  876 3297 3279 2048  144\n",
      " 2618 2806 2200 2009  967 2289 1356 2428 3089 1675  475 3076 1324  417 2199\n",
      " 2742  190 1736 2441  692 2953 1580 3167 2111 1497 3117 3190 3390 2508 3226\n",
      " 2404 2483  788   90 2802 1964  987 1206  969 3395  811  406 1637 2911 1402\n",
      " 2151  872 2088 1308 3100 1410   85  369  418 2551 1985  428 1024 3403 2962\n",
      "  521 1494 1870 2710  527 3394 2627 3147  618 2767  346  553 2880 1283 1940\n",
      " 1568  611 3142  694  877 2218 2663 1160 3223 3309 2639 2863 1148 2479 2104\n",
      " 1302 1339 2633 1563 1253 1412  277  425 2449    0 1193 1638 3288 2760 2135\n",
      " 1324 2517  198  279  306  976  586 2666 1247 1969 2399   92 3056 2001 2650\n",
      "  260 2415  614  654 2500 1226 3003  905  741  480  368 2004 1413 1917 2646\n",
      "  968 1474 1694 2086 2749 1870 2410 1910  930 1590 3411  510 1998  956 3211\n",
      " 1315   83 1499 2562 1855 2119 2469  502 2350  603 1292 3332 1249 1365 2133\n",
      "   91 2375   49 2180  872 2313 3283 1242 2469 2196  381 3308  132 1859 2592\n",
      "  803 2629 2897 1631 2092 1639  604 1386 1710 2140 1052 2660 2968 1489   42\n",
      "  873 1793 1388  788 2767 1739  419  503   83 2967  821  256 1927 2730  193\n",
      " 2941 2728 2019 1711  518 2168  126  299  388 2646 2728 2516 2702 1485  829\n",
      "  646 1811 2591 3205 2295 2114 3266  497  116 2648 1255  651 1268  773 1487\n",
      " 1788 1902  265 1640 1037 2508  670 1180  777 2189   50 3054 1776 1082 1305\n",
      " 2937 1878 2017 2101 2501 1074  627 1265  509 1911 1211 2589]\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro4median_category = pandas.read_csv(\"corpus_filtro4median_category.csv\",encoding='utf-8')\n",
    "text=corpus_filtro4median_category['text'][:]\n",
    "category=corpus_filtro4median_category['category'][:]\n",
    "median=corpus_filtro4median_category['median'][:]\n",
    "\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4median_category)*20/100)\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4median_category) - 1, size=num_dev_tweets)\n",
    "\n",
    "print(num_dev_tweets)\n",
    "print(random_samples)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4median_category['text'][:]\n",
    "vec_category=corpus_filtro4median_category['category'][:]\n",
    "\n",
    "for i in range( len(corpus_filtro4median_category)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset_c3.csv')\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset_c3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases para los clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import pandas\n",
    "\n",
    "class Clasificador1:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        self.__clasiffier_1_train(path)\n",
    "    \n",
    "    def __clasiffier_1_train(self, path):\n",
    "        corpus_filtro5_trainingset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_list=corpus_filtro5_trainingset['text'][:]\n",
    "        category_list=corpus_filtro5_trainingset['category'][:]\n",
    "        pos=0\n",
    "        for t in text_list:\n",
    "            if type(t) != str:\n",
    "                text_list[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.train_features = self.vectorizer.fit_transform(text_list)\n",
    "\n",
    "        self.nb_1 = MultinomialNB()\n",
    "        self.nb_1.fit(self.train_features , category_list)\n",
    "    def predict_clasiffier_1(self, path):       \n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text'][:]\n",
    "        category_devlist=corpus_filtro5_devset['category'][:]\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features = self.vectorizer.transform(text_devlist)\n",
    "        return self.nb_1.predict(self.test_features)\n",
    "\n",
    "class Clasificador2:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        self.__clasiffier_2_train()\n",
    "        self.__median_predictor_train()\n",
    "\n",
    "    \n",
    "    def __median_predictor_train(self):\n",
    "        corpus_filtro5 = pandas.read_csv(\"corpus_filtro5_trainingset_c2.csv\",encoding='utf-8')\n",
    "        text_list=corpus_filtro5['text'][:]\n",
    "        median_list=corpus_filtro5['median'][:]\n",
    "        self.category_list_c1=[]\n",
    "\n",
    "        pos=0\n",
    "        for t in text_list:\n",
    "            if type(t) != str:\n",
    "                text_list[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "\n",
    "        self.train_features_c1 = vectorizer.fit_transform(text_list)\n",
    "\n",
    "        for i in median_list:\n",
    "            if(i == 1):\n",
    "                self.category_list_c1.append(1)\n",
    "            else:\n",
    "                self.category_list_c1.append(0)\n",
    "        \n",
    "        self.nb_median1 = MultinomialNB()\n",
    "        self.nb_median1.fit(self.train_features_c1 , self.category_list_c1[:])\n",
    "\n",
    "        self.category_list_c2=[]\n",
    "        for i in median_list:\n",
    "            if(i == 2):\n",
    "                self.category_list_c2.append(1)\n",
    "            else:\n",
    "                self.category_list_c2.append(0)\n",
    "        self.nb_median2 = MultinomialNB()\n",
    "        self.nb_median2.fit(self.train_features_c1 , self.category_list_c2)\n",
    "\n",
    "        self.category_list_c3=[]\n",
    "        for i in median_list:\n",
    "            if(i == 3):\n",
    "                self.category_list_c3.append(1)\n",
    "            else:\n",
    "                self.category_list_c3.append(0)\n",
    "\n",
    "        self.nb_median3 = MultinomialNB()\n",
    "        self.nb_median3.fit(self.train_features_c1 , self.category_list_c3)       \n",
    "\n",
    "        self.category_list_c4=[]\n",
    "        for i in median_list:\n",
    "            if(i == 4):\n",
    "                self.category_list_c4.append(1)\n",
    "            else:\n",
    "                self.category_list_c4.append(0)\n",
    "\n",
    "        self.nb_median4 = MultinomialNB()\n",
    "        self.nb_median4.fit(self.train_features_c1 , self.category_list_c4[:])\n",
    "        \n",
    "        self.category_list_c5=[]\n",
    "        for i in median_list:\n",
    "            if(i == 5):\n",
    "                self.category_list_c5.append(1)\n",
    "            else:\n",
    "                self.category_list_c5.append(0)\n",
    "        self.nb_median5 = MultinomialNB()\n",
    "        self.nb_median5.fit(self.train_features_c1 , self.category_list_c5)\n",
    "\n",
    "        self.category_list_c0=[]\n",
    "        for i in median_list:\n",
    "            if(i == 0):\n",
    "                self.category_list_c0.append(1)\n",
    "            else:\n",
    "                self.category_list_c0.append(0)\n",
    "                \n",
    "        self.nb_median0 = MultinomialNB()\n",
    "        self.nb_median0.fit(self.train_features_c1 , self.category_list_c0)\n",
    "        \n",
    "    def __predict_median1_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        median_devlist=corpus_filtro5_devset['median']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "\n",
    "        self.median_list_c1=[]\n",
    "        for med in median_devlist:\n",
    "            if med == 1:\n",
    "                self.median_list_c1.append(1)\n",
    "            else:\n",
    "                self.median_list_c1.append(0)\n",
    "            \n",
    "\n",
    "        self.test_features_c1 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median1.predict_proba(self.test_features_c1), self.nb_median1.predict(self.test_features_c1)\n",
    "    def __predict_median2_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        median_devlist=corpus_filtro5_devset['median']\n",
    "        \n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "\n",
    "        self.median_list_c2=[]\n",
    "        for med in median_devlist:\n",
    "            if med == 2:\n",
    "                self.median_list_c2.append(1)\n",
    "            else:\n",
    "                self.median_list_c2.append(0)\n",
    "\n",
    "        \n",
    "        self.test_features_c2 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median2.predict_proba(self.test_features_c2), self.nb_median2.predict(self.test_features_c2)\n",
    "    def __predict_median3_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        median_devlist=corpus_filtro5_devset['median']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "\n",
    "        self.median_list_c3=[]\n",
    "        for med in median_devlist:\n",
    "            if med == 3:\n",
    "                self.median_list_c3.append(1)\n",
    "            else:\n",
    "                self.median_list_c3.append(0)\n",
    "\n",
    "        self.test_features_c3 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median3.predict_proba(self.test_features_c3), self.nb_median3.predict(self.test_features_c3)\n",
    "    def __predict_median4_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        median_devlist=corpus_filtro5_devset['median']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.median_list_c4=[]\n",
    "        for med in median_devlist:\n",
    "            if med == 4:\n",
    "                self.median_list_c4.append(1)\n",
    "            else:\n",
    "                self.median_list_c4.append(0)\n",
    "\n",
    "        self.test_features_c4 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median4.predict_proba(self.test_features_c4), self.nb_median4.predict(self.test_features_c4)\n",
    "    def __predict_median5_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        median_devlist=corpus_filtro5_devset['median']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.median_list_c5=[]\n",
    "        for med in median_devlist:\n",
    "            if med == 5:\n",
    "                self.median_list_c5.append(1)\n",
    "            else:\n",
    "                self.median_list_c5.append(0)\n",
    "\n",
    "        self.test_features_c5 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median5.predict_proba(self.test_features_c5), self.nb_median5.predict(self.test_features_c5)\n",
    "    def __predict_median0_c2(self, path):\n",
    "        corpus_filtro5_devset = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset['text']\n",
    "        median_devlist=corpus_filtro5_devset['median']\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.median_list_c0=[]\n",
    "        for med in median_devlist:\n",
    "            if med == 0:\n",
    "                self.median_list_c0.append(1)\n",
    "            else:\n",
    "                self.median_list_c0.append(0)\n",
    "\n",
    "        self.test_features_c0 = vectorizer.transform(text_devlist)\n",
    "        return self.nb_median0.predict_proba(self.test_features_c0), self.nb_median0.predict(self.test_features_c0)\n",
    "    def __clasiffier_2_train(self):\n",
    "        corpus_filtro5_trainingset_c2 = pandas.read_csv(\"corpus_filtro5_trainingset_c2.csv\",encoding='utf-8')\n",
    "        text_list_c2=corpus_filtro5_trainingset_c2['text'][:]\n",
    "        category_list_c2=corpus_filtro5_trainingset_c2['category'][:]\n",
    "        pos_c2=0\n",
    "        for t in text_list_c2:\n",
    "            if type(t) != str:\n",
    "                text_list_c2[pos_c2]=\"NaN\"\n",
    "            pos_c2+=1\n",
    "        self.train_features_class2 = self.vectorizer.fit_transform(text_list_c2)\n",
    "\n",
    "        self.nb_2 = MultinomialNB()\n",
    "        self.nb_2.fit(self.train_features_class2, category_list_c2)\n",
    "\n",
    "    \n",
    "    def predict_median(self, path):\n",
    "        self.c1_pred, self.c1_predict=self.__predict_median1_c2(path)\n",
    "        self.c2_pred, self.c2_predict=self.__predict_median2_c2(path)\n",
    "        self.c3_pred, self.c3_predict=self.__predict_median3_c2(path)\n",
    "        self.c4_pred, self.c4_predict=self.__predict_median4_c2(path)\n",
    "        self.c5_pred, self.c5_predict=self.__predict_median5_c2(path)\n",
    "        self.c0_pred, self.c0_predict=self.__predict_median0_c2(path)        \n",
    "        self.predicts=[]\n",
    "        proba=[]\n",
    "        for i in range( len(self.c1_pred) ):\n",
    "            maximum=np.max([self.c1_pred[i][1],self.c2_pred[i][1],self.c3_pred[i][1],self.c4_pred[i][1],self.c5_pred[i][1],self.c0_pred[i][1]])\n",
    "            if(maximum == self.c1_pred[i][1]):\n",
    "                self.predicts.append(1)\n",
    "            elif(maximum == self.c2_pred[i][1]):\n",
    "                self.predicts.append(2)\n",
    "            elif(maximum == self.c3_pred[i][1]):\n",
    "                self.predicts.append(3)\n",
    "            elif(maximum == self.c4_pred[i][1]):\n",
    "                self.predicts.append(4)\n",
    "            elif(maximum == self.c5_pred[i][1]):\n",
    "                self.predicts.append(5)\n",
    "            elif(maximum == self.c0_pred[i][1]):\n",
    "                self.predicts.append(0)\n",
    "            proba.append(maximum)\n",
    "        self.m0 = []\n",
    "        self.m1 = []\n",
    "        self.m2 = []\n",
    "        self.m3 = []\n",
    "        self.m4 = []\n",
    "        self.m5 = []\n",
    "\n",
    "        for pred in self.predicts:\n",
    "            if pred == 0:\n",
    "                self.m0.append(1)\n",
    "                self.m1.append(0)\n",
    "                self.m2.append(0)\n",
    "                self.m3.append(0)\n",
    "                self.m4.append(0)\n",
    "                self.m5.append(0)\n",
    "            elif pred == 1:\n",
    "                self.m0.append(0)\n",
    "                self.m1.append(1)\n",
    "                self.m2.append(0)\n",
    "                self.m3.append(0)\n",
    "                self.m4.append(0)\n",
    "                self.m5.append(0)\n",
    "            elif pred == 2:\n",
    "                self.m0.append(0)\n",
    "                self.m1.append(0)\n",
    "                self.m2.append(1)\n",
    "                self.m3.append(0)\n",
    "                self.m4.append(0)\n",
    "                self.m5.append(0)\n",
    "            elif pred == 3:\n",
    "                self.m0.append(0)\n",
    "                self.m1.append(0)\n",
    "                self.m2.append(0)\n",
    "                self.m3.append(1)\n",
    "                self.m4.append(0)\n",
    "                self.m5.append(0)\n",
    "            elif pred == 4:\n",
    "                self.m0.append(0)\n",
    "                self.m1.append(0)\n",
    "                self.m2.append(0)\n",
    "                self.m3.append(0)\n",
    "                self.m4.append(1)\n",
    "                self.m5.append(0)\n",
    "            elif pred == 5:\n",
    "                self.m0.append(0)\n",
    "                self.m1.append(0)\n",
    "                self.m2.append(0)\n",
    "                self.m3.append(0)\n",
    "                self.m4.append(0)\n",
    "                self.m5.append(1)\n",
    "\n",
    "        \n",
    "        \n",
    "        corpus_filtro5_devset_c2 = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist_c2=corpus_filtro5_devset_c2['text'][:]\n",
    "        category_devlist_c2=corpus_filtro5_devset_c2['category'][:]\n",
    "        pos=0\n",
    "        for t in text_devlist_c2:\n",
    "            if type(t) != str:\n",
    "                text_devlist_c2[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features_class2 = self.vectorizer.transform(text_devlist_c2)\n",
    "                    \n",
    "        return self.predicts, proba, self.nb_2.predict(self.test_features_class2)\n",
    "\n",
    "class Clasificador3:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english')\n",
    "        self.__clasiffier_3_train()\n",
    "    def __clasiffier_3_train(self):\n",
    "        corpus_filtro5_trainingset_c3 = pandas.read_csv(\"corpus_filtro5_trainingset_c3.csv\",encoding='utf-8')\n",
    "        text_list=corpus_filtro5_trainingset_c3['text'][:]\n",
    "        category_list=corpus_filtro5_trainingset_c3['category'][:]\n",
    "        pos=0\n",
    "        for t in text_list:\n",
    "            if type(t) != str:\n",
    "                text_list[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.train_features = self.vectorizer.fit_transform(text_list)\n",
    "        self.nb = MultinomialNB()\n",
    "        self.nb.fit(self.train_features , category_list)\n",
    "    def predict_clasiffier_1(self, path, test):\n",
    "        corpus_filtro5_devset_c3 = pandas.read_csv(path,encoding='utf-8')\n",
    "        text_devlist=corpus_filtro5_devset_c3['text'][:]\n",
    "        category_devlist=corpus_filtro5_devset_c3[test][:]\n",
    "        pos=0\n",
    "        for t in text_devlist:\n",
    "            if type(t) != str:\n",
    "                text_devlist[pos]=\"NaN\"\n",
    "            pos+=1\n",
    "        self.test_features = self.vectorizer.transform(text_devlist)\n",
    "        return self.nb.predict(self.test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "clasificador = Clasificador1(\"corpus_filtro5_trainingset.csv\")\n",
    "print(clasificador.predict_clasiffier_1(\"corpus_filtro5_devset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 2, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 0, 3, 3, 0, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 3, 0, 3, 4, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 2, 3, 3, 0, 0, 3, 0, 0, 2, 3, 3, 3, 1, 0, 1, 0, 3, 0, 3, 0, 3, 0, 2, 3, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 3, 0, 0, 0, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 1, 3, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 3, 3, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 3, 0, 0, 1, 0, 0, 1, 3, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 3, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 1, 0, 1, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 1, 3, 0, 0, 3, 0, 0, 1, 0, 0, 3, 0, 0, 3, 0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 0, 0, 2, 3, 0, 0, 3, 1, 0, 3, 0, 3, 0, 0, 0, 0, 1, 3, 4, 0, 1, 0, 0, 1, 0, 3, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 2, 1, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 1, 3, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 4, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 3, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 0, 2, 1, 0, 0, 2, 3, 0, 0, 0, 0, 0, 1, 0, 0, 3, 3, 0, 0, 0, 0, 1, 3] \n",
      " [0.14429728069433556, 0.0080072433066223792, 0.35024084320958127, 0.016118143031049269, 0.0076756058499228485, 0.0046635985122550289, 0.98196383441178214, 0.3919272691225183, 0.4345699805097451, 0.97340988021403085, 0.90824188105277581, 0.022761955680062333, 0.28486307004560124, 0.0052041928642125417, 0.054434513926639477, 0.01101016957662436, 0.19108193594181938, 0.10321056580841052, 0.0021193500931978139, 0.0028184421024604047, 0.61108263680464214, 0.91225116466332112, 0.986366467065666, 0.45213819240024383, 0.0052439474914487113, 0.11432590413252407, 0.026207937870492536, 0.72570075307723247, 0.69045310216658473, 0.0025068213657334507, 0.0019731147684367402, 0.94113068602874517, 0.056117689233400381, 0.001460438229593439, 3.7039746579166465e-05, 0.89643305150974018, 0.77620989891625158, 0.75068883229341443, 0.0028124480355045524, 0.00071678966739919213, 0.0090779721635872775, 0.97441601551972279, 0.97721342255817922, 2.2315299751133907e-05, 0.00014894861423728493, 0.17649006247745436, 0.87667505298359005, 0.036479223396900663, 0.00050583495223068843, 0.020305308272462321, 0.035572239471414864, 0.0038097210169154737, 0.0045112432685676012, 0.97281582649798493, 0.020863348307146836, 0.37922356442691574, 0.69567423384538285, 0.00094136662955099284, 0.054986865059227445, 0.99959922262482881, 0.44782576996652756, 0.00093857578360165793, 0.0069847336421577462, 0.87687005240564431, 0.044382739548413569, 0.97857049042456035, 0.41153567117556961, 0.15150853157624031, 0.56323364453409275, 0.00028809242404178811, 0.75542875409164978, 0.015157061703714076, 0.0046615060619971298, 0.034525258054678848, 0.00058600435496030257, 0.046865096225525912, 0.047305845268291201, 0.0058079551249128978, 8.9016041461607289e-05, 0.57355554833187172, 0.27166359727333866, 0.0037344193089414984, 0.019035456112175376, 0.99067797219291476, 0.93820706694141198, 0.027785509259751925, 0.99735410966656757, 0.92648320327904121, 0.0024508397486972663, 0.00272909880651068, 0.2226770297872947, 0.0012588904763844184, 0.12427692074938901, 0.39391231408238603, 0.00098027985046913579, 0.00058239007476654952, 0.73902295761507208, 0.83037778138802154, 0.02135361055009749, 0.86057546121860651, 0.0016196988049360216, 0.99426358997566111, 0.006678166824387822, 0.002740357342112022, 0.43236069474687766, 0.19789884514042996, 0.99123414855704806, 0.0024994669652573217, 0.46103217109470179, 0.0039143416924231735, 0.995652028389716, 0.14175640753745999, 0.015643584860940477, 0.34234451316111764, 0.6840635686195311, 0.0025420740997090267, 0.80534011670242511, 0.049284886240637558, 0.076714809137793452, 0.026680690331498141, 0.00064958001566129927, 0.003229956414181839, 0.002358694428211809, 0.42666557221408424, 0.050205462986950666, 0.0055198533721540254, 0.78914030326199891, 0.47279628925516065, 1.0, 0.05158270490024669, 0.76209577317966004, 0.00063876272950257095, 0.00022287861640176741, 0.12692100364915124, 0.91757207533094209, 0.005676705205761432, 0.0011351163322532219, 0.68119874866082775, 0.9142739705191012, 0.18919770535100366, 0.47337033559618447, 0.43987572878368236, 0.36732378069204885, 0.055850560352317481, 0.95371462626578907, 0.21851153940503615, 0.038785510634036645, 0.36391909433589081, 0.96372332393175864, 0.11657678987842308, 0.11264348825832496, 0.0038262169654943428, 0.00065562782513690682, 0.13324324564221829, 0.0037780249410837227, 0.32122521174523228, 0.18517094418058305, 0.65432712150137584, 0.90929773812560055, 0.73590241687082425, 0.38252493592928827, 0.39034089722680887, 0.33200288642959785, 0.61263920720150933, 0.35651007150351616, 0.9854458800114424, 0.99904215568068222, 1.0, 0.98440709122067771, 0.94373441424300419, 0.39964632187545551, 0.0024471249862314374, 8.344107900516854e-05, 0.0031926153049918935, 2.6350352241382787e-05, 0.0001787171231043077, 0.19303744225557884, 0.032166501444382392, 0.0062807115745097397, 0.78812277913994611, 0.029821214275007713, 0.95178028763237299, 0.091131223650859669, 0.56046061795132518, 0.29709293641373369, 0.2481938252763819, 0.096526154483536228, 0.12772355560042087, 0.34303101232784822, 0.0003206890567062446, 0.0063733523898476009, 0.49446773168145242, 0.92813013170498992, 0.73236266418719953, 0.88428453724005551, 0.015937262512329756, 0.00044786980873318913, 0.013562179518497872, 0.38329762014384872, 0.00041792090924298847, 0.96944659341996797, 0.65077100626294282, 0.0025142999633445168, 0.96627151312614368, 0.046815647173780367, 0.42371215859941319, 0.035004305488164175, 0.060453364645528813, 0.026630413163596659, 0.36973838459584396, 0.023466907002093435, 0.00028288647540085089, 0.39023905055167579, 0.0012068878843652638, 0.11458690303856493, 0.3785980876347313, 0.97271031785455786, 0.002809888042011161, 0.96986733878388043, 0.00228686555961467, 0.91961694318982046, 0.00026362331612644115, 0.91041264389807464, 0.99561525996965039, 0.00024497506382424844, 0.025539628218103835, 0.0029092955981644413, 0.88274489747149953, 0.3714107513791029, 0.038952346559485859, 0.03695549850634617, 0.23758738407202604, 0.0015230188123740583, 0.91308065352916934, 0.90317551044063182, 0.95719654391414366, 0.68849815155848904, 0.89752827989006745, 0.016827747423079329, 0.00035332103568040392, 0.00066842770489776275, 0.0011798031951847385, 0.0023344314037063445, 0.0016035622287915749, 0.053502122285594735, 0.096466019470554623, 0.03955197986209541, 0.21307364285048955, 0.008176932518811705, 0.7335202324186989, 0.59413802542010941, 0.0024665913216180392, 0.90681374024696604, 0.55890837243639191, 0.90698761564041352, 0.34532798374393914, 0.031680835950379911, 0.14809095602424621, 0.024895537208728179, 0.066904230858503264, 0.19989999897232405, 0.99827885895725821, 0.87019925204515503, 0.54559124529776171, 0.77962151157413606, 0.22032875434412996, 0.43599671223668185, 0.19446236431911579, 0.035753937705753566, 0.00719964619534559, 0.0014531597949648772, 0.43566390912788311, 0.51134240668262698, 0.57514434759837019, 0.10660886227601203, 0.019558158390987793, 0.95450347689384174, 0.87965417703193693, 0.0052364110687431343, 0.39070140537716352, 9.1232752011564968e-05, 0.0041523213012498068, 0.5959871728274192, 0.0020680401269275901, 0.0059262673380023351, 0.0021744197053750153, 0.00011307498361658828, 0.15813546732979927, 0.61047128779388271, 0.70539003538308809, 0.90188924407770588, 0.010845091439923227, 0.00070979928717712713, 0.96984838184206446, 0.98981984050219096, 0.36599691607232676, 0.00013828752206180176, 0.89855315916919865, 0.52638915749306869, 0.048029538159183073, 0.9999985181392399, 0.34310918940339441, 0.90138238782007862, 0.85202685547155899, 0.99784300295429873, 0.021926001831797191, 0.130371094003501, 0.80283495222812651, 0.66307734736533885, 0.92587475429168853, 0.0016490651354668325, 0.99864500851623161, 0.051547442365466538, 0.73184597771521509, 0.89608955506091148, 0.73969318978253173, 0.64817084243498435, 0.58293711252571623, 0.019904215175477701, 0.0057169332000690659, 0.13251655897579043, 0.012069992259723253, 0.84510811092351312, 0.42059144091401757, 0.014639575867452773, 0.012091178113272072, 0.013232251493347254, 0.13958403080752479, 0.00078796118261733716, 0.085366092263206253, 0.99997920402623897, 0.75752890555434738, 0.06829175853047928, 0.089995415580288035, 0.23375695096705773, 0.0063929272683467559, 0.014188611844444844, 0.9010868420647431, 0.98237001502898114, 0.092045321387619616, 0.0032114034315022166, 0.055850560352317481, 0.73172526844088159, 0.43161639757630721, 0.87466596765173443, 0.96798114498687371, 0.056002132685142286, 0.1678184482712676, 0.28142010641819321, 0.00152338160353775, 0.017921208677495509, 0.049506241760469, 0.003048590506471072, 0.97140059088500186, 0.020683565724845112, 0.010335316585720112, 0.34993515985084916, 0.99725680008892548, 0.96949378660858765, 0.0043708802002452333, 0.65891645122403508, 0.00068388694543866061, 0.85242941253596527, 0.0016257701471376624, 0.023945565876373875, 0.16689453559292172, 0.031830721743036146, 0.70395081119402603, 0.12182952172325966, 0.0027182297457409733, 0.00080175736793534089, 0.58250906809924485, 0.00053055115875320024, 0.69577736239809773, 0.00019079285506215369, 0.99965388493261553, 0.099642904638657825, 0.99525986703564129, 0.17606710212136603, 0.3224350080997439, 0.0048654186525775812, 0.68704246819461257, 0.933775762606596, 0.035958221672589843, 0.10719890472975678, 0.7002998849035279, 0.048479705885755647, 0.71147467151184773, 0.0011934276351464625, 0.25653697869367925, 0.012879175862637841, 0.015832562463265844, 0.31563833392504564, 0.012160815634123284, 0.94095072875651597, 0.82453267228880534, 0.0045474811358310616, 0.069015551016861493, 0.33256289059204031, 0.33069159369154894, 0.97521538879041825, 0.16301915570271436, 0.0013495498771674745, 0.99012690157563388, 0.65934334567164576, 0.81956091286876509, 0.0090361665225397597, 0.80448684774728541, 0.0009274135429479071, 0.23484030499298744, 0.98790965454721735, 0.95225105627228024, 0.074022563057014726, 0.076588618769336841, 0.99973739014782304, 0.16097595421432892, 0.00048176859201790528, 0.99179288938720744, 0.37661881057872687, 0.0028323292516975033, 0.35943473625637867, 0.37861616671482895, 0.0092375025678438742, 0.99998991439967644, 0.17086065711645951, 0.9697108040268515, 0.96611259818750417, 0.36712273560164127, 0.98133838975632626, 0.007465586400897054, 0.59270918557639773, 0.0033008174033928197, 0.00041923485458262829, 0.84759385090770278, 0.16097849716462984, 6.9618215109673384e-05, 0.39519063561774548, 0.55280463634892063, 0.018706637132502243, 0.017405524633644594, 0.91551108238656265, 0.085725997869408058, 0.0035744283527757892, 2.4309114039957809e-05, 0.97756090560175291, 0.80443505423274064, 0.80424863230802768, 0.66203020106705079, 0.63807391659441892, 0.055039097101979732, 0.99999825686150112, 0.0025125547006474716, 0.87103514301434459, 0.030654508328306702, 0.0002179906073484593, 0.52276862023241477, 0.93933423664312554, 0.11598791404506259, 0.046492895146821936, 0.99997184903042091, 0.052566775344736152, 0.79224437212656096, 0.040334849200517248, 0.89324220822196165, 0.039861280429332728, 0.0033946429759076865, 0.059285724290360463, 0.007147840736785779, 0.15523005304282397, 0.0098382381174296415, 0.46691631165850789, 0.035941527393280659, 0.00032170031039341427, 0.12785544527531231, 0.85389545168740499, 0.87460062433517893, 0.17889615538009965, 0.96559698359111346, 0.0070330261725825597, 0.0067343336919272632, 0.0052954646636659553, 0.03599833840400405, 0.069894154497802202, 0.89354461655860917, 0.10583633415621027, 0.12803736537137578, 0.28801426919049955, 0.36749881470145773, 0.34424189837662833, 0.68890090176262686, 0.99889801499030739, 0.89387017039684358, 0.57261467532933963, 0.092045321387619616, 0.0003121811740857375, 0.80131663563554001, 0.21280184336524302, 0.13076435259040134, 0.12825235407745136, 0.00293386302555194, 0.18937327757308148, 0.12769612050507692, 0.039234952354873655, 0.016381548215194344, 0.68202580191862017, 0.89203684534975802, 0.54194557088016471, 0.00083139875510635799, 0.98623287340258314, 0.90156753127005185, 0.015650156118461305, 0.099347261855619054, 0.70038684620120484, 0.00014801813935286215, 0.95950240052508939, 0.99829160091829816, 0.85146467527120129, 0.99999943148289638, 0.99376728778820844, 0.012625910245952957, 0.51070661060369571, 0.30246953705664792, 0.0053854177837153062, 0.66821010885839727, 0.98408618695470007, 0.29950229258173588, 0.016458335838560111, 0.11119546685204719, 0.0040646734873674836, 0.01408740063680637, 0.32082430010434254, 0.027630750581664244, 0.015299428809791765, 0.48106974761195886, 0.80605019932929778, 0.064612516468971176, 0.98309781720946221, 0.036934415782787236, 0.002667761716450512, 0.98824946846915618, 0.34197831923304545, 0.0052969280908907413, 0.99946172381298204, 0.00052291478638703359, 0.85962354610392089, 0.030844968895443258, 0.29470855479106739, 0.99005705437252667, 0.78471234960098424, 0.37934236995040865, 0.0014378469635631331, 0.52046321593099554, 0.64033951967052383, 0.020643965451711703, 0.12066740006457852, 0.071090245268163613, 0.46677757165448758, 0.48398588952383076, 0.0021968961116843539, 0.0034563485023099668, 0.99093724263445293, 0.99999998337528961, 0.00078796118261733716, 0.72348094901987137, 0.65017129741786861, 0.87015244411219295, 0.036690117183707023, 0.23042652454262003, 6.782029027727067e-05, 0.00029613588651419446, 0.12378361660375956, 0.3562520739420531, 0.083677227387034203, 0.93947736341313848, 0.0091795531592243299, 0.93923396298318274, 0.91151528511334812, 0.95909163286416788, 0.64147519184076163, 0.99999999940899897, 0.084463462823075652, 0.032588707082287789, 0.97828169276093091, 0.48715108963214265, 0.96396833528693615, 0.025560406557422994, 0.074148397956844608, 0.00014610222587873369, 0.16858587633770786, 0.57746442351128024, 0.9033103230370545, 0.14754968965890408, 0.66035505836714947, 0.0036937818128714409, 3.594995649608121e-05, 0.74354352391247902, 0.047826623575909354, 0.44354546937425193, 0.21167363455681512, 0.7619775945185342, 0.070058811387249911, 0.09087995219556104, 0.012909066501428845, 0.00052372080776614974, 0.9946646022293727, 0.027497133827515167, 0.32999270622049515, 0.038551779508066049, 0.8577493370887368, 0.92609459622255996, 0.04172568778569323, 0.033265422176030471, 0.98112938451455567, 0.074148397956844608, 0.95930380778474056, 0.92989226735689223, 2.2266896548025219e-05, 0.036594157347716899, 0.99984040088107562, 0.90341802503451152, 0.12924395298441638, 0.8052377641953169, 0.0084891343649305898, 4.3976712769277791e-05] \n",
      " [0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0\n",
      " 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
      " 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
      " 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
      " 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
      " 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
      " 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
      " 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
      " 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
      " 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1\n",
      " 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "clasificador=Clasificador2()\n",
    "predict, proba, humor_prob=clasificador.predict_median(\"corpus_filtro5_devset_c2.csv\")\n",
    "print(predict, \"\\n\", proba, \"\\n\", humor_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
      " 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1\n",
      " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
      " 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0\n",
      " 0 1 1 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
      " 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1\n",
      " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1\n",
      " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "clasificador = Clasificador3()\n",
    "print(clasificador.predict_clasiffier_1(\"corpus_filtro5_devset_c3.csv\", \"category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medidas para el clasificador 1\n",
      "Precision:  0.7427745664739884\n",
      "Recall:  0.803125\n",
      "Accuracy  0.5492063492063493\n",
      "E-measure  0.7717717717717717\n",
      "Medidas para el clasificador 2\n",
      "Precision:  0.7710144927536232\n",
      "Recall:  0.8036253776435045\n",
      "Accuracy  0.5528846153846154\n",
      "E-measure  0.7869822485207102\n",
      "Medidas para el clasificador 3\n",
      "Precision:  0.7523584905660378\n",
      "Recall:  0.8285714285714286\n",
      "Accuracy  0.6794871794871795\n",
      "E-measure  0.788627935723115\n",
      "Predicter Median 1\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 12 \t|\t 28 \t|\n",
      "system negative|\t 92 \t|\t 492 \t|\n",
      "Presicion  0.3\n",
      "Recall  0.11538461538461539\n",
      "Accuracy  0.0641025641025641\n",
      "E-measure  0.16666666666666669\n",
      "Predicter Median 2\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 7 \t|\t 19 \t|\n",
      "system negative|\t 85 \t|\t 513 \t|\n",
      "Presicion  0.2692307692307692\n",
      "Recall  0.07608695652173914\n",
      "Accuracy  0.041666666666666664\n",
      "E-measure  0.11864406779661017\n",
      "Predicter Median 3\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 19 \t|\t 104 \t|\n",
      "system negative|\t 82 \t|\t 419 \t|\n",
      "Presicion  0.15447154471544716\n",
      "Recall  0.18811881188118812\n",
      "Accuracy  0.1971153846153846\n",
      "E-measure  0.16964285714285715\n",
      "Predicter Median 4\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 5 \t|\n",
      "system negative|\t 70 \t|\t 549 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 5\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 15 \t|\t 609 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 0\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 209 \t|\t 221 \t|\n",
      "system negative|\t 33 \t|\t 161 \t|\n",
      "Presicion  0.48604651162790696\n",
      "Recall  0.8636363636363636\n",
      "Accuracy  0.6891025641025641\n",
      "E-measure  0.6220238095238095\n",
      "Confusion matrix\n",
      "\t 0 \t 1 \t 2 \t 3 \t 4 \t 5 \n",
      "0 \t 209 \t 67 \t 52 \t 62 \t 32 \t 8\n",
      "1 \t 11 \t 12 \t 5 \t 9 \t 2 \t 1\n",
      "2 \t 2 \t 2 \t 7 \t 8 \t 6 \t 1\n",
      "3 \t 19 \t 23 \t 27 \t 19 \t 30 \t 5\n",
      "4 \t 1 \t 0 \t 1 \t 3 \t 0 \t 0\n",
      "5 \t 0 \t 0 \t 0 \t 0 \t 0 \t 0\n",
      "Macro averaging\n",
      "Precision  0.3768595041322314  Recall 0.3768595041322314  F 0.3768595041322314\n",
      "Micro averaging\n",
      "Precision  0.03495813759568722  Recall 0.14427281590257493  F 0.0562794411478525\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro5_devset = pandas.read_csv(\"corpus_filtro5_devset.csv\",encoding='utf-8')\n",
    "corpus_filtro5_devset_c2 = pandas.read_csv(\"corpus_filtro5_devset_c2.csv\",encoding='utf-8')\n",
    "corpus_filtro5_devset_c3 = pandas.read_csv(\"corpus_filtro5_devset_c3.csv\",encoding='utf-8')\n",
    "\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 1\n",
    "list_cat_1 = []\n",
    "list_ecat_1 = []\n",
    "\n",
    "\n",
    "list_cat_1=corpus_filtro5_devset['category'][:]\n",
    "\n",
    "clasificador = Clasificador1(\"corpus_filtro5_trainingset.csv\")\n",
    "list_ecat_1  = clasificador.predict_clasiffier_1(\"corpus_filtro5_devset.csv\")\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 2\n",
    "list_cat_2 = []\n",
    "list_ecat_2 = []\n",
    "list_edem_2=[]\n",
    "medians = []\n",
    "\n",
    "\n",
    "list_cat_2=corpus_filtro5_devset_c2['category'][:]\n",
    "clasificador2=Clasificador2()\n",
    "list_emed_2, _, list_ecat_2=clasificador2.predict_median(\"corpus_filtro5_devset_c2.csv\")\n",
    "medians=corpus_filtro5_devset_c2['median'][:];\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 3\n",
    "list_cat_3 = []\n",
    "list_ecat_3 = []\n",
    "\n",
    "list_cat_3=corpus_filtro5_devset_c3['category'][:]\n",
    "clasificador3 = Clasificador3()\n",
    "list_ecat_3=clasificador3.predict_clasiffier_1(\"corpus_filtro5_devset_c3.csv\", \"category\")\n",
    "\n",
    "\n",
    "#Variables para clasificador 1\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_1) ):\n",
    "    if(list_cat_1[i] == 1 and list_ecat_1[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_1[i] == 1 and list_ecat_1[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_1[i] == 0 and list_ecat_1[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "print(\"Medidas para el clasificador 1\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "#Variables para el clasificador 2\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_2) ):\n",
    "    if(list_cat_2[i] == 1 and list_ecat_2[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_2[i] == 1 and list_ecat_2[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_2[i] == 0 and list_ecat_2[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "\n",
    "print(\"Medidas para el clasificador 2\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "#Variables para el clasificador 3\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_3) ):\n",
    "    if(list_cat_3[i] == 1 and list_ecat_3[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_3[i] == 1 and list_ecat_3[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_3[i] == 0 and list_ecat_3[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "print(\"Medidas para el clasificador 3\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "#Variables para las medianas\n",
    "tp_m1=0\n",
    "fp_m1=0\n",
    "tn_m1=0\n",
    "fn_m1=0\n",
    "\n",
    "tp_m2=0\n",
    "fp_m2=0\n",
    "tn_m2=0\n",
    "fn_m2=0\n",
    "\n",
    "tp_m3=0\n",
    "fp_m3=0\n",
    "tn_m3=0\n",
    "fn_m3=0\n",
    "\n",
    "tp_m4=0\n",
    "fp_m4=0\n",
    "tn_m4=0\n",
    "fn_m4=0\n",
    "\n",
    "tp_m5=0\n",
    "fp_m5=0\n",
    "tn_m5=0\n",
    "fn_m5=0\n",
    "\n",
    "tp_m0=0\n",
    "fp_m0=0\n",
    "tn_m0=0\n",
    "fn_m0=0\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c1) ):\n",
    "    if(clasificador2.m1[i] == 1 and medians[i]==1):\n",
    "        tp_m1+=1\n",
    "    elif(clasificador2.m1[i] == 0 and medians[i]==1):\n",
    "        fn_m1+=1\n",
    "    elif(clasificador2.m1[i] == 1 and medians[i]!=1):\n",
    "        fp_m1+=1\n",
    "    else:\n",
    "        tn_m1+=1\n",
    "\n",
    "p_m1=-1\n",
    "r_m1=-1\n",
    "if(tp_m1 + fp_m1 > 0):\n",
    "    p_m1 = tp_m1/(tp_m1 + fp_m1)\n",
    "if(tp_m1 + fn_m1):\n",
    "    r_m1 = tp_m1 / (tp_m1 + fn_m1)\n",
    "\n",
    "print(\"Predicter Median 1\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m1,\"\\t|\\t\", fp_m1, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m1, \"\\t|\\t\", tn_m1, \"\\t|\")\n",
    "if p_m1 > 0:\n",
    "    print(\"Presicion \", p_m1)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m1 > 0:\n",
    "    print(\"Recall \", r_m1)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m1 > 0 and r_m1 > 0):\n",
    "    print(\"Accuracy \", (tp_m1 + fp_m1) / (tp_m1 + fp_m1 + tn_m1 + fn_m1))\n",
    "    print(\"E-measure \", 2*p_m1*r_m1/(p_m1 + r_m1))\n",
    "\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c2) ):\n",
    "    if(clasificador2.m2[i] == 1 and medians[i]==2):\n",
    "        tp_m2+=1\n",
    "    elif(clasificador2.m2[i] == 0 and medians[i]==2):\n",
    "        fn_m2+=1\n",
    "    elif(clasificador2.m2[i] == 1 and medians[i]!=2):\n",
    "        fp_m2+=1\n",
    "    else:\n",
    "        tn_m2+=1\n",
    "\n",
    "p_m2=-1\n",
    "r_m2=-1\n",
    "if(tp_m2 + fp_m2 > 0):\n",
    "    p_m2 = tp_m2/(tp_m2 + fp_m2)\n",
    "if(tp_m2 + fn_m2):\n",
    "    r_m2 = tp_m2 / (tp_m2 + fn_m2)\n",
    "\n",
    "print(\"Predicter Median 2\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m2,\"\\t|\\t\", fp_m2, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m2, \"\\t|\\t\", tn_m2, \"\\t|\")\n",
    "if p_m2 > 0:\n",
    "    print(\"Presicion \", p_m2)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m2 > 0:\n",
    "    print(\"Recall \", r_m2)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m2 > 0 and r_m2 > 0):\n",
    "    print(\"Accuracy \", (tp_m2 + fp_m2) / (tp_m2 + fp_m2 + tn_m2 + fn_m2))\n",
    "    print(\"E-measure \", 2*p_m2*r_m2/(p_m2 + r_m2))\n",
    "    \n",
    "for i in range( len(clasificador2.median_list_c3) ):\n",
    "    if(clasificador2.m3[i] == 1 and medians[i]==3):\n",
    "        tp_m3+=1\n",
    "    elif(clasificador2.m3[i] == 0 and medians[i]==3):\n",
    "        fn_m3+=1\n",
    "    elif(clasificador2.m3[i] == 1 and medians[i]!=3):\n",
    "        fp_m3+=1\n",
    "    else:\n",
    "        tn_m3+=1\n",
    "\n",
    "p_m3=-1\n",
    "r_m3=-1\n",
    "if(tp_m3 + fp_m3 > 0):\n",
    "    p_m3 = tp_m3/(tp_m3 + fp_m3)\n",
    "if(tp_m3 + fn_m3):\n",
    "    r_m3 = tp_m3 / (tp_m3 + fn_m3)\n",
    "\n",
    "print(\"Predicter Median 3\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m3,\"\\t|\\t\", fp_m3, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m3, \"\\t|\\t\", tn_m3, \"\\t|\")\n",
    "if p_m3 > 0:\n",
    "    print(\"Presicion \", p_m3)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m3 > 0:\n",
    "    print(\"Recall \", r_m3)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m3 > 0 and r_m3 > 0):\n",
    "    print(\"Accuracy \", (tp_m3 + fp_m3) / (tp_m3 + fp_m3 + tn_m3 + fn_m3))\n",
    "    print(\"E-measure \", 2*p_m3*r_m3/(p_m3 + r_m3))\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c4) ):\n",
    "    if(clasificador2.m4[i] == 1 and medians[i]==4):\n",
    "        tp_m4+=1\n",
    "    elif(clasificador2.m4[i] == 0 and medians[i]==4):\n",
    "        fn_m4+=1\n",
    "    elif(clasificador2.m4[i] == 1 and medians[i]!=4):\n",
    "        fp_m4+=1\n",
    "    else:\n",
    "        tn_m4+=1\n",
    "\n",
    "p_m4=-1\n",
    "r_m4=-1\n",
    "if(tp_m4 + fp_m4 > 0):\n",
    "    p_m4 = tp_m4/(tp_m4 + fp_m4)\n",
    "if(tp_m4 + fn_m4):\n",
    "    r_m4 = tp_m4 / (tp_m4 + fn_m4)\n",
    "\n",
    "print(\"Predicter Median 4\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m4,\"\\t|\\t\", fp_m4, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m4, \"\\t|\\t\", tn_m4, \"\\t|\")\n",
    "if p_m4 > 0:\n",
    "    print(\"Presicion \", p_m4)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m4 > 0:\n",
    "    print(\"Recall \", r_m4)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m4 > 0 and r_m4 > 0):\n",
    "    print(\"Accuracy \", (tp_m4 + fp_m4) / (tp_m4 + fp_m4 + tn_m4 + fn_m4))\n",
    "    print(\"E-measure \", 2*p_m4*r_m4/(p_m4 + r_m4))\n",
    "\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c5) ):\n",
    "    if(clasificador2.m5[i] == 1 and medians[i]==5):\n",
    "        tp_m5+=1\n",
    "    elif(clasificador2.m5[i] == 0 and medians[i]==5):\n",
    "        fn_m5+=1\n",
    "    elif(clasificador2.m5[i] == 1 and medians[i]!=5):\n",
    "        fp_m5+=1\n",
    "    else:\n",
    "        tn_m5+=1\n",
    "\n",
    "p_m5=-1\n",
    "r_m5=-1\n",
    "if(tp_m5 + fp_m5 > 0):\n",
    "    p_m5 = tp_m5/(tp_m5 + fp_m5)\n",
    "if(tp_m5 + fn_m5):\n",
    "    r_m5 = tp_m5 / (tp_m5 + fn_m5)\n",
    "\n",
    "print(\"Predicter Median 5\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m5,\"\\t|\\t\", fp_m5, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m5, \"\\t|\\t\", tn_m5, \"\\t|\")\n",
    "if p_m5 > 0:\n",
    "    print(\"Presicion \", p_m5)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m5 > 0:\n",
    "    print(\"Recall \", r_m5)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m5 > 0 and r_m5 > 0):\n",
    "    print(\"Accuracy \", (tp_m5 + fp_m5) / (tp_m5 + fp_m5 + tn_m5 + fn_m5))\n",
    "    print(\"E-measure \", 2*p_m5*r_m5/(p_m5 + r_m5))\n",
    "    \n",
    "for i in range( len(clasificador2.median_list_c0) ):\n",
    "    if(clasificador2.m0[i] == 1 and medians[i]==0):\n",
    "        tp_m0+=1\n",
    "    elif(clasificador2.m0[i] == 0 and medians[i]==0):\n",
    "        fn_m0+=1\n",
    "    elif(clasificador2.m0[i] == 1 and medians[i]!=0):\n",
    "        fp_m0+=1\n",
    "    else:\n",
    "        tn_m0+=1\n",
    "\n",
    "p_m0=-1\n",
    "r_m0=-1\n",
    "if(tp_m0 + fp_m0 > 0):\n",
    "    p_m0 = tp_m0/(tp_m0 + fp_m0)\n",
    "if(tp_m0 + fn_m0):\n",
    "    r_m0 = tp_m0 / (tp_m0 + fn_m0)\n",
    "\n",
    "print(\"Predicter Median 0\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m0,\"\\t|\\t\", fp_m0, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m0, \"\\t|\\t\", tn_m0, \"\\t|\")\n",
    "if p_m0 > 0:\n",
    "    print(\"Presicion \", p_m0)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m0 > 0:\n",
    "    print(\"Recall \", r_m0)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m0 > 0 and r_m0 > 0):\n",
    "    print(\"Accuracy \", (tp_m0 + fp_m0) / (tp_m0 + fp_m0 + tn_m0 + fn_m0))\n",
    "    print(\"E-measure \", 2*p_m0*r_m0/(p_m0 + r_m0))\n",
    "    \n",
    "print(\"Confusion matrix\")\n",
    "print(\"\\t 0 \\t 1 \\t 2 \\t 3 \\t 4 \\t 5 \")\n",
    "\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c0_predict)):\n",
    "    if(medians[pos] != 0 and clasificador2.predicts[pos] == 0):\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"0 \\t\", tp_m0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c1_predict)):\n",
    "    if(medians[pos] != 1 and clasificador2.predicts[pos] == 1):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"1 \\t\", conf0, \"\\t\", tp_m1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c2_predict)):\n",
    "    if(medians[pos] != 2 and clasificador2.predicts[pos] == 2):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"2 \\t\", conf0, \"\\t\", conf1, \"\\t\", tp_m2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c3_predict)):\n",
    "    if(medians[pos] != 3 and clasificador2.predicts[pos] == 3):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"3 \\t\", conf0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", tp_m3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c4_predict)):\n",
    "    if(medians[pos] != 4 and clasificador2.predicts[pos] == 4):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"4 \\t\", conf0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", tp_m4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c5_predict)):\n",
    "    if(medians[pos] != 5 and clasificador2.predicts[pos] == 5):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "print(\"5 \\t\", conf0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", tp_m5);\n",
    "\n",
    "print(\"Macro averaging\")\n",
    "macropresicion=(tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) / ( (tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) + (fp_m0 + fp_m1 + fp_m2 + fp_m3 + fp_m4 + fp_m5))\n",
    "macrorecall=(tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) / ( (tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) + (fn_m0 + fn_m1 + fn_m2 + fn_m3 + fn_m4 + fn_m5))\n",
    "macrof=2*macropresicion*macrorecall/(macrorecall + macropresicion)\n",
    "print(\"Precision \", macropresicion, \" Recall\", macrorecall, \" F\", macrof)\n",
    "\n",
    "print(\"Micro averaging\")\n",
    "micropresicion=(p_m0 + p_m1 + p_m2 + p_m3 + p_m4 + p_m5)/6\n",
    "microrecall=(p_m0 + r_m1 + r_m2 + r_m3 + r_m4 + r_m5)/6\n",
    "microf=2*micropresicion*microrecall/(micropresicion + microrecall)\n",
    "print(\"Precision \", micropresicion, \" Recall\", microrecall, \" F\", microf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de medidas para las medianas hecho de forma independiente, se presentan las tablas de contingencia de las predicciones de los clasificadores binarios sin tener en cuenta las probabilidades de los otros clasificadores binarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicter Median 1\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 3 \t|\t 4 \t|\n",
      "system negative|\t 99 \t|\t 514 \t|\n",
      "Presicion  0.42857142857142855\n",
      "Recall  0.029411764705882353\n",
      "Accuracy  0.01129032258064516\n",
      "E-measure  0.055045871559633024\n",
      "Predicter Median 2\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 3 \t|\n",
      "system negative|\t 95 \t|\t 522 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 3\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 4 \t|\t 4 \t|\n",
      "system negative|\t 106 \t|\t 506 \t|\n",
      "Presicion  0.5\n",
      "Recall  0.03636363636363636\n",
      "Accuracy  0.012903225806451613\n",
      "E-measure  0.06779661016949153\n",
      "Predicter Median 4\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 3 \t|\n",
      "system negative|\t 57 \t|\t 560 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 5\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 11 \t|\t 609 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 0\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 135 \t|\t 67 \t|\n",
      "system negative|\t 110 \t|\t 308 \t|\n",
      "Presicion  0.6683168316831684\n",
      "Recall  0.5510204081632653\n",
      "Accuracy  0.3258064516129032\n",
      "E-measure  0.6040268456375838\n"
     ]
    }
   ],
   "source": [
    "#Variables para las medianas\n",
    "tp_m1=0\n",
    "fp_m1=0\n",
    "tn_m1=0\n",
    "fn_m1=0\n",
    "\n",
    "tp_m2=0\n",
    "fp_m2=0\n",
    "tn_m2=0\n",
    "fn_m2=0\n",
    "\n",
    "tp_m3=0\n",
    "fp_m3=0\n",
    "tn_m3=0\n",
    "fn_m3=0\n",
    "\n",
    "tp_m4=0\n",
    "fp_m4=0\n",
    "tn_m4=0\n",
    "fn_m4=0\n",
    "\n",
    "tp_m5=0\n",
    "fp_m5=0\n",
    "tn_m5=0\n",
    "fn_m5=0\n",
    "\n",
    "tp_m0=0\n",
    "fp_m0=0\n",
    "tn_m0=0\n",
    "fn_m0=0\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c1) ):\n",
    "    if(clasificador2.median_list_c1[i] == 1 and clasificador2.c1_predict[i]==1):\n",
    "        tp_m1+=1\n",
    "    elif(clasificador2.median_list_c1[i] == 1 and clasificador2.c1_predict[i]==0):\n",
    "        fn_m1+=1\n",
    "    elif(clasificador2.median_list_c1[i] == 0 and clasificador2.c1_predict[i]==1):\n",
    "        fp_m1+=1\n",
    "    else:\n",
    "        tn_m1+=1\n",
    "\n",
    "p_m1=-1\n",
    "r_m1=-1\n",
    "if(tp_m1 + fp_m1 > 0):\n",
    "    p_m1 = tp_m1/(tp_m1 + fp_m1)\n",
    "if(tp_m1 + fn_m1):\n",
    "    r_m1 = tp_m1 / (tp_m1 + fn_m1)\n",
    "\n",
    "print(\"Predicter Median 1\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m1,\"\\t|\\t\", fp_m1, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m1, \"\\t|\\t\", tn_m1, \"\\t|\")\n",
    "if p_m1 > 0:\n",
    "    print(\"Presicion \", p_m1)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m1 > 0:\n",
    "    print(\"Recall \", r_m1)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m1 > 0 and r_m1 > 0):\n",
    "    print(\"Accuracy \", (tp_m1 + fp_m1) / (tp_m1 + fp_m1 + tn_m1 + fn_m1))\n",
    "    print(\"E-measure \", 2*p_m1*r_m1/(p_m1 + r_m1))\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c2) ):\n",
    "    if(clasificador2.median_list_c2[i] == 1 and clasificador2.c2_predict[i]==1):\n",
    "        tp_m2+=1\n",
    "    elif(clasificador2.median_list_c2[i] == 1 and clasificador2.c2_predict[i]==0):\n",
    "        fn_m2+=1\n",
    "    elif(clasificador2.median_list_c2[i] == 0 and clasificador2.c2_predict[i]==1):\n",
    "        fp_m2+=1\n",
    "    else:\n",
    "        tn_m2+=1\n",
    "\n",
    "p_m2=-1\n",
    "r_m2=-1\n",
    "if(tp_m2 + fp_m2 > 0):\n",
    "    p_m2 = tp_m2/(tp_m2 + fp_m2)\n",
    "if(tp_m2 + fn_m2):\n",
    "    r_m2 = tp_m2 / (tp_m2 + fn_m2)\n",
    "\n",
    "print(\"Predicter Median 2\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m2,\"\\t|\\t\", fp_m2, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m2, \"\\t|\\t\", tn_m2, \"\\t|\")\n",
    "if p_m2 > 0:\n",
    "    print(\"Presicion \", p_m2)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m2 > 0:\n",
    "    print(\"Recall \", r_m2)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m2 > 0 and r_m2 > 0):\n",
    "    print(\"Accuracy \", (tp_m2 + fp_m2) / (tp_m2 + fp_m2 + tn_m2 + fn_m2))\n",
    "    print(\"E-measure \", 2*p_m2*r_m2/(p_m2 + r_m2))\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c3) ):\n",
    "    if(clasificador2.median_list_c3[i] == 1 and clasificador2.c3_predict[i]==1):\n",
    "        tp_m3+=1\n",
    "    elif(clasificador2.median_list_c3[i] == 1 and clasificador2.c3_predict[i]==0):\n",
    "        fn_m3+=1\n",
    "    elif(clasificador2.median_list_c3[i] == 0 and clasificador2.c3_predict[i]==1):\n",
    "        fp_m3+=1\n",
    "    else:\n",
    "        tn_m3+=1\n",
    "\n",
    "p_m3=-1\n",
    "r_m3=-1\n",
    "if(tp_m3 + fp_m3 > 0):\n",
    "    p_m3 = tp_m3/(tp_m3 + fp_m3)\n",
    "if(tp_m3 + fn_m3):\n",
    "    r_m3 = tp_m3 / (tp_m3 + fn_m3)\n",
    "\n",
    "print(\"Predicter Median 3\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m3,\"\\t|\\t\", fp_m3, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m3, \"\\t|\\t\", tn_m3, \"\\t|\")\n",
    "if p_m3 > 0:\n",
    "    print(\"Presicion \", p_m3)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m3 > 0:\n",
    "    print(\"Recall \", r_m3)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m3 > 0 and r_m3 > 0):\n",
    "    print(\"Accuracy \", (tp_m3 + fp_m3) / (tp_m3 + fp_m3 + tn_m3 + fn_m3))\n",
    "    print(\"E-measure \", 2*p_m3*r_m3/(p_m3 + r_m3))\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c4) ):\n",
    "    if(clasificador2.median_list_c4[i] == 1 and clasificador2.c4_predict[i]==1):\n",
    "        tp_m4+=1\n",
    "    elif(clasificador2.median_list_c4[i] == 1 and clasificador2.c4_predict[i]==0):\n",
    "        fn_m4+=1\n",
    "    elif(clasificador2.median_list_c4[i] == 0 and clasificador2.c4_predict[i]==1):\n",
    "        fp_m4+=1\n",
    "    else:\n",
    "        tn_m4+=1\n",
    "\n",
    "p_m4=-1\n",
    "r_m4=-1\n",
    "if(tp_m4 + fp_m4 > 0):\n",
    "    p_m4 = tp_m4/(tp_m4 + fp_m4)\n",
    "if(tp_m4 + fn_m4):\n",
    "    r_m4 = tp_m4 / (tp_m4 + fn_m4)\n",
    "\n",
    "print(\"Predicter Median 4\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m4,\"\\t|\\t\", fp_m4, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m4, \"\\t|\\t\", tn_m4, \"\\t|\")\n",
    "if p_m4 > 0:\n",
    "    print(\"Presicion \", p_m4)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m4 > 0:\n",
    "    print(\"Recall \", r_m4)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m4 > 0 and r_m4 > 0):\n",
    "    print(\"Accuracy \", (tp_m4 + fp_m4) / (tp_m4 + fp_m4 + tn_m4 + fn_m4))\n",
    "    print(\"E-measure \", 2*p_m4*r_m4/(p_m4 + r_m4))\n",
    "\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c5) ):\n",
    "    if(clasificador2.median_list_c5[i] == 1 and clasificador2.c5_predict[i]==1):\n",
    "        tp_m5+=1\n",
    "    elif(clasificador2.median_list_c5[i] == 1 and clasificador2.c5_predict[i]==0):\n",
    "        fn_m5+=1\n",
    "    elif(clasificador2.median_list_c5[i] == 0 and clasificador2.c5_predict[i]==1):\n",
    "        fp_m5+=1\n",
    "    else:\n",
    "        tn_m5+=1\n",
    "\n",
    "p_m5=-1\n",
    "r_m5=-1\n",
    "if(tp_m5 + fp_m5 > 0):\n",
    "    p_m5 = tp_m5/(tp_m5 + fp_m5)\n",
    "if(tp_m5 + fn_m5):\n",
    "    r_m5 = tp_m5 / (tp_m5 + fn_m5)\n",
    "\n",
    "print(\"Predicter Median 5\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m5,\"\\t|\\t\", fp_m5, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m5, \"\\t|\\t\", tn_m5, \"\\t|\")\n",
    "if p_m5 > 0:\n",
    "    print(\"Presicion \", p_m5)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m5 > 0:\n",
    "    print(\"Recall \", r_m5)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m5 > 0 and r_m5 > 0):\n",
    "    print(\"Accuracy \", (tp_m5 + fp_m5) / (tp_m5 + fp_m5 + tn_m5 + fn_m5))\n",
    "    print(\"E-measure \", 2*p_m5*r_m5/(p_m5 + r_m5))\n",
    "    \n",
    "for i in range( len(clasificador2.median_list_c0) ):\n",
    "    if(clasificador2.median_list_c0[i] == 1 and clasificador2.c0_predict[i]==1):\n",
    "        tp_m0+=1\n",
    "    elif(clasificador2.median_list_c0[i] == 1 and clasificador2.c0_predict[i]==0):\n",
    "        fn_m0+=1\n",
    "    elif(clasificador2.median_list_c0[i] == 0 and clasificador2.c0_predict[i]==1):\n",
    "        fp_m0+=1\n",
    "    else:\n",
    "        tn_m0+=1\n",
    "\n",
    "p_m0=-1\n",
    "r_m0=-1\n",
    "if(tp_m0 + fp_m0 > 0):\n",
    "    p_m0 = tp_m0/(tp_m0 + fp_m0)\n",
    "if(tp_m0 + fn_m0):\n",
    "    r_m0 = tp_m0 / (tp_m0 + fn_m0)\n",
    "\n",
    "print(\"Predicter Median 0\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m0,\"\\t|\\t\", fp_m0, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m0, \"\\t|\\t\", tn_m0, \"\\t|\")\n",
    "if p_m0 > 0:\n",
    "    print(\"Presicion \", p_m0)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m0 > 0:\n",
    "    print(\"Recall \", r_m0)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m0 > 0 and r_m0 > 0):\n",
    "    print(\"Accuracy \", (tp_m0 + fp_m0) / (tp_m0 + fp_m0 + tn_m0 + fn_m0))\n",
    "    print(\"E-measure \", 2*p_m0*r_m0/(p_m0 + r_m0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador 1 sin información de Pos-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4 = pandas.read_csv(\"corpus_filtro3.csv\",encoding='utf-8')\n",
    "\n",
    "num_dev_tweets=math.floor(len(corpus_filtro4)*20/100)\n",
    "#Se generan posiciones en el arreglo tweets al azar que representen el 20% del corpus actual\n",
    "random_samples=np.random.randint(0, len(corpus_filtro4) - 1, size=num_dev_tweets)\n",
    "\n",
    "#Listas para entrenamiento\n",
    "text_train = []\n",
    "category_train=[]\n",
    "\n",
    "#Listas para desarrollo\n",
    "text_dev = []\n",
    "category_dev=[]\n",
    "\n",
    "#Listas con datos del corpus\n",
    "vec_text=corpus_filtro4['text'][:]\n",
    "vec_category=corpus_filtro4['category'][:]\n",
    "\n",
    "#Se separa el 20% de corpus para desarrollo del 80% para entrenamiento\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    if(np.any(random_samples[:] == i)):\n",
    "        text_dev.append(vec_text[i])\n",
    "        category_dev.append(vec_category[i])\n",
    "    else:\n",
    "        text_train.append(vec_text[i])\n",
    "        category_train.append(vec_category[i])\n",
    "\n",
    "\n",
    "#Se guardan las instancias corpus de desarrollo en corpus_filtro5_devset.csv\n",
    "#y corpus de entrenamiento en corpus_filtro5_trainingset.csv\n",
    "d = {'text' : text_train,\n",
    "    'category': category_train\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_trainingset_no_postag.csv')\n",
    "\n",
    "\n",
    "d = {'text' : text_dev,\n",
    "    'category': category_dev\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category'])\n",
    "df.to_csv('corpus_filtro5_devset_no_postag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medidas para el clasificador 1 sin post-tag\n",
      "Precision:  0.72\n",
      "Recall:  0.7476038338658147\n",
      "Accuracy  0.5241935483870968\n",
      "E-measure  0.7335423197492164\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro5_devset = pandas.read_csv(\"corpus_filtro5_devset_no_postag.csv\",encoding='utf-8')\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 1\n",
    "list_cat_1 = []\n",
    "list_ecat_1 = []\n",
    "\n",
    "\n",
    "list_cat_1=corpus_filtro5_devset['category'][:]\n",
    "\n",
    "clasificador = Clasificador1(\"corpus_filtro5_trainingset_no_postag.csv\")\n",
    "list_ecat_1  = clasificador.predict_clasiffier_1(\"corpus_filtro5_devset_no_postag.csv\")\n",
    "\n",
    "#Variables para clasificador 1\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_1) ):\n",
    "    if(list_cat_1[i] == 1 and list_ecat_1[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_1[i] == 1 and list_ecat_1[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_1[i] == 0 and list_ecat_1[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "print(\"Medidas para el clasificador 1 sin post-tag\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados de la etapa de desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La medida de Acuracy nos dice que el clasificador 1 tiene un 55% de los tweets etiquetados correctamente, esta medida puede ser tenida en cuenta como valedera debido a que los tweets chistosos y no chistosos están balanceados en el conjunto de entrenamiento para el clasificador 1.\n",
    "La medida F para el clasificador 1 marca 0.77.\n",
    "\n",
    "El clasificador 2 es el mismo que el clasificador 1, la variación se explica debido a que el conjunto de desarrollo fue elegido al azar y es distinto al conjunto de desarrollo del clasificador 1. De todas maneras las diferencias en las medidas no son notorias, para el clasificador 2, en comparación al clasificador 1, aumentó la Precision en 0.03 puntos. Lo que se explica debido a que en la elección al azar distinta, el clasificador 2 encontró menos falsos positivos. que en el caso del clasificador 1. Esta variación también terminó afectando la medida F que aumentó en 0.015 puntos. Todo explicable por la variación de los datos de entrenamiento y desarrollo, ya que el clasificador es el mismo.\n",
    "\n",
    "El clasificador 3 por su parte arroja una Acurracy de 0.67, pero el conjunto de desarrollo está desbalanceado, 149 tweets son considerados no humoristicos en comparación con los restantes 459 tweets considerados como humorísticos. La F-measure marcó un 0.78. Las medidas que componen la F-measure son Presicion y Recall, la primera marcó 0.75 y la segunda marcó una mayor puntuación de 0.82. Esto puede sugiere que los errores mayormente cometidos por el clasificador están en la generación de falsos positivos, es decir tweets clasificados como humorísticos pero que en realidad no lo son. Parece aceptable la disminución de falsos negativos, y la consiguiente disminución del recall, en favor de una disminución de los falsos positivos y por consiguiente un aumento de la Presicion. Habrá que estudiar los resultados para determinar de que forma hacer esto.\n",
    "\n",
    "El clasificador 1 sin información de POS-tag arrojó una medida F de 0.73, 0.04 puntos por debajo de la medida F del clasificador 1 con información de POS-tag. Esto nos dice que la incorporación de características, y la ingeniería de características puede influir de forma positiva en manera significativa en la performance de los clasificadores.\n",
    "\n",
    "Para el clasificador 3 una primera mejora puede venir del análisis de los casos clasificados como falsos positivos y agregar alguna regla pos-clasificación en caso de encontrar un patròn en dichos casos. Hay que tener cuidado con el sobre entrenamiento ya que las reglas podrían estar ajustando demasiado el clasificador al conjunto de desarrollo.\n",
    "\n",
    "Para el clasificador dado en 1 y 2 se deberían analizar los casos para los cuales se clasfició mal y tratar de inferir patrones comunes para estos casos, una solución es agregar reglas de pos-procesamiento, pero se puede caer en el problema de sobre entrenamiento. Otra posibilidad es estudiar los patrones sintácticos en los casos de error, lemas, cantidad de palabras desconocidas, estructuras de los árboles sintácticos y agregar como features información lèxica y sintáctica inferida de estudiar los resultados en estos casos.\n",
    "\n",
    "En cuanto a la predicción de las medianas, en la matriz de confusión lo primero que se puede apreciar es la baja taza de verdaderos positivos y falsos positivos para los valores de mediana 4 y 5. También se nota que las predicciones actuales funcionan mejor para detectar verdaderos negativos que para detectar verdaderos positivos. El clasificador binario para la mediana con valor 0 fue el que con mayor éxito pudio detectar verdaderos positivos. Para cada fila en la matriz de confusión se puede observar que la distribución de los falsos positivos en las columnas es bastante uniforme. Mientras que observando las columnas se puede apreciar que la distribución de falsos negativos está más que nada concentrada en las medianas 0 y 3. Este comportamiento puede deberse a la predominacia de tweets con medianas 0 y 3 en el training set y development set.\n",
    "Se realizó el cálculo de microaveraging y macroaveraging para la predicción de medianas, arrojando valores muy bajos para la Presicion y Recall. Posibles mejoras a cada clasificador binario sería estudiar porque los clasificadores binarios para medianas 1,2,4,5 arrojaron medidas bajas y tratar de mejorar su funcionamiento. También cualquier modificación al clasificador 2, pos-procesamiento o incorporación de features pueden incidir positiva o negativamente en la predicción de las medianas.\n",
    "\n",
    "La conclusión más importante es que hay mucho por trabajar en los clasificadores diseñados, antes de realizar la comparación entre ellos a través de correr el conjunto de prueba. En base al cambio visto entre el clasificador 1 con pos-tag respecto al clasificador 1 sin pos-tag al proyecto le faltó trabajar más la ingeniería de características.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_f = []\n",
    "\n",
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro1 = pandas.read_csv(\"corpus_humor_testing.csv\",encoding='utf-8')\n",
    "\n",
    "#Generacion de patron de Hashtag\n",
    "pattern_hashtag = re.compile(r'#.+?\\b')\n",
    "\n",
    "\n",
    "#Se sustituye el Hashtag por el string vacio, los tweets que\n",
    "#van pasando por este procesamiento se van guardando en el arreglo\n",
    "#text_f\n",
    "for i in range(len(corpus_filtro1)):\n",
    "    text_f.append(re.sub(pattern_hashtag, \"\", corpus_filtro1['text'][i]))\n",
    "#El segundo filtro aplicado es guardado en el archivo corpus_filtro2.csv\n",
    "d = {'id' : corpus_filtro1['id'][:],\n",
    "    'text' : text_f,\n",
    "    'account_id': corpus_filtro1['account_id'][:],\n",
    "    'n':corpus_filtro1['n'][:], \n",
    "    '1':corpus_filtro1['1'][:],\n",
    "    '2':corpus_filtro1['2'][:],\n",
    "    '3':corpus_filtro1['3'][:],\n",
    "    '4':corpus_filtro1['4'][:],\n",
    "    '5':corpus_filtro1['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_humor_testing_no_hashtags.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar Category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro2 = pandas.read_csv(\"corpus_humor_testing_no_hashtags.csv\",encoding='utf-8')\n",
    "\n",
    "#Listas para salvar los tweets filtrados\n",
    "text = []\n",
    "category=[]\n",
    "h1=[]\n",
    "h2=[]\n",
    "h3=[]\n",
    "h4=[]\n",
    "h5=[]\n",
    "\n",
    "#Listas del corpus\n",
    "vec_text=corpus_filtro2['text'][:]\n",
    "vec_no_humor =corpus_filtro2['n'][:]\n",
    "vec_e1 =corpus_filtro2['1'][:]\n",
    "vec_e2 =corpus_filtro2['2'][:]\n",
    "vec_e3 =corpus_filtro2['3'][:]\n",
    "vec_e4 =corpus_filtro2['4'][:]\n",
    "vec_e5 =corpus_filtro2['5'][:]\n",
    "\n",
    "#Si hay igual o mas votos de humor que de no humor la categoria tiene el valor 1\n",
    "#en caso contrario tiene el valor 0\n",
    "for i in range(len(corpus_filtro2)):\n",
    "    if  vec_e1[i] + vec_e2[i] + vec_e3[i] + vec_e4[i] + vec_e5[i] >= vec_no_humor[i]:\n",
    "        category.append(1)\n",
    "    else:\n",
    "        category.append(0)\n",
    "    text.append(vec_text[i])\n",
    "    h1.append(vec_e1[i])\n",
    "    h2.append(vec_e2[i])\n",
    "    h3.append(vec_e3[i])\n",
    "    h4.append(vec_e4[i])\n",
    "    h5.append(vec_e5[i])\n",
    "#El tercer procesamiento aplicado sobre los tweets es guardado en el archivo corpus_filtro.3\n",
    "d = {'text' : text,\n",
    "    'category': category,\n",
    "     'n': vec_no_humor,\n",
    "     '1':h1,\n",
    "     '2':h2,\n",
    "     '3':h3,\n",
    "     '4':h4,\n",
    "     '5':h5\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category', 'n', '1','2','3','4','5'])\n",
    "df.to_csv('corpus_humor_testing_no_hashtags_with_category.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Se carga la instancia salvada del corpus que se encuentra en el archivo\n",
    "#corpus_filtro4.csv\n",
    "corpus_filtro4 = pandas.read_csv(\"corpus_humor_testing_no_hashtags_with_category.csv\",encoding='utf-8')\n",
    "\n",
    "text_list=corpus_filtro4['text'][:]\n",
    "category_list=corpus_filtro4['category'][:]\n",
    "\n",
    "n_list=corpus_filtro4['n'][:]\n",
    "h1_list=corpus_filtro4['1'][:]\n",
    "h2_list=corpus_filtro4['2'][:]\n",
    "h3_list=corpus_filtro4['3'][:]\n",
    "h4_list=corpus_filtro4['4'][:]\n",
    "h5_list=corpus_filtro4['5'][:]\n",
    "\n",
    "medians=[]\n",
    "pos=0\n",
    "\n",
    "#Caclulo de la mediana, en values se expanden los votos de cada estrella\n",
    "#de la siguiente manera si #cant_3Estrellas_en_tweet = 4, entonces se guardan\n",
    "#en values 4 treses 3,3,3,3 para permitir el calculo de la mediana\n",
    "#Por ultimo en la estructura medians se guarda el valor discreto (0,1,2,3,4 o 5) de la mediana\n",
    "#si la cantidad de votos es par se toma el voto del medio con valor mas grande\n",
    "for i in range( len(corpus_filtro4)):\n",
    "    values = []\n",
    "    for j in range( h1_list[i]):\n",
    "        values.append(1)\n",
    "    for j in range( h2_list[i] ):\n",
    "        values.append(2)\n",
    "    for j in range( h3_list[i] ):\n",
    "        values.append(3)\n",
    "    for j in range( h4_list[i] ):\n",
    "        values.append(4)\n",
    "    for j in range( h5_list[i] ):\n",
    "        values.append(5)\n",
    "    for j in range( n_list[i] ):\n",
    "        values.append(0)\n",
    "    mediana=np.median(values)\n",
    "    if( len(values) % 2 == 1 ):\n",
    "        medians.append(math.floor(mediana))\n",
    "    else:\n",
    "        for i in values:\n",
    "            if( i >= mediana):\n",
    "                medians.append(i)\n",
    "                break\n",
    "    pos+=1\n",
    "\n",
    "    \n",
    "#El nuevo procesamiento que suma la informacion de la mediana se guarda\n",
    "#en el archivo corpus_filtro4median.csv\n",
    "d = {'text' : corpus_filtro4['text'][:],\n",
    "    'category' : corpus_filtro4['category'][:],\n",
    "    'median': medians,\n",
    "     'n':corpus_filtro4['n'][:],\n",
    "    '1':corpus_filtro4['1'][:],\n",
    "    '2':corpus_filtro4['2'][:],\n",
    "    '3':corpus_filtro4['3'][:],\n",
    "    '4':corpus_filtro4['4'][:],\n",
    "    '5':corpus_filtro4['5'][:]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', 'median', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_humor_testing_no_hashtags_with_category_and_median.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar category_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_filtro4median = pandas.read_csv(\"corpus_humor_testing_no_hashtags_with_category_and_median.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro4median['text'][:]\n",
    "category_list=corpus_filtro4median['category'][:]\n",
    "category_2_list=[]\n",
    "median_list=corpus_filtro4median['median'][:]\n",
    "category_list=corpus_filtro4median['category'][:]\n",
    "n_list=corpus_filtro4median['n'][:]\n",
    "h1_list=corpus_filtro4median['1'][:]\n",
    "h2_list=corpus_filtro4median['2'][:]\n",
    "h3_list=corpus_filtro4median['3'][:]\n",
    "h4_list=corpus_filtro4median['4'][:]\n",
    "h5_list=corpus_filtro4median['5'][:]\n",
    "pos=0\n",
    "for m in median_list:\n",
    "    if(m<1):\n",
    "        category_2_list.append(0)\n",
    "    else:\n",
    "        category_2_list.append(1)\n",
    "    pos+=1\n",
    "d = {'text' : text_list,\n",
    "    'category': category_list,\n",
    "     'median': median_list,\n",
    "     'category_2': category_2_list,\n",
    "     'n': n_list,\n",
    "     '1': h1_list,\n",
    "     '2': h2_list,\n",
    "     '3': h3_list,\n",
    "     '4':h4_list,\n",
    "     '5':h5_list\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text','category','median', 'category_2', 'n', '1', '2', '3', '4', '5'])\n",
    "df.to_csv('corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar información de post-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5   0 %\r"
     ]
    }
   ],
   "source": [
    "#Se carga la instancia de tweets salvada en el bloque de codigo anterior\n",
    "corpus_filtro3 = pandas.read_csv(\"corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2.csv\",encoding='utf-8')\n",
    "text_list=corpus_filtro3['text'][:5]\n",
    "\n",
    "#Se genera el patron para identificar las POS de las palabras\n",
    "pattern_pos = re.compile(r'pos=\"(.*?)\"')\n",
    "\n",
    "#Variables para datos estadisticos\n",
    "num_sentences=len(corpus_filtro3)\n",
    "numsentence=1\n",
    "\n",
    "#Variable auxiliar para adherir las POS a los tweets\n",
    "pos=0\n",
    "\n",
    "#Para cada documento en el corpus de tweets que va siendo procesado\n",
    "#utilizando la herramienta freeling se analizan las categorias gramaticales\n",
    "#de todas las palabras que aparecen en cada tweet y esta informacion es\n",
    "#adherida al texto del tweet\n",
    "for d in text_list:\n",
    "    if(type(d) == str):\n",
    "        xml = analyzer.run(d.encode(), 'flush')\n",
    "        print(numsentence, \" \", math.floor( numsentence/num_sentences*100),\"%\", end=\"\\r\")\n",
    "        for sentence in xml:        \n",
    "            for token in sentence:\n",
    "                token_byte=etree.tostring(token)\n",
    "                m = re.search(pattern_pos, token_byte.decode())\n",
    "                if m is not None:\n",
    "                    text_list[ pos] = text_list[pos] + \" \" + m.group(1)\n",
    "    pos+=1\n",
    "    numsentence+=1\n",
    "\n",
    "#El procesamiento actual del corpus es guardado en el documento corpus_filtro4.csv\n",
    "d = {'text' : text_list[:5],\n",
    "    'category': corpus_filtro3['category'][:5],   \n",
    "     'median':corpus_filtro3['median'][:5],\n",
    "     'category_2':corpus_filtro3['category_2'][:5],\n",
    "     'n':corpus_filtro3['n'][:5], \n",
    "     '1':corpus_filtro3['1'][:5],\n",
    "     '2':corpus_filtro3['2'][:5],\n",
    "     '3':corpus_filtro3['3'][:5],\n",
    "     '4':corpus_filtro3['4'][:5],\n",
    "     '5':corpus_filtro3['5'][:5]\n",
    "    }\n",
    "df = pandas.DataFrame(d, columns = ['text', 'category', 'median', 'category_2', 'n', '1','2','3','4','5'])\n",
    "df.to_csv('corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2_and_postag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medidas para el clasificador 1 sin pos tag\n",
      "Precision:  0.5\n",
      "Recall:  1.0\n",
      "Accuracy  0.4\n",
      "E-measure  0.6666666666666666\n",
      "Medidas para el clasificador 1\n",
      "Precision:  0.5\n",
      "Recall:  0.5\n",
      "Accuracy  0.4\n",
      "E-measure  0.5\n",
      "Medidas para el clasificador 2\n",
      "Precision:  1.0\n",
      "Recall:  0.6666666666666666\n",
      "Accuracy  0.4\n",
      "E-measure  0.8\n",
      "Medidas para el clasificador 3\n",
      "Precision:  0.3333333333333333\n",
      "Recall:  1.0\n",
      "Accuracy  0.6\n",
      "E-measure  0.5\n",
      "Predicter Median 1\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 0 \t|\t 5 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 2\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 2 \t|\t 3 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 3\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 0 \t|\t 5 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 4\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 1 \t|\t 4 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 5\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 0 \t|\t 0 \t|\n",
      "system negative|\t 0 \t|\t 5 \t|\n",
      "Cant calculate Presicion, division by zero\n",
      "Cant calculate Recall, division by zero\n",
      "Predicter Median 0\n",
      "\t\t gold positive  | gold negative\n",
      "system positive|\t 2 \t|\t 3 \t|\n",
      "system negative|\t 0 \t|\t 0 \t|\n",
      "Presicion  0.4\n",
      "Recall  1.0\n",
      "Accuracy  1.0\n",
      "E-measure  0.5714285714285715\n",
      "Confusion matrix\n",
      "\t 0 \t 1 \t 2 \t 3 \t 4 \t 5 \n",
      "0 \t 2 \t 0 \t 2 \t 0 \t 1 \t 0\n",
      "1 \t 0 \t 0 \t 0 \t 0 \t 0 \t 0\n",
      "2 \t 0 \t 0 \t 0 \t 0 \t 0 \t 0\n",
      "3 \t 0 \t 0 \t 0 \t 0 \t 0 \t 0\n",
      "4 \t 0 \t 0 \t 0 \t 0 \t 0 \t 0\n",
      "5 \t 0 \t 0 \t 0 \t 0 \t 0 \t 0\n",
      "Macro averaging\n",
      "Precision  0.4  Recall 0.4  F 0.4000000000000001\n",
      "Micro averaging\n",
      "Precision  0.06666666666666667  Recall 0.16666666666666666  F 0.09523809523809522\n"
     ]
    }
   ],
   "source": [
    "corpus_filtro5_devset_no_postag = pandas.read_csv(\"corpus_filtro5_devset_no_postag.csv\",encoding='utf-8')\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 1\n",
    "list_cat_1 = []\n",
    "list_ecat_1 = []\n",
    "\n",
    "\n",
    "list_cat_1=corpus_filtro5_devset_no_postag['category'][:5]\n",
    "\n",
    "clasificador = Clasificador1(\"corpus_filtro5_trainingset_no_postag.csv\")\n",
    "list_ecat_1  = clasificador.predict_clasiffier_1(\"corpus_filtro5_devset_no_postag.csv\")\n",
    "\n",
    "#Variables para clasificador 1\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_1) ):\n",
    "    if(list_cat_1[i] == 1 and list_ecat_1[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_1[i] == 1 and list_ecat_1[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_1[i] == 0 and list_ecat_1[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "print(\"Medidas para el clasificador 1 sin pos tag\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "corpus_filtro5_devset = pandas.read_csv(\"corpus_filtro5_devset.csv\",encoding='utf-8')\n",
    "corpus_filtro5_devset_c2 = pandas.read_csv(\"corpus_filtro5_devset_c2.csv\",encoding='utf-8')\n",
    "corpus_filtro5_devset_c3 = pandas.read_csv(\"corpus_filtro5_devset_c3.csv\",encoding='utf-8')\n",
    "\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 1\n",
    "list_cat_1 = []\n",
    "list_ecat_1 = []\n",
    "\n",
    "\n",
    "list_cat_1=corpus_filtro5_devset['category'][:5]\n",
    "\n",
    "clasificador = Clasificador1(\"corpus_filtro5_trainingset.csv\")\n",
    "list_ecat_1  = clasificador.predict_clasiffier_1(\"corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2_and_postag.csv\")\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 2\n",
    "list_cat_2 = []\n",
    "list_ecat_2 = []\n",
    "list_edem_2=[]\n",
    "medians = []\n",
    "\n",
    "\n",
    "list_cat_2=corpus_filtro5_devset_c2['category'][:5]\n",
    "clasificador2=Clasificador2()\n",
    "list_emed_2, _, list_ecat_2=clasificador2.predict_median(\"corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2_and_postag.csv\")\n",
    "medians=corpus_filtro5_devset_c2['median'][:5];\n",
    "\n",
    "#Lista de categorias del cuerpo de desarrollo\n",
    "#y de categorias estimadas sobre dicho cuerpo\n",
    "#para el clasificador 3\n",
    "list_cat_3 = []\n",
    "list_ecat_3 = []\n",
    "\n",
    "list_cat_3=corpus_filtro5_devset_c3['category'][:5]\n",
    "clasificador3 = Clasificador3()\n",
    "list_ecat_3=clasificador3.predict_clasiffier_1(\"corpus_humor_testing_no_hashtags_with_category_and_median_and_category_2_and_postag.csv\", \"category_2\")\n",
    "\n",
    "\n",
    "#Variables para clasificador 1\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_1) ):\n",
    "    if(list_cat_1[i] == 1 and list_ecat_1[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_1[i] == 1 and list_ecat_1[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_1[i] == 0 and list_ecat_1[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "print(\"Medidas para el clasificador 1\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "#Variables para el clasificador 2\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_2) ):\n",
    "    if(list_cat_2[i] == 1 and list_ecat_2[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_2[i] == 1 and list_ecat_2[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_2[i] == 0 and list_ecat_2[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "\n",
    "print(\"Medidas para el clasificador 2\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "#Variables para el clasificador 3\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "\n",
    "for i in range( len(list_cat_3) ):\n",
    "    if(list_cat_3[i] == 1 and list_ecat_3[i]==1):\n",
    "        tp+=1\n",
    "    elif(list_cat_3[i] == 1 and list_ecat_3[i]==0):\n",
    "        fn+=1\n",
    "    elif(list_cat_3[i] == 0 and list_ecat_3[i]==1):\n",
    "        fp+=1\n",
    "    else:\n",
    "        tn+=1\n",
    "\n",
    "p = tp/(tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "print(\"Medidas para el clasificador 3\")\n",
    "print(\"Precision: \", p )\n",
    "print(\"Recall: \", r)\n",
    "print(\"Accuracy \", (tp + fp) / (tp + fp + tn + fn))\n",
    "print(\"E-measure \", 2*p*r/(p + r))\n",
    "\n",
    "\n",
    "#Variables para las medianas\n",
    "tp_m1=0\n",
    "fp_m1=0\n",
    "tn_m1=0\n",
    "fn_m1=0\n",
    "\n",
    "tp_m2=0\n",
    "fp_m2=0\n",
    "tn_m2=0\n",
    "fn_m2=0\n",
    "\n",
    "tp_m3=0\n",
    "fp_m3=0\n",
    "tn_m3=0\n",
    "fn_m3=0\n",
    "\n",
    "tp_m4=0\n",
    "fp_m4=0\n",
    "tn_m4=0\n",
    "fn_m4=0\n",
    "\n",
    "tp_m5=0\n",
    "fp_m5=0\n",
    "tn_m5=0\n",
    "fn_m5=0\n",
    "\n",
    "tp_m0=0\n",
    "fp_m0=0\n",
    "tn_m0=0\n",
    "fn_m0=0\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c1) ):\n",
    "    if(clasificador2.m1[i] == 1 and medians[i]==1):\n",
    "        tp_m1+=1\n",
    "    elif(clasificador2.m1[i] == 0 and medians[i]==1):\n",
    "        fn_m1+=1\n",
    "    elif(clasificador2.m1[i] == 1 and medians[i]!=1):\n",
    "        fp_m1+=1\n",
    "    else:\n",
    "        tn_m1+=1\n",
    "\n",
    "p_m1=-1\n",
    "r_m1=-1\n",
    "if(tp_m1 + fp_m1 > 0):\n",
    "    p_m1 = tp_m1/(tp_m1 + fp_m1)\n",
    "if(tp_m1 + fn_m1):\n",
    "    r_m1 = tp_m1 / (tp_m1 + fn_m1)\n",
    "\n",
    "print(\"Predicter Median 1\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m1,\"\\t|\\t\", fp_m1, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m1, \"\\t|\\t\", tn_m1, \"\\t|\")\n",
    "if p_m1 > 0:\n",
    "    print(\"Presicion \", p_m1)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m1 > 0:\n",
    "    print(\"Recall \", r_m1)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m1 > 0 and r_m1 > 0):\n",
    "    print(\"Accuracy \", (tp_m1 + fp_m1) / (tp_m1 + fp_m1 + tn_m1 + fn_m1))\n",
    "    print(\"E-measure \", 2*p_m1*r_m1/(p_m1 + r_m1))\n",
    "\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c2) ):\n",
    "    if(clasificador2.m2[i] == 1 and medians[i]==2):\n",
    "        tp_m2+=1\n",
    "    elif(clasificador2.m2[i] == 0 and medians[i]==2):\n",
    "        fn_m2+=1\n",
    "    elif(clasificador2.m2[i] == 1 and medians[i]!=2):\n",
    "        fp_m2+=1\n",
    "    else:\n",
    "        tn_m2+=1\n",
    "\n",
    "p_m2=-1\n",
    "r_m2=-1\n",
    "if(tp_m2 + fp_m2 > 0):\n",
    "    p_m2 = tp_m2/(tp_m2 + fp_m2)\n",
    "if(tp_m2 + fn_m2):\n",
    "    r_m2 = tp_m2 / (tp_m2 + fn_m2)\n",
    "\n",
    "print(\"Predicter Median 2\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m2,\"\\t|\\t\", fp_m2, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m2, \"\\t|\\t\", tn_m2, \"\\t|\")\n",
    "if p_m2 > 0:\n",
    "    print(\"Presicion \", p_m2)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m2 > 0:\n",
    "    print(\"Recall \", r_m2)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m2 > 0 and r_m2 > 0):\n",
    "    print(\"Accuracy \", (tp_m2 + fp_m2) / (tp_m2 + fp_m2 + tn_m2 + fn_m2))\n",
    "    print(\"E-measure \", 2*p_m2*r_m2/(p_m2 + r_m2))\n",
    "    \n",
    "for i in range( len(clasificador2.median_list_c3) ):\n",
    "    if(clasificador2.m3[i] == 1 and medians[i]==3):\n",
    "        tp_m3+=1\n",
    "    elif(clasificador2.m3[i] == 0 and medians[i]==3):\n",
    "        fn_m3+=1\n",
    "    elif(clasificador2.m3[i] == 1 and medians[i]!=3):\n",
    "        fp_m3+=1\n",
    "    else:\n",
    "        tn_m3+=1\n",
    "\n",
    "p_m3=-1\n",
    "r_m3=-1\n",
    "if(tp_m3 + fp_m3 > 0):\n",
    "    p_m3 = tp_m3/(tp_m3 + fp_m3)\n",
    "if(tp_m3 + fn_m3):\n",
    "    r_m3 = tp_m3 / (tp_m3 + fn_m3)\n",
    "\n",
    "print(\"Predicter Median 3\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m3,\"\\t|\\t\", fp_m3, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m3, \"\\t|\\t\", tn_m3, \"\\t|\")\n",
    "if p_m3 > 0:\n",
    "    print(\"Presicion \", p_m3)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m3 > 0:\n",
    "    print(\"Recall \", r_m3)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m3 > 0 and r_m3 > 0):\n",
    "    print(\"Accuracy \", (tp_m3 + fp_m3) / (tp_m3 + fp_m3 + tn_m3 + fn_m3))\n",
    "    print(\"E-measure \", 2*p_m3*r_m3/(p_m3 + r_m3))\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c4) ):\n",
    "    if(clasificador2.m4[i] == 1 and medians[i]==4):\n",
    "        tp_m4+=1\n",
    "    elif(clasificador2.m4[i] == 0 and medians[i]==4):\n",
    "        fn_m4+=1\n",
    "    elif(clasificador2.m4[i] == 1 and medians[i]!=4):\n",
    "        fp_m4+=1\n",
    "    else:\n",
    "        tn_m4+=1\n",
    "\n",
    "p_m4=-1\n",
    "r_m4=-1\n",
    "if(tp_m4 + fp_m4 > 0):\n",
    "    p_m4 = tp_m4/(tp_m4 + fp_m4)\n",
    "if(tp_m4 + fn_m4):\n",
    "    r_m4 = tp_m4 / (tp_m4 + fn_m4)\n",
    "\n",
    "print(\"Predicter Median 4\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m4,\"\\t|\\t\", fp_m4, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m4, \"\\t|\\t\", tn_m4, \"\\t|\")\n",
    "if p_m4 > 0:\n",
    "    print(\"Presicion \", p_m4)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m4 > 0:\n",
    "    print(\"Recall \", r_m4)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m4 > 0 and r_m4 > 0):\n",
    "    print(\"Accuracy \", (tp_m4 + fp_m4) / (tp_m4 + fp_m4 + tn_m4 + fn_m4))\n",
    "    print(\"E-measure \", 2*p_m4*r_m4/(p_m4 + r_m4))\n",
    "\n",
    "\n",
    "for i in range( len(clasificador2.median_list_c5) ):\n",
    "    if(clasificador2.m5[i] == 1 and medians[i]==5):\n",
    "        tp_m5+=1\n",
    "    elif(clasificador2.m5[i] == 0 and medians[i]==5):\n",
    "        fn_m5+=1\n",
    "    elif(clasificador2.m5[i] == 1 and medians[i]!=5):\n",
    "        fp_m5+=1\n",
    "    else:\n",
    "        tn_m5+=1\n",
    "\n",
    "p_m5=-1\n",
    "r_m5=-1\n",
    "if(tp_m5 + fp_m5 > 0):\n",
    "    p_m5 = tp_m5/(tp_m5 + fp_m5)\n",
    "if(tp_m5 + fn_m5):\n",
    "    r_m5 = tp_m5 / (tp_m5 + fn_m5)\n",
    "\n",
    "print(\"Predicter Median 5\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m5,\"\\t|\\t\", fp_m5, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m5, \"\\t|\\t\", tn_m5, \"\\t|\")\n",
    "if p_m5 > 0:\n",
    "    print(\"Presicion \", p_m5)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m5 > 0:\n",
    "    print(\"Recall \", r_m5)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m5 > 0 and r_m5 > 0):\n",
    "    print(\"Accuracy \", (tp_m5 + fp_m5) / (tp_m5 + fp_m5 + tn_m5 + fn_m5))\n",
    "    print(\"E-measure \", 2*p_m5*r_m5/(p_m5 + r_m5))\n",
    "    \n",
    "for i in range( len(clasificador2.median_list_c0) ):\n",
    "    if(clasificador2.m0[i] == 1 and medians[i]==0):\n",
    "        tp_m0+=1\n",
    "    elif(clasificador2.m0[i] == 0 and medians[i]==0):\n",
    "        fn_m0+=1\n",
    "    elif(clasificador2.m0[i] == 1 and medians[i]!=0):\n",
    "        fp_m0+=1\n",
    "    else:\n",
    "        tn_m0+=1\n",
    "\n",
    "p_m0=-1\n",
    "r_m0=-1\n",
    "if(tp_m0 + fp_m0 > 0):\n",
    "    p_m0 = tp_m0/(tp_m0 + fp_m0)\n",
    "if(tp_m0 + fn_m0):\n",
    "    r_m0 = tp_m0 / (tp_m0 + fn_m0)\n",
    "\n",
    "print(\"Predicter Median 0\")\n",
    "print(\"\\t\\t gold positive  | gold negative\")\n",
    "print(\"system positive|\\t\", tp_m0,\"\\t|\\t\", fp_m0, \"\\t|\")\n",
    "print(\"system negative|\\t\", fn_m0, \"\\t|\\t\", tn_m0, \"\\t|\")\n",
    "if p_m0 > 0:\n",
    "    print(\"Presicion \", p_m0)\n",
    "else:\n",
    "    print(\"Cant calculate Presicion, division by zero\")\n",
    "if r_m0 > 0:\n",
    "    print(\"Recall \", r_m0)\n",
    "else:\n",
    "    print(\"Cant calculate Recall, division by zero\")\n",
    "if(p_m0 > 0 and r_m0 > 0):\n",
    "    print(\"Accuracy \", (tp_m0 + fp_m0) / (tp_m0 + fp_m0 + tn_m0 + fn_m0))\n",
    "    print(\"E-measure \", 2*p_m0*r_m0/(p_m0 + r_m0))\n",
    "    \n",
    "print(\"Confusion matrix\")\n",
    "print(\"\\t 0 \\t 1 \\t 2 \\t 3 \\t 4 \\t 5 \")\n",
    "\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c0_predict)):\n",
    "    if(medians[pos] != 0 and clasificador2.predicts[pos] == 0):\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"0 \\t\", tp_m0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c1_predict)):\n",
    "    if(medians[pos] != 1 and clasificador2.predicts[pos] == 1):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"1 \\t\", conf0, \"\\t\", tp_m1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c2_predict)):\n",
    "    if(medians[pos] != 2 and clasificador2.predicts[pos] == 2):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"2 \\t\", conf0, \"\\t\", conf1, \"\\t\", tp_m2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c3_predict)):\n",
    "    if(medians[pos] != 3 and clasificador2.predicts[pos] == 3):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"3 \\t\", conf0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", tp_m3, \"\\t\", conf4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c4_predict)):\n",
    "    if(medians[pos] != 4 and clasificador2.predicts[pos] == 4):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 5):\n",
    "            conf5+=1\n",
    "print(\"4 \\t\", conf0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", tp_m4,\"\\t\", conf5);\n",
    "\n",
    "conf0=0\n",
    "conf1=0\n",
    "conf2=0\n",
    "conf3=0\n",
    "conf4=0\n",
    "conf5=0\n",
    "\n",
    "\n",
    "for pos in range(len(clasificador2.c5_predict)):\n",
    "    if(medians[pos] != 5 and clasificador2.predicts[pos] == 5):\n",
    "        if(medians[pos] == 0):\n",
    "            conf0+=1\n",
    "        if(medians[pos] == 1):\n",
    "            conf1+=1\n",
    "        if(medians[pos] == 2):\n",
    "            conf2+=1\n",
    "        if(medians[pos] == 3):\n",
    "            conf3+=1\n",
    "        if(medians[pos] == 4):\n",
    "            conf4+=1\n",
    "print(\"5 \\t\", conf0, \"\\t\", conf1, \"\\t\", conf2,\"\\t\", conf3, \"\\t\", conf4,\"\\t\", tp_m5);\n",
    "\n",
    "print(\"Macro averaging\")\n",
    "macropresicion=(tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) / ( (tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) + (fp_m0 + fp_m1 + fp_m2 + fp_m3 + fp_m4 + fp_m5))\n",
    "macrorecall=(tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) / ( (tp_m0 + tp_m1 + tp_m2 + tp_m4 + tp_m5) + (fn_m0 + fn_m1 + fn_m2 + fn_m3 + fn_m4 + fn_m5))\n",
    "macrof=2*macropresicion*macrorecall/(macrorecall + macropresicion)\n",
    "print(\"Precision \", macropresicion, \" Recall\", macrorecall, \" F\", macrof)\n",
    "\n",
    "if(p_m0 < 0):\n",
    "    p_m0=0\n",
    "if(p_m1 < 0):\n",
    "    p_m1=0\n",
    "if(p_m2 < 0):\n",
    "    p_m2=0\n",
    "if(p_m3 < 0):\n",
    "    p_m3=0\n",
    "if(p_m3 < 0):\n",
    "    p_m3=0\n",
    "if(p_m4 < 0):\n",
    "    p_m4=0\n",
    "if(p_m4 < 0):\n",
    "    p_m4=0\n",
    "if(p_m5 < 0):\n",
    "    p_m5=0\n",
    "\n",
    "if(r_m0 < 0):\n",
    "    r_m0=0\n",
    "if(r_m1 < 0):\n",
    "    r_m1=0\n",
    "if(r_m2 < 0):\n",
    "    r_m2=0\n",
    "if(r_m3 < 0):\n",
    "    r_m3=0\n",
    "if(r_m3 < 0):\n",
    "    r_m3=0\n",
    "if(r_m4 < 0):\n",
    "    r_m4=0\n",
    "if(r_m4 < 0):\n",
    "    r_m4=0\n",
    "if(r_m5 < 0):\n",
    "    r_m5=0\n",
    "\n",
    "print(\"Micro averaging\")\n",
    "micropresicion=(p_m0 + p_m1 + p_m2 + p_m3 + p_m4 + p_m5)/6\n",
    "microrecall=(r_m0 + r_m1 + r_m2 + r_m3 + r_m4 + r_m5)/6\n",
    "microf=2*micropresicion*microrecall/(micropresicion + microrecall)\n",
    "print(\"Precision \", micropresicion, \" Recall\", microrecall, \" F\", microf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
